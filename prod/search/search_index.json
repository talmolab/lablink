{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"LabLink","text":"<p>Cloud-based virtual teaching lab accessible through Chrome browser.</p> <p>Deploy tutorial environments to AWS with pre-installed software\u2014students only need a web browser to get started.</p>"},{"location":"#getting-started","title":"Getting Started","text":"PrerequisitesQuickstartConfiguration <p> Set up your AWS account, credentials, and required tools before deployment.</p> <p> View requirements</p> <p> Deploy LabLink to AWS in 15 minutes with our step-by-step guide.</p> <p> Get started</p> <p> Customize your deployment with environment variables, instance types, and more.</p> <p> Configure</p>"},{"location":"#components","title":"Components","text":"<ul> <li> <p> Allocator</p> <p>Web service managing VM requests, user authentication, and database operations.</p> <p> Learn more</p> </li> <li> <p> Client VMs</p> <p>EC2 instances running pre-installed tutorial software with GPU support and health monitoring.</p> <p> Learn more</p> </li> <li> <p> Infrastructure</p> <p>Terraform templates for AWS deployment including VPC, security groups, and auto-scaling.</p> <p> AWS setup</p> </li> </ul>"},{"location":"#resources","title":"Resources","text":"<ul> <li> GitHub - Source code, issues, and contributions</li> <li> Template - Ready-to-use deployment template</li> <li> Support - Report issues or request features</li> </ul>"},{"location":"adapting/","title":"Adapting LabLink for Your Research Software","text":"<p>This guide walks you through customizing LabLink for your own research software, beyond the default SLEAP configuration.</p>"},{"location":"adapting/#overview","title":"Overview","text":"<p>LabLink is designed to be software-agnostic. While it ships with SLEAP as the default research software, you can adapt it for any computational workflow that can run in Docker.</p>"},{"location":"adapting/#adaptation-checklist","title":"Adaptation Checklist","text":"<ul> <li> Create custom Docker image with your software</li> <li> Configure Git repository (if needed)</li> <li> Update LabLink configuration</li> <li> Test locally</li> <li> Deploy to AWS</li> </ul>"},{"location":"adapting/#step-by-step-guide","title":"Step-by-Step Guide","text":""},{"location":"adapting/#step-1-create-your-docker-image","title":"Step 1: Create Your Docker Image","text":"<p>Your Docker image should contain:</p> <ol> <li>Base OS (Ubuntu, Debian, etc.)</li> <li>Your research software and dependencies</li> <li>LabLink client service (optional, for health monitoring)</li> </ol>"},{"location":"adapting/#option-a-extend-lablink-client-base","title":"Option A: Extend LabLink Client Base","text":"<p>Build on top of the existing LabLink client image:</p> <p><code>Dockerfile</code>: <pre><code>FROM ghcr.io/talmolab/lablink-client-base-image:latest\n\n# Install your software\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    your-dependencies \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Install your Python package\nCOPY requirements.txt /app/\nRUN pip install -r /app/requirements.txt\n\n# Copy your code\nCOPY your_software/ /app/your_software/\n\n# Set entrypoint\nCMD [\"python\", \"/app/your_software/main.py\"]\n</code></pre></p>"},{"location":"adapting/#option-b-build-from-scratch","title":"Option B: Build from Scratch","text":"<p>Create a completely custom image:</p> <p><code>Dockerfile</code>: <pre><code>FROM ubuntu:20.04\n\n# Install basic dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    python3.9 \\\n    python3-pip \\\n    git \\\n    curl \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Install your research software\nRUN pip3 install your-research-package\n\n# Optional: Include LabLink client for monitoring\nCOPY --from=ghcr.io/talmolab/lablink-client-base-image:latest \\\n    /app/lablink-client-service \\\n    /app/lablink-client-service\n\n# Your startup script\nCOPY entrypoint.sh /entrypoint.sh\nRUN chmod +x /entrypoint.sh\n\nENTRYPOINT [\"/entrypoint.sh\"]\n</code></pre></p>"},{"location":"adapting/#build-and-push","title":"Build and Push","text":"<pre><code># Build your image\ndocker build -t ghcr.io/your-org/your-research-image:latest .\n\n# Test locally\ndocker run -it ghcr.io/your-org/your-research-image:latest\n\n# Push to registry\ndocker login ghcr.io\ndocker push ghcr.io/your-org/your-research-image:latest\n</code></pre> <p>GitHub Container Registry</p> <p>Use GitHub Container Registry (ghcr.io) for free image hosting. See GitHub Packages.</p>"},{"location":"adapting/#step-2-prepare-your-code-repository","title":"Step 2: Prepare Your Code Repository","text":"<p>If your research code lives in a Git repository, LabLink can automatically clone it onto each VM.</p> <p>Requirements: - Public repository (or configure SSH keys for private) - Code that can run non-interactively - Dependencies installable via package manager</p> <p>Example Repository Structure: <pre><code>your-research-repo/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 setup.py (or pyproject.toml)\n\u251c\u2500\u2500 your_package/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 analysis.py\n\u251c\u2500\u2500 configs/\n\u2502   \u2514\u2500\u2500 default_config.yaml\n\u2514\u2500\u2500 scripts/\n    \u2514\u2500\u2500 run_analysis.sh\n</code></pre></p> <p>Entrypoint: Ensure your code has a clear entrypoint (script, main function, etc.)</p>"},{"location":"adapting/#step-3-update-lablink-configuration","title":"Step 3: Update LabLink Configuration","text":"<p>Edit the allocator configuration to use your custom image and repository.</p> <p><code>lablink-infrastructure/config/config.yaml</code>:</p> <pre><code>machine:\n  machine_type: \"g4dn.xlarge\"  # Choose appropriate instance type\n  image: \"ghcr.io/your-org/your-research-image:latest\"\n  ami_id: \"ami-067cc81f948e50e06\"  # Ubuntu 20.04 + Docker (us-west-2)\n  repository: \"https://github.com/your-org/your-research-code.git\"\n  software: \"your-software-name\"\n\n# ... rest of config\n</code></pre> <p>Key Fields:</p> <ul> <li><code>image</code>: Your Docker image from Step 1</li> <li><code>repository</code>: Your Git repository (or empty string if none)</li> <li><code>software</code>: Identifier for your software (used by client service)</li> <li><code>machine_type</code>: EC2 instance type appropriate for your workload</li> </ul>"},{"location":"adapting/#step-4-test-locally","title":"Step 4: Test Locally","text":"<p>Before deploying to AWS, test your setup locally.</p>"},{"location":"adapting/#run-your-docker-image","title":"Run Your Docker Image","text":"<pre><code>docker run -d \\\n  --name test-client \\\n  -e ALLOCATOR_HOST=localhost \\\n  -e ALLOCATOR_PORT=5000 \\\n  ghcr.io/your-org/your-research-image:latest\n</code></pre>"},{"location":"adapting/#check-logs","title":"Check Logs","text":"<pre><code>docker logs test-client\n</code></pre> <p>Verify: - Image starts without errors - Dependencies are available - Your code runs as expected</p>"},{"location":"adapting/#test-full-stack","title":"Test Full Stack","text":"<p>Run both allocator and your client locally:</p> <pre><code># Terminal 1: Start allocator\ndocker run -d -p 5000:5000 --name allocator \\\n  ghcr.io/talmolab/lablink-allocator-image:latest\n\n# Terminal 2: Start your client\ndocker run -d --name client \\\n  -e ALLOCATOR_HOST=host.docker.internal \\\n  -e ALLOCATOR_PORT=5000 \\\n  ghcr.io/your-org/your-research-image:latest\n\n# Check allocator web UI\nopen http://localhost:5000\n</code></pre>"},{"location":"adapting/#step-5-deploy-to-aws","title":"Step 5: Deploy to AWS","text":"<p>Once local testing succeeds, deploy to AWS.</p>"},{"location":"adapting/#update-configuration","title":"Update Configuration","text":"<p>Commit your configuration changes:</p> <pre><code>git add lablink-infrastructure/config/config.yaml\ngit commit -m \"Configure LabLink for [your software]\"\ngit push\n</code></pre>"},{"location":"adapting/#deploy-via-terraform","title":"Deploy via Terraform","text":"<pre><code>cd lablink-infrastructure\n\nterraform init\n\nterraform apply \\\n  -var=\"resource_suffix=dev\" \\\n  -var=\"allocator_image_tag=linux-amd64-latest-test\"\n</code></pre>"},{"location":"adapting/#verify-deployment","title":"Verify Deployment","text":"<ol> <li> <p>Get allocator IP:    <pre><code>terraform output ec2_public_ip\n</code></pre></p> </li> <li> <p>Access web interface: <code>http://&lt;ec2_ip&gt;:80</code></p> </li> <li> <p>Create client VMs via admin interface</p> </li> <li> <p>Monitor VM creation and status</p> </li> </ol>"},{"location":"adapting/#advanced-customization","title":"Advanced Customization","text":""},{"location":"adapting/#custom-ami","title":"Custom AMI","text":"<p>For faster VM startup, create a custom AMI with your software pre-installed:</p> <pre><code># Launch base instance\naws ec2 run-instances --image-id ami-067cc81f948e50e06 ...\n\n# SSH in and install your software\nssh -i key.pem ubuntu@&lt;instance-ip&gt;\nsudo apt-get update\n# ... install your software\n\n# Create AMI\naws ec2 create-image \\\n  --instance-id i-xxxxx \\\n  --name \"your-software-ami\" \\\n  --description \"Custom AMI with your software\"\n\n# Use in config.yaml\nmachine:\n  ami_id: \"ami-your-custom-ami\"\n</code></pre> <p>Benefits: - Faster VM startup - Pre-installed dependencies - Consistent environment</p>"},{"location":"adapting/#environment-specific-configurations","title":"Environment-Specific Configurations","text":"<p>Create separate configs for different workloads:</p> <p><code>conf/config-cpu.yaml</code>: <pre><code>machine:\n  machine_type: \"c5.2xlarge\"  # CPU-optimized\n  image: \"ghcr.io/your-org/your-research-image-cpu:latest\"\n  software: \"your-software-cpu\"\n</code></pre></p> <p><code>conf/config-gpu.yaml</code>: <pre><code>machine:\n  machine_type: \"p3.2xlarge\"  # GPU-optimized\n  image: \"ghcr.io/your-org/your-research-image-gpu:latest\"\n  software: \"your-software-gpu\"\n</code></pre></p> <p>Use with: <pre><code>python main.py --config-name=config-gpu\n</code></pre></p>"},{"location":"adapting/#multi-software-support","title":"Multi-Software Support","text":"<p>Support multiple research software packages in one deployment:</p> <pre><code># Use software identifier to select behavior\nmachine:\n  software: \"multi\"  # Or pass dynamically\n\n# Client code checks software identifier:\n# if config.client.software == \"sleap\":\n#     run_sleap()\n# elif config.client.software == \"your_tool\":\n#     run_your_tool()\n</code></pre>"},{"location":"adapting/#private-docker-registries","title":"Private Docker Registries","text":"<p>Use private registries (Docker Hub, ECR):</p> <ol> <li>Store registry credentials in AWS Secrets Manager</li> <li>Update user data script to authenticate:    <pre><code>aws ecr get-login-password --region us-west-2 | \\\n  docker login --username AWS --password-stdin &lt;account&gt;.dkr.ecr.us-west-2.amazonaws.com\n</code></pre></li> <li>Reference private image:    <pre><code>image: \"&lt;account&gt;.dkr.ecr.us-west-2.amazonaws.com/your-image:latest\"\n</code></pre></li> </ol>"},{"location":"adapting/#custom-health-checks","title":"Custom Health Checks","text":"<p>Implement software-specific health monitoring:</p> <p><code>your_health_check.py</code>: <pre><code>import requests\n\ndef check_health():\n    \"\"\"Check if your software is healthy.\"\"\"\n    # Example: Check GPU availability\n    try:\n        import torch\n        assert torch.cuda.is_available()\n    except:\n        return False\n\n    # Example: Check disk space\n    import shutil\n    _, _, free = shutil.disk_usage(\"/\")\n    if free &lt; 10 * 1024**3:  # Less than 10GB\n        return False\n\n    return True\n\ndef report_to_allocator(status):\n    \"\"\"Report status to allocator.\"\"\"\n    requests.post(\n        f\"http://{ALLOCATOR_HOST}:{ALLOCATOR_PORT}/health\",\n        json={\"status\": \"healthy\" if status else \"unhealthy\"}\n    )\n</code></pre></p>"},{"location":"adapting/#example-adapting-for-pytorch-training","title":"Example: Adapting for PyTorch Training","text":"<p>Complete example for a PyTorch training workflow.</p>"},{"location":"adapting/#dockerfile","title":"Dockerfile","text":"<pre><code>FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu20.04\n\n# Install Python and dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    python3.9 python3-pip git \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Install PyTorch\nRUN pip3 install torch torchvision torchaudio \\\n    --index-url https://download.pytorch.org/whl/cu118\n\n# Install your training code dependencies\nCOPY requirements.txt /app/\nRUN pip3 install -r /app/requirements.txt\n\n# Copy training scripts\nCOPY train.py /app/\nCOPY utils/ /app/utils/\n\nWORKDIR /app\nCMD [\"python3\", \"train.py\"]\n</code></pre>"},{"location":"adapting/#configuration","title":"Configuration","text":"<pre><code>machine:\n  machine_type: \"g5.2xlarge\"  # A10G GPU\n  image: \"ghcr.io/your-org/pytorch-training:latest\"\n  repository: \"https://github.com/your-org/training-data.git\"\n  software: \"pytorch-training\"\n  ami_id: \"ami-067cc81f948e50e06\"\n</code></pre>"},{"location":"adapting/#training-script","title":"Training Script","text":"<p><code>train.py</code>: <pre><code>import torch\nimport sys\n\ndef main():\n    # Check GPU\n    if not torch.cuda.is_available():\n        print(\"ERROR: No GPU available\")\n        sys.exit(1)\n\n    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n\n    # Your training code here\n    model = YourModel().cuda()\n    optimizer = torch.optim.Adam(model.parameters())\n\n    # Training loop\n    for epoch in range(100):\n        train_epoch(model, optimizer)\n        print(f\"Epoch {epoch} complete\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre></p>"},{"location":"adapting/#troubleshooting","title":"Troubleshooting","text":""},{"location":"adapting/#image-wont-start","title":"Image Won't Start","text":"<p>Check: <pre><code>docker logs &lt;container&gt;\n</code></pre></p> <p>Common issues: - Missing dependencies - Incorrect entrypoint - Permission errors</p>"},{"location":"adapting/#repository-clone-fails","title":"Repository Clone Fails","text":"<p>Check: - Repository URL is correct - Repository is public (or SSH keys configured) - Network connectivity from VM</p>"},{"location":"adapting/#software-not-found","title":"Software Not Found","text":"<p>Check: - Software installed in Docker image - PATH environment variable set correctly - Dependencies installed</p>"},{"location":"adapting/#performance-issues","title":"Performance Issues","text":"<p>Check: - Instance type appropriate for workload - GPU drivers installed (for GPU instances) - Sufficient disk space</p>"},{"location":"adapting/#best-practices","title":"Best Practices","text":"<ol> <li>Test locally first: Always test Docker images locally before AWS deployment</li> <li>Pin versions: Use specific tags (<code>v1.0.0</code>) not <code>:latest</code> in production</li> <li>Minimize image size: Remove unnecessary dependencies</li> <li>Document requirements: Clear README for your custom setup</li> <li>Version your images: Tag images with version numbers</li> <li>Use multi-stage builds: Reduce final image size</li> <li>Cache dependencies: Layer Dockerfile for faster builds</li> </ol>"},{"location":"adapting/#next-steps","title":"Next Steps","text":"<ul> <li>Deployment: Deploy your customized setup</li> <li>Workflows: Set up CI/CD for your images</li> <li>Configuration: Fine-tune your settings</li> <li>FAQ: Common customization questions</li> </ul>"},{"location":"adapting/#need-help","title":"Need Help?","text":"<ul> <li>Check Troubleshooting for common issues</li> <li>Review example configurations (if available)</li> <li>Open an issue on GitHub</li> </ul>"},{"location":"api-endpoints/","title":"API Endpoints","text":"<p>This document outlines the API endpoints provided by the LabLink Allocator service.</p>"},{"location":"api-endpoints/#public-api-endpoints","title":"Public API Endpoints","text":"<p>These endpoints are designed for interaction with client VMs and end-users without requiring admin authentication.</p>"},{"location":"api-endpoints/#request-a-vm","title":"Request a VM","text":"<p>Assigns an available VM to a user.</p> <p>Endpoint: <code>POST /api/request_vm</code></p> <p>Description: Submits a user's email and a Chrome Remote Desktop (CRD) command to be assigned to an available VM. If a VM is available, it is assigned to the user, and the user is shown a success page with connection details.</p> <p>Authentication: None</p> <p>Request Body: <code>application/x-www-form-urlencoded</code></p> <ul> <li><code>email</code> (string, required): The user's email address.</li> <li><code>crd_command</code> (string, required): The CRD command for the session.</li> </ul> <p>Success Response:</p> <ul> <li>Code: <code>200 OK</code></li> <li>Content: An HTML page (<code>success.html</code>) displaying the assigned VM's hostname and PIN.</li> </ul> <p>Error Response:</p> <ul> <li>Code: <code>200 OK</code></li> <li>Content: An HTML page (<code>index.html</code>) with an error message displayed if no VMs are available, if required fields are missing, or if an invalid CRD command is provided.</li> </ul> <p>Client Usage: This endpoint is not used by the client service. It is called from the allocator's web interface when an end-user manually requests a VM.</p>"},{"location":"api-endpoints/#vm-startup-registration","title":"VM Startup Registration","text":"<p>Used by a client VM to register itself with the allocator upon startup and to listen for an assignment.</p> <p>Endpoint: <code>POST /vm_startup</code></p> <p>Description: A client VM calls this endpoint after it boots up. It sends its hostname and then listens for a PostgreSQL notification that contains the assigned <code>CrdCommand</code> and <code>Pin</code>. This is a long-polling request.</p> <p>Authentication: None</p> <p>Request Body: <code>application/json</code></p> <pre><code>{\n  \"hostname\": \"lablink-vm-prod-1\"\n}\n</code></pre> <p>Success Response:</p> <ul> <li>Code: <code>200 OK</code></li> <li>Content: <pre><code>{\n  \"status\": \"success\",\n  \"pin\": \"123456\",\n  \"command\": \"crd --code=...\"\n}\n</code></pre></li> </ul> <p>Error Response:</p> <ul> <li>Code: <code>400 Bad Request</code> if <code>hostname</code> is not provided.</li> <li>Code: <code>404 Not Found</code> if the VM with the given <code>hostname</code> is not found in the database.</li> </ul> <p>Client Usage:</p> <ul> <li>When: Called by the <code>subscribe</code> service, which is started by <code>start.sh</code> when the client container boots.</li> <li>How: The service sends a POST request with the VM's hostname. It then waits indefinitely for a response. When an end-user is assigned this VM, the allocator responds with the CRD command and PIN, which the client then uses to start the remote desktop session.</li> </ul>"},{"location":"api-endpoints/#get-unassigned-vm-count","title":"Get Unassigned VM Count","text":"<p>Retrieves the number of available (unassigned) VMs.</p> <p>Endpoint: <code>GET /api/unassigned_vms_count</code></p> <p>Description: Returns the current count of VMs that are running and not yet assigned to a user.</p> <p>Authentication: None</p> <p>Request Body: None</p> <p>Success Response:</p> <ul> <li>Code: <code>200 OK</code></li> <li>Content: <pre><code>{\n  \"count\": 5\n}\n</code></pre></li> </ul> <p>Client Usage: This endpoint is not used by the client service. It is intended for external monitoring or UI components on the allocator to display the number of available VMs.</p>"},{"location":"api-endpoints/#update-vm-in-use-status","title":"Update VM In-Use Status","text":"<p>Updates the \"in-use\" status of a VM.</p> <p>Endpoint: <code>POST /api/update_inuse_status</code></p> <p>Description: Called by the client VM to indicate whether a user is actively using it.</p> <p>Authentication: None</p> <p>Request Body: <code>application/json</code></p> <pre><code>{\n  \"hostname\": \"lablink-vm-prod-1\",\n  \"status\": true\n}\n</code></pre> <p>Success Response:</p> <ul> <li>Code: <code>200 OK</code></li> <li>Content: <pre><code>{\n  \"message\": \"In-use status updated successfully.\"\n}\n</code></pre></li> </ul> <p>Error Response:</p> <ul> <li>Code: <code>400 Bad Request</code> if <code>hostname</code> or <code>status</code> is missing.</li> <li>Code: <code>500 Internal Server Error</code> on failure.</li> </ul> <p>Client Usage:</p> <ul> <li>When: Called by the <code>update_inuse_status</code> service, which is started by <code>start.sh</code> and runs continuously.</li> <li>How: The service monitors for the presence of the research software process (e.g., <code>sleap</code>). When the process starts, it sends a POST request with <code>status: true</code>. When the process stops, it sends <code>status: false</code>. This allows the allocator to know if a user is actively using the VM.</li> </ul>"},{"location":"api-endpoints/#update-gpu-health","title":"Update GPU Health","text":"<p>Updates the GPU health status of a VM.</p> <p>Endpoint: <code>POST /api/gpu_health</code></p> <p>Description: Called by the client VM to report its GPU health status.</p> <p>Authentication: None</p> <p>Request Body: <code>application/json</code></p> <pre><code>{\n  \"hostname\": \"lablink-vm-prod-1\",\n  \"gpu_status\": \"healthy\"\n}\n</code></pre> <p>Success Response:</p> <ul> <li>Code: <code>200 OK</code></li> <li>Content: <pre><code>{\n  \"message\": \"GPU health status updated successfully.\"\n}\n</code></pre></li> </ul> <p>Error Response:</p> <ul> <li>Code: <code>400 Bad Request</code> if <code>hostname</code> or <code>gpu_status</code> is missing.</li> <li>Code: <code>500 Internal Server Error</code> on failure.   Client Usage:</li> <li>When: Called by the <code>check_gpu</code> service, which is started by <code>start.sh</code> and runs continuously.</li> <li>How: The service periodically runs <code>nvidia-smi</code>. Based on the output, it determines the GPU status (<code>Healthy</code>, <code>Unhealthy</code>, or <code>N/A</code>) and sends a POST request to the allocator whenever the status changes.</li> </ul>"},{"location":"api-endpoints/#update-vm-status","title":"Update VM Status","text":"<p>Updates the overall status of a VM (e.g., <code>initializing</code>, <code>running</code>, <code>error</code>).</p> <p>Endpoint: <code>POST /api/vm-status</code></p> <p>Description: Called by the client VM during its startup sequence to report its current status.</p> <p>Authentication: None</p> <p>Request Body: <code>application/json</code></p> <pre><code>{\n  \"hostname\": \"lablink-vm-prod-1\",\n  \"status\": \"running\"\n}\n</code></pre> <p>Success Response:</p> <ul> <li>Code: <code>200 OK</code></li> <li>Content: <pre><code>{\n  \"message\": \"VM status updated successfully.\"\n}\n</code></pre></li> </ul> <p>Error Response:</p> <ul> <li>Code: <code>400 Bad Request</code> if <code>hostname</code> or <code>status</code> is missing.</li> <li>Code: <code>500 Internal Server Error</code> on failure.   Client Usage: This endpoint is not called from the <code>packages/client</code> code. Instead, it is called by the <code>user_data.sh</code> script during the client VM's initial boot sequence (cloud-init). This script reports <code>initializing</code> and <code>running</code> statuses to the allocator, allowing it to track the VM's progress before the client service container has started.</li> </ul>"},{"location":"api-endpoints/#receive-vm-metrics","title":"Receive VM Metrics","text":"<p>Receives and stores startup metrics from a VM.</p> <p>Endpoint: <code>POST /api/vm-metrics/&lt;hostname&gt;</code></p> <p>Description: Called by the client VM's <code>user_data.sh</code> script to post timing metrics for <code>cloud-init</code> and container startup.</p> <p>Authentication: None</p> <p>URL Parameters:</p> <ul> <li><code>hostname</code> (string, required): The hostname of the VM reporting metrics.</li> </ul> <p>Request Body: <code>application/json</code></p> <pre><code>{\n  \"cloud_init_start\": 1678886400,\n  \"cloud_init_end\": 1678886460,\n  \"cloud_init_duration_seconds\": 60\n}\n</code></pre> <p>Success Response:</p> <ul> <li>Code: <code>200 OK</code></li> <li>Content: <pre><code>{\n  \"message\": \"VM metrics posted successfully.\"\n}\n</code></pre></li> </ul> <p>Error Response:</p> <ul> <li>Code: <code>404 Not Found</code> if the VM does not exist.</li> <li>Code: <code>500 Internal Server Error</code> on failure.</li> </ul> <p>Client Usage:</p> <ul> <li>When: At the end of the client container's startup sequence.</li> <li>How: The <code>start.sh</code> script, which is the container's entrypoint, records its start and end times. It then sends a <code>curl</code> POST request with these timing metrics to the allocator. This helps in monitoring the duration of container startup.</li> </ul>"},{"location":"api-endpoints/#receive-vm-logs","title":"Receive VM Logs","text":"<p>Receives and stores logs pushed from a VM.</p> <p>Endpoint: <code>POST /api/vm-logs</code></p> <p>Description: Called by the CloudWatch agent on the client VM (via a Lambda subscription) to push <code>cloud-init</code> logs to the allocator.</p> <p>Authentication: None</p> <p>Request Body: <code>application/json</code></p> <pre><code>{\n  \"log_group\": \"/aws/ec2/lablink\",\n  \"log_stream\": \"lablink-vm-prod-1\",\n  \"messages\": [\"log line 1\", \"log line 2\"]\n}\n</code></pre> <p>Success Response:</p> <ul> <li>Code: <code>200 OK</code></li> <li>Content: <pre><code>{\n  \"message\": \"VM logs posted successfully.\"\n}\n</code></pre></li> </ul> <p>Error Response:</p> <ul> <li>Code: <code>400 Bad Request</code> if required fields are missing.</li> <li>Code: <code>404 Not Found</code> if the VM does not exist.</li> <li>Code: <code>500 Internal Server Error</code> on failure.</li> </ul> <p>Client Usage: This endpoint is not called directly from any code in <code>packages/client</code>. It is part of the AWS infrastructure that forwards logs from the client VM. Logs from the client VM are sent to AWS CloudWatch, which triggers a Lambda function that then forwards the log messages to this endpoint on the allocator.</p>"},{"location":"api-endpoints/#admin-api-endpoints","title":"Admin API Endpoints","text":"<p>These endpoints require HTTP Basic Authentication and are intended for administrators to manage the VM pool.</p>"},{"location":"api-endpoints/#launch-vms","title":"Launch VMs","text":"<p>Endpoint: <code>POST /api/launch</code></p> <p>Description: Takes a number of VMs to create, generates a Terraform variables file, and runs <code>terraform apply</code> to provision the new instances.</p> <p>Authentication: HTTP Basic Auth</p> <p>Request Body: <code>application/x-www-form-urlencoded</code></p> <ul> <li><code>num_vms</code> (integer, required): The number of new VMs to launch.</li> </ul> <p>Success Response:</p> <ul> <li>Code: <code>200 OK</code></li> <li>Content: An HTML page (<code>dashboard.html</code>) displaying the Terraform output and a real-time status monitor for the VMs.</li> </ul> <p>Error Response:</p> <ul> <li>Code: <code>200 OK</code></li> <li>Content: An HTML page (<code>dashboard.html</code>) displaying the Terraform error output.</li> </ul>"},{"location":"api-endpoints/#destroy-all-vms","title":"Destroy All VMs","text":"<p>Endpoint: <code>POST /destroy</code></p> <p>Description: Runs <code>terraform destroy</code> to terminate all EC2 instances and associated resources created by LabLink. It also clears all records from the <code>vms</code> table in the database. This is a destructive action.</p> <p>Authentication: HTTP Basic Auth</p> <p>Request Body: None</p> <p>Success Response:</p> <ul> <li>Code: <code>200 OK</code></li> <li>Content: An HTML page (<code>delete-dashboard.html</code>) displaying the Terraform output.</li> </ul> <p>Error Response:</p> <ul> <li>Code: <code>200 OK</code></li> <li>Content: An HTML page (<code>delete-dashboard.html</code>) displaying the Terraform error output.</li> </ul>"},{"location":"api-endpoints/#download-all-user-data","title":"Download All User Data","text":"<p>Endpoint: <code>GET /api/scp-client</code></p> <p>Description: Connects to each running VM via SSH, finds all files matching the configured <code>extension</code>, copies them to a temporary directory on the allocator, zips them, and provides the zip file for download.</p> <p>Authentication: HTTP Basic Auth</p> <p>Request Body: None</p> <p>Success Response:</p> <ul> <li>Code: <code>200 OK</code></li> <li>Content-Type: <code>application/zip</code></li> <li>Content: A zip file containing the data from all VMs.</li> </ul> <p>Error Response:</p> <ul> <li>Code: <code>404 Not Found</code> if no VMs or no files are found.</li> <li>Code: <code>500 Internal Server Error</code> if an error occurs during the SSH/SCP process.</li> </ul>"},{"location":"api-endpoints/#get-status-of-all-vms","title":"Get Status of All VMs","text":"<p>Endpoint: <code>GET /api/vm-status</code></p> <p>Description: Returns a JSON object mapping each VM hostname to its current status. Used by the admin dashboard.</p> <p>Authentication: None (but intended for admin dashboard)</p> <p>Request Body: None</p> <p>Success Response:</p> <ul> <li>Code: <code>200 OK</code></li> <li>Content: <pre><code>{\n  \"lablink-vm-prod-1\": \"running\",\n  \"lablink-vm-prod-2\": \"initializing\"\n}\n</code></pre></li> </ul> <p>Error Response:</p> <ul> <li>Code: <code>404 Not Found</code> if no VMs are found in the database.</li> <li>Code: <code>500 Internal Server Error</code> on failure.</li> </ul>"},{"location":"api-endpoints/#get-status-of-a-specific-vm","title":"Get Status of a Specific VM","text":"<p>Endpoint: <code>GET /api/vm-status/&lt;hostname&gt;</code></p> <p>Description: Returns the status of a specific VM.</p> <p>Authentication: None (but intended for admin dashboard)</p> <p>URL Parameters:</p> <ul> <li><code>hostname</code> (string, required): The hostname of the VM.   Request Body: None</li> </ul> <p>Success Response:</p> <ul> <li>Code: <code>200 OK</code></li> <li>Content: <pre><code>{\n  \"hostname\": \"lablink-vm-prod-1\",\n  \"status\": \"running\"\n}\n</code></pre></li> </ul> <p>Error Response:</p> <ul> <li>Code: <code>404 Not Found</code> if the VM is not found.</li> <li>Code: <code>500 Internal Server Error</code> on failure.</li> </ul>"},{"location":"api-endpoints/#get-logs-for-a-specific-vm","title":"Get Logs for a Specific VM","text":"<p>Endpoint: <code>GET /api/vm-logs/&lt;hostname&gt;</code></p> <p>Description: Returns the stored logs for a specific VM. Used by the admin log viewer page.</p> <p>Authentication: HTTP Basic Auth (via the <code>/admin/logs/&lt;hostname&gt;</code> page)</p> <p>URL Parameters:</p> <ul> <li><code>hostname</code> (string, required): The hostname of the VM.   Request Body: None</li> </ul> <p>Success Response:</p> <ul> <li>Code: <code>200 OK</code></li> <li>Content: <pre><code>{\n  \"hostname\": \"lablink-vm-prod-1\",\n  \"logs\": \"Starting cloud-init...\\n...\"\n}\n</code></pre></li> </ul> <p>Error Response:</p> <ul> <li>Code: <code>404 Not Found</code> if the VM is not found.</li> <li>Code: <code>503 Service Unavailable</code> if the logs are not yet available because the CloudWatch agent is still being installed.</li> <li>Code: <code>500 Internal Server Error</code> on failure.</li> </ul>"},{"location":"architecture/","title":"Architecture","text":"<p>This page describes LabLink's architecture, components, and how they interact.</p>"},{"location":"architecture/#system-overview","title":"System Overview","text":"graph TB     subgraph GitHub[\"GitHub\"]         SourceCode[Source Code&lt;br/&gt;Repository]         Actions[GitHub Actions&lt;br/&gt;CI/CD]         SourceCode --&gt; Actions     end      subgraph Artifacts[\"Build Artifacts\"]         DockerImages[Docker Images&lt;br/&gt;ghcr.io]         TerraformDeploy[Terraform Apply&lt;br/&gt;Infrastructure]     end      Actions --&gt; DockerImages     Actions --&gt; TerraformDeploy      subgraph AWS[\"AWS Cloud\"]         subgraph AllocatorInstance[\"Allocator EC2 Instance\"]             subgraph AllocatorContainer[\"Docker Container: lablink-allocator\"]                 Flask[Flask App&lt;br/&gt;Port 80&lt;br/&gt;&lt;br/&gt;\u2022 Web UI&lt;br/&gt;\u2022 API&lt;br/&gt;\u2022 Terraform]                 PostgreSQL[(PostgreSQL DB&lt;br/&gt;Port 5432&lt;br/&gt;&lt;br/&gt;\u2022 VM table&lt;br/&gt;\u2022 Triggers&lt;br/&gt;\u2022 Listen/Notify)]                 Flask &lt;--&gt; PostgreSQL             end         end          subgraph ClientInstances[\"Client EC2 Instances (Dynamic)\"]             subgraph ClientContainer[\"Docker Container: lablink-client\"]                 Subscribe[Subscribe Service&lt;br/&gt;&lt;br/&gt;\u2022 Heartbeat&lt;br/&gt;\u2022 GPU Check&lt;br/&gt;\u2022 Status]                 Research[Research Code&lt;br/&gt;User Repo&lt;br/&gt;&lt;br/&gt;\u2022 SLEAP/Custom&lt;br/&gt;\u2022 Your Software]                 Subscribe --&gt; Research             end             Note[Multiple instances,&lt;br/&gt;dynamically created]         end          subgraph AWSResources[\"AWS Resources\"]             SecurityGroups[Security Groups&lt;br/&gt;\u2022 Port 80&lt;br/&gt;\u2022 Port 22&lt;br/&gt;\u2022 Port 5432]             ElasticIPs[Elastic IPs&lt;br/&gt;Static IPs]             S3[S3 Bucket&lt;br/&gt;TF State]         end     end      TerraformDeploy --&gt; AllocatorInstance     DockerImages -.-&gt; AllocatorContainer     DockerImages -.-&gt; ClientContainer     Flask --&gt;|spawns via&lt;br/&gt;Terraform| ClientInstances     Subscribe -.-&gt;|heartbeat,&lt;br/&gt;status updates| Flask      style GitHub fill:#f0f0f0     style Artifacts fill:#e1f5ff     style AWS fill:#fff4e1     style AllocatorInstance fill:#ffe6e6     style AllocatorContainer fill:#fff     style ClientInstances fill:#e6ffe6     style ClientContainer fill:#fff     style AWSResources fill:#f0f0f0     style Flask fill:#4a90e2,color:#fff     style PostgreSQL fill:#336791,color:#fff     style Subscribe fill:#4a90e2,color:#fff     style Research fill:#8bc34a,color:#fff"},{"location":"architecture/#component-details","title":"Component Details","text":""},{"location":"architecture/#allocator-service","title":"Allocator Service","text":"<p>Purpose: Central management server for VM allocation and orchestration.</p> <p>Technology Stack:</p> <ul> <li>Flask: Web application framework</li> <li>PostgreSQL: Relational database for VM state</li> <li>SQLAlchemy: ORM for database operations</li> <li>Terraform: Infrastructure provisioning</li> <li>Docker: Containerization</li> </ul> <p>Key Responsibilities:</p> <ol> <li> <p>Web Interface:</p> </li> <li> <p>Admin dashboard for VM management</p> </li> <li>VM creation interface</li> <li> <p>Instance listing and monitoring</p> </li> <li> <p>API Endpoints:</p> </li> <li> <p><code>/request_vm</code>: Allocate VM to user</p> </li> <li><code>/admin/create</code>: Create new VM instances</li> <li><code>/admin/instances</code>: List all instances</li> <li><code>/admin/destroy</code>: Destroy instances</li> <li> <p><code>/vm_startup</code>: Client registration</p> </li> <li> <p>Database Management:</p> </li> <li> <p>Tracks VM states (available, in-use, failed)</p> </li> <li>PostgreSQL listen/notify for real-time updates</li> <li> <p>Automated triggers for state changes</p> </li> <li> <p>Infrastructure Orchestration:</p> </li> <li>Spawns client VMs via Terraform</li> <li>Manages AWS credentials</li> <li>Handles security group configuration</li> </ol> <p>Configuration: See <code>packages/allocator/src/lablink_allocator/conf/structured_config.py</code></p>"},{"location":"architecture/#client-service","title":"Client Service","text":"<p>Purpose: Runs on dynamically created VMs to execute research workloads.</p> <p>Technology Stack:</p> <ul> <li>Python: Service implementation</li> <li>Docker: Container runtime</li> <li>Custom Software: SLEAP or user-defined</li> </ul> <p>Key Responsibilities:</p> <ol> <li> <p>Health Monitoring:</p> </li> <li> <p>GPU health checks (every 20 seconds)</p> </li> <li>System resource monitoring</li> <li> <p>Reports status to allocator</p> </li> <li> <p>Allocator Communication:</p> </li> <li> <p>Heartbeat mechanism</p> </li> <li>Status updates (in-use, available)</li> <li> <p>Failure reporting</p> </li> <li> <p>Research Execution:</p> </li> <li>Clones configured repository</li> <li>Runs containerized research software</li> <li>Executes user-defined CRD commands</li> </ol> <p>Configuration: See <code>packages/client/src/lablink_client/conf/structured_config.py</code></p>"},{"location":"architecture/#database-schema","title":"Database Schema","text":"<p>Table: <code>vms</code></p> Column Type Description <code>Hostname</code> VARCHAR(255) VM hostname/identifier (Primary Key) <code>UserEmail</code> VARCHAR(255) User email <code>Status</code> VARCHAR(50) VM status (available/in-use/failed) <code>CrdCommand</code> TEXT Command to execute on VM <code>CreatedAt</code> TIMESTAMP Creation timestamp <p>Triggers:</p> <ul> <li><code>notify_vm_update</code>: Sends PostgreSQL NOTIFY on row changes</li> </ul>"},{"location":"architecture/#vm-state-machine","title":"VM State Machine","text":"<p>The <code>status</code> field in the <code>vms</code> table follows this lifecycle:</p> stateDiagram-v2     [*] --&gt; available: VM Created&lt;br/&gt;(terraform apply)      available --&gt; in_use: Software process starts&lt;br/&gt;(detected by update_inuse_status)     in_use --&gt; available: Software process stops&lt;br/&gt;(task complete or crash)      available --&gt; failed: Startup failure&lt;br/&gt;(boot error)     in_use --&gt; failed: Health check failed&lt;br/&gt;(GPU error, system crash)      failed --&gt; available: Admin intervention&lt;br/&gt;(manual reset)     failed --&gt; [*]: VM Destroyed&lt;br/&gt;(terraform destroy)     available --&gt; [*]: VM Destroyed&lt;br/&gt;(terraform destroy)     in_use --&gt; [*]: Force destroy&lt;br/&gt;(admin action)      note right of available         VM ready, waiting         Software not running         Heartbeat active     end note      note right of in_use         Configured software running         User workload active         Sending status updates     end note      note right of failed         Requires attention         Health checks failing         Removed from pool     end note <p>State Transitions:</p> <ul> <li>available \u2192 in_use: Configured software process starts running on the VM</li> <li>in_use \u2192 available: Software process stops (task complete or process ends)</li> <li>available/in_use \u2192 failed: Health checks fail or errors occur</li> <li>failed \u2192 available: Admin manually resets and fixes the VM</li> <li>any \u2192 [*]: VM is destroyed via Terraform</li> </ul> <p>State Mapping to Database Columns</p> State <code>Status</code> column value <code>InUse</code> column value Description### Infrastructure Components available \"running\" <code>False</code> VM is ready and waiting for user workload in_use \"running\" <code>True</code> Configured software is running on VM failed \"failed\" (Any) VM has encountered an error or health check failed initializing \"initializing\" <code>False</code> VM is booting up and not yet ready <p>Note: The <code>in_use</code> status indicates whether the configured software (e.g., SLEAP) is actively running on the VM, not whether a user has been assigned the VM. This is monitored by the <code>update_inuse_status</code> service which checks for the configured process.</p>"},{"location":"architecture/#security-groups","title":"Security Groups","text":"<p>Allocator Security Group:</p> <ul> <li>Port 80 (HTTP): Web interface and API</li> <li>Port 22 (SSH): Administrative access</li> <li>Port 5432 (PostgreSQL): Database connections from clients</li> </ul> <p>Client Security Groups:</p> <ul> <li>Port 22 (SSH): Administrative access</li> <li>Egress: Full internet access for package downloads</li> </ul>"},{"location":"architecture/#networking","title":"Networking","text":"<ul> <li>Elastic IPs: Static IPs for allocators (one per environment)</li> <li>VPC: Default VPC or custom (configurable)</li> <li>Route 53 (Optional): DNS management for friendly URLs</li> </ul>"},{"location":"architecture/#storage","title":"Storage","text":"<ul> <li> <p>S3 Buckets: Terraform state storage</p> </li> <li> <p>Separate state per environment (dev/test/prod)</p> </li> <li>Versioning enabled</li> <li> <p>Encrypted at rest</p> </li> <li> <p>EBS Volumes: Instance root volumes</p> </li> <li>Allocator: 30GB (configurable)</li> <li>Clients: Depends on AMI</li> </ul>"},{"location":"architecture/#data-flow","title":"Data Flow","text":""},{"location":"architecture/#vm-request-flow","title":"VM Request Flow","text":"sequenceDiagram     actor User     participant WebUI as Web UI/API     participant Flask as Flask App     participant DB as PostgreSQL     participant Client as Client VM      User-&gt;&gt;WebUI: Submit VM request     WebUI-&gt;&gt;Flask: POST /request_vm     Flask-&gt;&gt;DB: SELECT * FROM vms&lt;br/&gt;WHERE status='available'&lt;br/&gt;LIMIT 1      alt VM Available         DB--&gt;&gt;Flask: Return VM details         Flask--&gt;&gt;WebUI: Return VM hostname&lt;br/&gt;and connection details         WebUI--&gt;&gt;User: Display VM info         Flask-&gt;&gt;DB: PostgreSQL NOTIFY&lt;br/&gt;vm_update         DB--&gt;&gt;Client: Notify event     else No VM Available         DB--&gt;&gt;Flask: No results         Flask--&gt;&gt;WebUI: Error: No VMs available         WebUI--&gt;&gt;User: Queue request or&lt;br/&gt;show error message     end      Note over Client: Later: update_inuse_status service&lt;br/&gt;monitors for software process     Client-&gt;&gt;Client: Software process starts     Client-&gt;&gt;Flask: Update status to in-use     Flask-&gt;&gt;DB: UPDATE vms&lt;br/&gt;SET status='in-use'"},{"location":"architecture/#vm-creation-flow","title":"VM Creation Flow","text":"sequenceDiagram     actor Admin     participant Flask as Flask App     participant Terraform     participant AWS as AWS EC2     participant VM as Client VM Instance     participant Docker as Docker Container      Admin-&gt;&gt;Flask: POST /admin/create&lt;br/&gt;(instance_count)     Flask-&gt;&gt;Terraform: Execute terraform apply&lt;br/&gt;(subprocess)      Terraform-&gt;&gt;AWS: Create security group     Terraform-&gt;&gt;AWS: Generate SSH key pair     Terraform-&gt;&gt;AWS: Launch EC2 instance&lt;br/&gt;with user_data script     AWS--&gt;&gt;Terraform: Return instance details&lt;br/&gt;(hostname, IP, etc.)     Terraform--&gt;&gt;Flask: Provisioning complete      Note over VM: Boot sequence begins     VM-&gt;&gt;VM: Execute user_data script      VM-&gt;&gt;Docker: Pull Docker image&lt;br/&gt;from ghcr.io     VM-&gt;&gt;VM: Clone user repository&lt;br/&gt;(if configured)     VM-&gt;&gt;Docker: Start client service&lt;br/&gt;(subscribe, check_gpu, etc.)     Docker-&gt;&gt;Flask: POST /vm_startup&lt;br/&gt;(hostname registration)      Flask--&gt;&gt;Admin: VMs created successfully&lt;br/&gt;(show instance details)"},{"location":"architecture/#health-check-flow","title":"Health Check Flow","text":"sequenceDiagram     participant Client as Client VM     participant Flask as Flask App     participant DB as PostgreSQL      Note over Client: Every 20 seconds      loop Health Check Cycle         Client-&gt;&gt;Client: Check GPU status         Client-&gt;&gt;Client: Check system resources          alt GPU/System Healthy             Client-&gt;&gt;Flask: POST /health_check&lt;br/&gt;(status: healthy)             Flask-&gt;&gt;DB: Verify VM record             Flask--&gt;&gt;Client: ACK         else GPU/System Unhealthy             Client-&gt;&gt;Flask: POST /health_check&lt;br/&gt;(status: failed)             Flask-&gt;&gt;DB: UPDATE vms&lt;br/&gt;SET status='failed'             Flask--&gt;&gt;Client: ACK             Note over DB: VM marked as failed&lt;br/&gt;removed from available pool         end     end"},{"location":"architecture/#deployment-environments","title":"Deployment Environments","text":"<p>LabLink supports multiple isolated environments:</p> Environment Purpose Image Tag Terraform Backend <code>dev</code> Local development <code>*-test</code> Local state <code>test</code> Staging/testing <code>*-test</code> <code>backend-test.hcl</code> <code>prod</code> Production Pinned tags <code>backend-prod.hcl</code> <p>Each environment has:</p> <ul> <li>Separate Terraform state</li> <li>Unique resource naming (<code>-dev</code>, <code>-test</code>, <code>-prod</code> suffix)</li> <li>Independent AWS resources</li> </ul>"},{"location":"architecture/#cicd-pipeline","title":"CI/CD Pipeline","text":"<p>See Workflows for detailed CI/CD architecture.</p> <p>Key Workflows:</p> <ol> <li> <p>Build Images (<code>lablink-images.yml</code>):</p> </li> <li> <p>Triggers on code changes</p> </li> <li>Builds allocator and client Docker images</li> <li> <p>Pushes to GitHub Container Registry</p> </li> <li> <p>Terraform Deploy (<code>lablink-allocator-terraform.yml</code>):</p> </li> <li> <p>Triggers on branch push or manual dispatch</p> </li> <li>Applies infrastructure changes</li> <li> <p>Supports environment selection</p> </li> <li> <p>Destroy (<code>lablink-allocator-destroy.yml</code>):</p> </li> <li>Manual trigger only</li> <li>Safely destroys environment resources</li> </ol>"},{"location":"architecture/#security-architecture","title":"Security Architecture","text":"<ul> <li>OIDC Authentication: GitHub Actions authenticate to AWS without stored credentials</li> <li>SSH Keys: Auto-generated per environment, ephemeral artifacts</li> <li>Secrets: Managed via GitHub Secrets and AWS Secrets Manager</li> <li>Network: Security groups restrict access by port and source</li> </ul> <p>See Security for detailed security considerations.</p>"},{"location":"architecture/#scalability-considerations","title":"Scalability Considerations","text":"<p>Current Architecture:</p> <ul> <li>Single allocator per environment</li> <li>Multiple clients per allocator</li> <li>Database handles concurrent requests</li> </ul> <p>Scaling Options:</p> <ul> <li>Horizontal: Multiple allocators with load balancer</li> <li>Vertical: Larger instance types for allocator</li> <li>Database: RDS for managed PostgreSQL at scale</li> </ul>"},{"location":"architecture/#technology-choices","title":"Technology Choices","text":"Component Technology Rationale Web Framework Flask Lightweight, Python ecosystem Database PostgreSQL LISTEN/NOTIFY, ACID compliance IaC Terraform Declarative, AWS support Containers Docker Portability, dependency isolation CI/CD GitHub Actions Native GitHub integration Config Hydra/OmegaConf Structured configs, easy overrides"},{"location":"architecture/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration: Customize components</li> <li>Deployment: Deploy the system</li> <li>Testing: Test the codebase</li> </ul>"},{"location":"aws-setup/","title":"AWS Setup from Scratch","text":"<p>This comprehensive guide walks you through setting up all required AWS resources for LabLink deployment from scratch.</p>"},{"location":"aws-setup/#overview","title":"Overview","text":"<p>To deploy LabLink, you'll need:</p> <ol> <li>AWS account with appropriate permissions</li> <li>S3 bucket for Terraform state</li> <li>Elastic IPs for each environment</li> <li>IAM role for GitHub Actions (OIDC)</li> <li>Optional: Route 53 hosted zone for DNS</li> </ol>"},{"location":"aws-setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>AWS account with admin access (or appropriate IAM permissions)</li> <li>AWS CLI installed and configured</li> <li>Basic understanding of AWS services</li> <li>Chosen AWS region for deployment</li> </ul> <p>Paid AWS Account Required</p> <p>LabLink deployment uses t3.micro instances for the allocator, which requires a paid AWS account. The AWS Free Tier has limitations that may prevent successful deployment. Ensure your AWS account has billing enabled and a valid payment method configured.</p>"},{"location":"aws-setup/#choosing-an-aws-region","title":"Choosing an AWS Region","text":"<p>Before starting, select the AWS region where you'll deploy LabLink. This is an important decision that affects performance, cost, and compliance.</p>"},{"location":"aws-setup/#region-selection-criteria","title":"Region Selection Criteria","text":"<p>1. Latency &amp; Geographic Proximity - Choose a region closest to your users for best performance - Lower latency = better user experience for VM access - Test latency: <code>ping ec2.{region}.amazonaws.com</code></p> <p>2. Instance Availability - Not all instance types are available in all regions - GPU instances (g4dn, g5, p3) have limited regional availability - Check availability: AWS Regional Services</p> <p>3. Pricing - EC2 pricing varies by region (5-30% difference) - US regions are typically cheaper than EU/Asia - Check pricing: EC2 Pricing Calculator</p> <p>4. Compliance &amp; Data Residency - GDPR (Europe): Use <code>eu-west-1</code>, <code>eu-central-1</code> - HIPAA (US Healthcare): Any US region with BAA - Data sovereignty requirements may mandate specific regions</p> <p>5. Service Availability - All LabLink features require: EC2, VPC, S3, Route 53 - These are available in all commercial regions</p>"},{"location":"aws-setup/#recommended-regions","title":"Recommended Regions","text":"Region Code Best For Notes US East (N. Virginia) <code>us-east-1</code> US East Coast, lowest cost Largest region, occasional availability issues US West (Oregon) <code>us-west-2</code> US West Coast, default Good balance of cost and stability Europe (Ireland) <code>eu-west-1</code> Europe, GDPR Best EU region for cost and availability Asia Pacific (Tokyo) <code>ap-northeast-1</code> Asia Good for Asian users Asia Pacific (Singapore) <code>ap-southeast-1</code> Southeast Asia Alternative for Asian users"},{"location":"aws-setup/#list-all-available-regions","title":"List All Available Regions","text":"<pre><code># AWS CLI\naws ec2 describe-regions --output table\n\n# Or with region names\naws ec2 describe-regions --query \"Regions[*].[RegionName,OptInStatus]\" --output table\n</code></pre>"},{"location":"aws-setup/#test-latency-to-regions","title":"Test Latency to Regions","text":"<pre><code># Test ping to various regions (macOS/Linux)\nfor region in us-east-1 us-west-2 eu-west-1 ap-northeast-1; do\n  echo -n \"$region: \"\n  ping -c 3 ec2.$region.amazonaws.com | grep avg | awk -F'/' '{print $5 \" ms\"}'\ndone\n</code></pre>"},{"location":"aws-setup/#configure-your-region-choice","title":"Configure Your Region Choice","text":"<p>Once you've selected a region, you'll need to configure it in two places:</p> <ol> <li>GitHub Secret: <code>AWS_REGION</code> (covered in Step 4.6)</li> <li>config.yaml: Must match the secret (covered later)</li> </ol> <p>Example:</p> <pre><code># lablink-infrastructure/config/config.yaml\napp:\n  region: \"us-west-2\" # Must match AWS_REGION secret\n</code></pre>"},{"location":"aws-setup/#ec2-service-quotas","title":"EC2 Service Quotas","text":"<p>Before launching multiple instances, especially GPU instances, you may need to request quota increases from AWS. New AWS accounts typically have low default quotas that can prevent mass instance launches.</p>"},{"location":"aws-setup/#why-quotas-matter","title":"Why Quotas Matter","text":"<p>AWS limits the number of vCPUs you can run concurrently for each instance type family. For example:</p> <ul> <li>On-Demand Standard instances (t3, m5, c5): Default ~5-20 vCPUs</li> <li>On-Demand G and VT instances (g4dn, g5): Default 0-4 vCPUs</li> <li>On-Demand P instances (p3, p4d): Default 0 vCPUs</li> <li>Spot instances: Separate quotas per instance family</li> </ul> <p>GPU Instance Quotas</p> <p>GPU instance types (g4dn, g5, p3, p4d) often have zero default quota for new accounts. You must request a quota increase before launching any GPU instances.</p>"},{"location":"aws-setup/#check-your-current-quotas","title":"Check Your Current Quotas","text":""},{"location":"aws-setup/#aws-cli","title":"AWS CLI","text":"<pre><code># List all EC2 quotas\naws service-quotas list-service-quotas \\\n  --service-code ec2 \\\n  --query 'Quotas[?starts_with(QuotaName, `Running On-Demand`)].{Name:QuotaName,Value:Value}' \\\n  --output table\n\n# Check specific quota (e.g., G and VT instances)\naws service-quotas get-service-quota \\\n  --service-code ec2 \\\n  --quota-code L-DB2E81BA \\\n  --query 'Quota.{Name:QuotaName,Value:Value}' \\\n  --output table\n</code></pre>"},{"location":"aws-setup/#aws-console","title":"AWS Console","text":"<ol> <li>Go to Service Quotas \u2192 AWS services \u2192 Amazon Elastic Compute Cloud (Amazon EC2)</li> <li>Search for \"Running On-Demand\"</li> <li>Review quotas for instance families you plan to use</li> </ol>"},{"location":"aws-setup/#common-quota-codes","title":"Common Quota Codes","text":"Instance Family Quota Code Description Standard (t3, m5, c5) <code>L-1216C47A</code> Running On-Demand Standard instances G and VT (g4dn, g5) <code>L-DB2E81BA</code> Running On-Demand G and VT instances P (p3, p4d) <code>L-417A185B</code> Running On-Demand P instances Spot Standard <code>L-34B43A08</code> All Standard Spot Instance Requests Spot G and VT <code>L-3819A6DF</code> All G and VT Spot Instance Requests"},{"location":"aws-setup/#request-a-quota-increase","title":"Request a Quota Increase","text":""},{"location":"aws-setup/#aws-cli_1","title":"AWS CLI","text":"<pre><code># Request quota increase (e.g., 64 vCPUs for G and VT instances)\naws service-quotas request-service-quota-increase \\\n  --service-code ec2 \\\n  --quota-code L-DB2E81BA \\\n  --desired-value 64\n</code></pre>"},{"location":"aws-setup/#aws-console_1","title":"AWS Console","text":"<ol> <li>Go to Service Quotas \u2192 AWS services \u2192 Amazon EC2</li> <li>Find the quota you need to increase</li> <li>Click Request increase at account level</li> <li>Enter desired value and submit</li> </ol> <p>Quota Increase Tips</p> <ul> <li>Request increases before you need them\u2014approval can take 1-3 business days</li> <li>Start with modest increases (e.g., 32-64 vCPUs) for faster approval</li> <li>Provide a use case description to expedite approval</li> <li>Quotas are per-region\u2014request increases in each region you'll use</li> </ul>"},{"location":"aws-setup/#calculating-required-vcpus","title":"Calculating Required vCPUs","text":"<p>To determine how many vCPUs you need, multiply the number of instances by vCPUs per instance:</p> Instance Type vCPUs 10 Instances = g4dn.xlarge 4 40 vCPUs g4dn.2xlarge 8 80 vCPUs g5.xlarge 4 40 vCPUs g5.2xlarge 8 80 vCPUs p3.2xlarge 8 80 vCPUs <p>Example: To run 20 <code>g4dn.xlarge</code> instances concurrently, you need a quota of at least 80 vCPUs for \"Running On-Demand G and VT instances\".</p>"},{"location":"aws-setup/#step-1-iam-permissions-setup","title":"Step 1: IAM Permissions Setup","text":""},{"location":"aws-setup/#11-create-iam-user-if-needed","title":"1.1 Create IAM User (If Needed)","text":"<p>If you don't have an IAM user with sufficient permissions:</p> <pre><code>aws iam create-user --user-name lablink-admin\n</code></pre>"},{"location":"aws-setup/#12-attach-required-policies","title":"1.2 Attach Required Policies","text":"<p>Attach these managed policies for LabLink operations:</p> <pre><code>aws iam attach-user-policy \\\n  --user-name lablink-admin \\\n  --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess\n\naws iam attach-user-policy \\\n  --user-name lablink-admin \\\n  --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess\n\naws iam attach-user-policy \\\n  --user-name lablink-admin \\\n  --policy-arn arn:aws:iam::aws:policy/IAMFullAccess\n</code></pre>"},{"location":"aws-setup/#13-create-access-keys","title":"1.3 Create Access Keys","text":"<p>For local Terraform usage:</p> <pre><code>aws iam create-access-key --user-name lablink-admin\n</code></pre> <p>Save the <code>AccessKeyId</code> and <code>SecretAccessKey</code> securely.</p> <p>Configure AWS CLI:</p> <pre><code>aws configure\n# Enter AccessKeyId\n# Enter SecretAccessKey\n# Enter region (e.g., us-west-2)\n# Enter output format (json)\n</code></pre>"},{"location":"aws-setup/#step-2-s3-bucket-for-terraform-state","title":"Step 2: S3 Bucket for Terraform State","text":""},{"location":"aws-setup/#21-create-s3-bucket","title":"2.1 Create S3 Bucket","text":"<p>Choose a globally unique bucket name:</p> <pre><code>export BUCKET_NAME=\"tf-state-lablink-allocator-bucket-$(date +%s)\"\n\naws s3api create-bucket \\\n  --bucket $BUCKET_NAME \\\n  --region us-west-2 \\\n  --create-bucket-configuration LocationConstraint=us-west-2\n</code></pre>"},{"location":"aws-setup/#22-enable-versioning","title":"2.2 Enable Versioning","text":"<p>Protect against accidental deletions:</p> <pre><code>aws s3api put-bucket-versioning \\\n  --bucket $BUCKET_NAME \\\n  --versioning-configuration Status=Enabled\n</code></pre>"},{"location":"aws-setup/#23-enable-encryption","title":"2.3 Enable Encryption","text":"<p>Encrypt state files at rest:</p> <pre><code>aws s3api put-bucket-encryption \\\n  --bucket $BUCKET_NAME \\\n  --server-side-encryption-configuration '{\n    \"Rules\": [{\n      \"ApplyServerSideEncryptionByDefault\": {\n        \"SSEAlgorithm\": \"AES256\"\n      }\n    }]\n  }'\n</code></pre>"},{"location":"aws-setup/#24-block-public-access","title":"2.4 Block Public Access","text":"<p>Ensure bucket is private:</p> <pre><code>aws s3api put-public-access-block \\\n  --bucket $BUCKET_NAME \\\n  --public-access-block-configuration \\\n    BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true\n</code></pre>"},{"location":"aws-setup/#25-update-configuration","title":"2.5 Update Configuration","text":"<p>Update bucket name in your LabLink configuration:</p> <p><code>lablink-infrastructure/config/config.yaml</code>:</p> <pre><code>bucket_name: \"tf-state-lablink-allocator-bucket-1234567890\"\n</code></pre> <p><code>lablink-infrastructure/backend-test.hcl</code> and <code>backend-prod.hcl</code>:</p> <pre><code>bucket = \"tf-state-lablink-allocator-bucket-1234567890\"\nkey    = \"lablink-allocator-&lt;env&gt;/terraform.tfstate\"\nregion = \"us-west-2\"\n</code></pre>"},{"location":"aws-setup/#step-3-elastic-ip-allocation","title":"Step 3: Elastic IP Allocation","text":"<p>Allocate static IPs for test and production environments.</p>"},{"location":"aws-setup/#31-allocate-elastic-ips","title":"3.1 Allocate Elastic IPs","text":"<pre><code># Test environment\naws ec2 allocate-address --region us-west-2 --tag-specifications \\\n  'ResourceType=elastic-ip,Tags=[{Key=Environment,Value=test},{Key=Project,Value=lablink}]'\n\n# Production environment\naws ec2 allocate-address --region us-west-2 --tag-specifications \\\n  'ResourceType=elastic-ip,Tags=[{Key=Environment,Value=prod},{Key=Project,Value=lablink}]'\n</code></pre>"},{"location":"aws-setup/#32-record-allocation-ids","title":"3.2 Record Allocation IDs","text":"<p>Save the <code>AllocationId</code> from each command output (format: <code>eipalloc-xxxxx</code>).</p>"},{"location":"aws-setup/#33-tag-elastic-ips","title":"3.3 Tag Elastic IPs","text":"<pre><code>aws ec2 create-tags \\\n  --resources eipalloc-test-xxxxx \\\n  --tags Key=Name,Value=lablink-test-eip\n\naws ec2 create-tags \\\n  --resources eipalloc-prod-xxxxx \\\n  --tags Key=Name,Value=lablink-prod-eip\n</code></pre>"},{"location":"aws-setup/#34-update-terraform-configuration","title":"3.4 Update Terraform Configuration","text":"<p><code>lablink-infrastructure/main.tf</code>:</p> <pre><code>variable \"allocated_eip\" {\n  description = \"Pre-allocated Elastic IP for production/test\"\n  type        = string\n  default     = \"\"\n}\n\nresource \"aws_eip_association\" \"lablink\" {\n  count         = var.allocated_eip != \"\" ? 1 : 0\n  instance_id   = aws_instance.lablink_allocator.id\n  allocation_id = var.allocated_eip\n}\n</code></pre> <p>Use when deploying:</p> <pre><code>terraform apply \\\n  -var=\"resource_suffix=test\" \\\n  -var=\"allocated_eip=eipalloc-test-xxxxx\"\n</code></pre>"},{"location":"aws-setup/#step-4-github-actions-oidc-configuration","title":"Step 4: GitHub Actions OIDC Configuration","text":"<p>Set up OpenID Connect (OIDC) for GitHub Actions to authenticate to AWS without storing long-term credentials. This is the recommended and most secure method for CI/CD authentication.</p>"},{"location":"aws-setup/#41-check-for-existing-oidc-provider","title":"4.1: Check for Existing OIDC Provider","text":"<p>Before creating a new OIDC provider, check if one already exists:</p>"},{"location":"aws-setup/#aws-cli_2","title":"AWS CLI","text":"<pre><code># Check your current AWS account\naws sts get-caller-identity\n\n# List OIDC providers\naws iam list-open-id-connect-providers\n</code></pre> <p>Look for a provider with URL <code>token.actions.githubusercontent.com</code>.</p>"},{"location":"aws-setup/#aws-console_2","title":"AWS Console","text":"<ol> <li>Go to IAM \u2192 Identity providers</li> <li>Look for provider with URL <code>token.actions.githubusercontent.com</code></li> <li>If it exists, note the ARN and skip to Step 4.2</li> </ol>"},{"location":"aws-setup/#42-create-oidc-provider-if-needed","title":"4.2: Create OIDC Provider (If Needed)","text":""},{"location":"aws-setup/#aws-cli_3","title":"AWS CLI","text":"<pre><code>aws iam create-open-id-connect-provider \\\n  --url https://token.actions.githubusercontent.com \\\n  --client-id-list sts.amazonaws.com \\\n  --thumbprint-list 6938fd4d98bab03faadb97b34396831e3780aea1\n</code></pre> <p>Note: If you get <code>EntityAlreadyExists</code> error, the provider already exists. You can proceed to create the IAM role.</p>"},{"location":"aws-setup/#aws-console_3","title":"AWS Console","text":"<ol> <li>Go to IAM \u2192 Identity providers</li> <li>Click Add provider</li> <li>Select OpenID Connect</li> <li>Provider URL: <code>https://token.actions.githubusercontent.com</code></li> <li>Click Get thumbprint (should show <code>6938fd4d98bab03faadb97b34396831e3780aea1</code>)</li> <li>Audience: <code>sts.amazonaws.com</code></li> <li>Click Add provider</li> </ol>"},{"location":"aws-setup/#43-create-iam-role-for-github-actions","title":"4.3: Create IAM Role for GitHub Actions","text":""},{"location":"aws-setup/#option-a-aws-cli-recommended-for-multiple-repositories","title":"Option A: AWS CLI (Recommended for Multiple Repositories)","text":"<p>Step 1: Get your AWS account ID:</p> <pre><code>aws sts get-caller-identity --query \"Account\" --output text\n</code></pre> <p>Step 2: Create trust policy file <code>github-trust-policy.json</code>:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Federated\": \"arn:aws:iam::YOUR_ACCOUNT_ID:oidc-provider/token.actions.githubusercontent.com\"\n      },\n      \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"token.actions.githubusercontent.com:aud\": \"sts.amazonaws.com\"\n        },\n        \"StringLike\": {\n          \"token.actions.githubusercontent.com:sub\": [\n            \"repo:YOUR_ORG/lablink:*\",\n            \"repo:YOUR_ORG/lablink-template:*\",\n            \"repo:YOUR_ORG/sleap-lablink:*\"\n          ]\n        }\n      }\n    }\n  ]\n}\n</code></pre> <p>Important: Replace:</p> <ul> <li><code>YOUR_ACCOUNT_ID</code> with your AWS account ID (from Step 1)</li> <li><code>YOUR_ORG</code> with your GitHub organization/username (e.g., <code>talmolab</code>)</li> </ul> <p>Step 3: Create the IAM role:</p> <pre><code>aws iam create-role \\\n  --role-name GitHubActionsLabLinkRole \\\n  --assume-role-policy-document file://github-trust-policy.json \\\n  --description \"Role for GitHub Actions to deploy LabLink infrastructure\"\n</code></pre> <p>Step 4: Note the role ARN from the output (format: <code>arn:aws:iam::ACCOUNT_ID:role/GitHubActionsLabLinkRole</code>)</p>"},{"location":"aws-setup/#option-b-aws-console","title":"Option B: AWS Console","text":"<p>Step 1: Create the role</p> <ol> <li>Go to IAM \u2192 Roles</li> <li>Click Create role</li> <li>Select Web identity as trusted entity type</li> <li>Choose:</li> <li>Identity provider: <code>token.actions.githubusercontent.com</code></li> <li>Audience: <code>sts.amazonaws.com</code></li> <li>Click Next</li> </ol> <p>Step 2: Skip permissions for now (we'll add them in Step 4.4)</p> <ol> <li>Click Next</li> <li>Role name: <code>GitHubActionsLabLinkRole</code></li> <li>Description: <code>Role for GitHub Actions to deploy LabLink infrastructure</code></li> <li>Click Create role</li> </ol> <p>Step 3: Edit trust policy for multiple repositories</p> <ol> <li>Click on the newly created role</li> <li>Go to Trust relationships tab</li> <li>Click Edit trust policy</li> <li>Replace the trust policy with:</li> </ol> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Federated\": \"arn:aws:iam::YOUR_ACCOUNT_ID:oidc-provider/token.actions.githubusercontent.com\"\n      },\n      \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"token.actions.githubusercontent.com:aud\": \"sts.amazonaws.com\"\n        },\n        \"StringLike\": {\n          \"token.actions.githubusercontent.com:sub\": [\n            \"repo:YOUR_ORG/lablink:*\",\n            \"repo:YOUR_ORG/lablink-template:*\",\n            \"repo:YOUR_ORG/sleap-lablink:*\"\n          ]\n        }\n      }\n    }\n  ]\n}\n</code></pre> <ol> <li>Replace <code>YOUR_ACCOUNT_ID</code> and <code>YOUR_ORG</code> with your values</li> <li>Click Update policy</li> </ol>"},{"location":"aws-setup/#44-attach-permissions-to-role","title":"4.4: Attach Permissions to Role","text":"<p>The role needs permissions to manage EC2, S3, Route53, IAM, and other AWS resources for infrastructure deployment.</p>"},{"location":"aws-setup/#option-a-aws-cli-use-aws-managed-policies-recommended","title":"Option A: AWS CLI - Use AWS Managed Policies (Recommended)","text":"<p>Attach multiple AWS managed policies to provide the required permissions:</p> <pre><code># Core infrastructure permissions\naws iam attach-role-policy \\\n  --role-name GitHubActionsLabLinkRole \\\n  --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess\n\naws iam attach-role-policy \\\n  --role-name GitHubActionsLabLinkRole \\\n  --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess\n\naws iam attach-role-policy \\\n  --role-name GitHubActionsLabLinkRole \\\n  --policy-arn arn:aws:iam::aws:policy/IAMFullAccess\n\naws iam attach-role-policy \\\n  --role-name GitHubActionsLabLinkRole \\\n  --policy-arn arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess\n\n# Monitoring and logging permissions\naws iam attach-role-policy \\\n  --role-name GitHubActionsLabLinkRole \\\n  --policy-arn arn:aws:iam::aws:policy/CloudWatchLogsFullAccess\n\naws iam attach-role-policy \\\n  --role-name GitHubActionsLabLinkRole \\\n  --policy-arn arn:aws:iam::aws:policy/AWSCloudTrail_FullAccess\n\n# Lambda and notifications permissions\naws iam attach-role-policy \\\n  --role-name GitHubActionsLabLinkRole \\\n  --policy-arn arn:aws:iam::aws:policy/AWSLambda_FullAccess\n\naws iam attach-role-policy \\\n  --role-name GitHubActionsLabLinkRole \\\n  --policy-arn arn:aws:iam::aws:policy/AmazonSNSFullAccess\n</code></pre> <p>Note: This approach uses 8 AWS managed policies that provide full access to the required services. While broader than strictly necessary, it ensures all Terraform operations succeed without permission errors.</p> Policy Purpose <code>AmazonEC2FullAccess</code> EC2 instances, security groups, key pairs, EIPs <code>AmazonS3FullAccess</code> Terraform state, CloudTrail logs <code>IAMFullAccess</code> Roles, instance profiles for CloudWatch/CloudTrail <code>AmazonDynamoDBFullAccess</code> Terraform state locking <code>CloudWatchLogsFullAccess</code> Log groups, metric filters, alarms <code>AWSCloudTrail_FullAccess</code> Audit logging <code>AWSLambda_FullAccess</code> Log processing functions <code>AmazonSNSFullAccess</code> Alert notifications"},{"location":"aws-setup/#option-b-aws-cli-create-custom-policy","title":"Option B: AWS CLI - Create Custom Policy","text":"<p>For more restrictive permissions, create a custom policy that covers all LabLink Terraform resources including EC2, ALB, CloudTrail, CloudWatch, Lambda, SNS, and Budgets.</p> <p>Create <code>lablink-terraform-policy.json</code>:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"TerraformStateManagement\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n        \"s3:DeleteObject\",\n        \"s3:ListBucket\",\n        \"s3:GetBucketVersioning\",\n        \"s3:GetBucketPolicy\",\n        \"s3:GetBucketAcl\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::lablink-terraform-state-*\",\n        \"arn:aws:s3:::lablink-terraform-state-*/*\",\n        \"arn:aws:s3:::tf-state-lablink-*\",\n        \"arn:aws:s3:::tf-state-lablink-*/*\"\n      ]\n    },\n    {\n      \"Sid\": \"DynamoDBStateLocking\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"dynamodb:GetItem\",\n        \"dynamodb:PutItem\",\n        \"dynamodb:DeleteItem\",\n        \"dynamodb:DescribeTable\"\n      ],\n      \"Resource\": \"arn:aws:dynamodb:*:*:table/lock-table\"\n    },\n    {\n      \"Sid\": \"EC2FullAccess\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\"ec2:*\"],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Sid\": \"IAMRolesAndInstanceProfiles\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"iam:CreateRole\",\n        \"iam:DeleteRole\",\n        \"iam:GetRole\",\n        \"iam:TagRole\",\n        \"iam:UntagRole\",\n        \"iam:PassRole\",\n        \"iam:AttachRolePolicy\",\n        \"iam:DetachRolePolicy\",\n        \"iam:PutRolePolicy\",\n        \"iam:DeleteRolePolicy\",\n        \"iam:GetRolePolicy\",\n        \"iam:CreateInstanceProfile\",\n        \"iam:DeleteInstanceProfile\",\n        \"iam:GetInstanceProfile\",\n        \"iam:AddRoleToInstanceProfile\",\n        \"iam:RemoveRoleFromInstanceProfile\",\n        \"iam:ListInstanceProfilesForRole\",\n        \"iam:ListAttachedRolePolicies\",\n        \"iam:ListRolePolicies\"\n      ],\n      \"Resource\": [\n        \"arn:aws:iam::*:role/lablink*\",\n        \"arn:aws:iam::*:role/lablink_*\",\n        \"arn:aws:iam::*:instance-profile/lablink*\",\n        \"arn:aws:iam::*:instance-profile/lablink_*\"\n      ]\n    },\n    {\n      \"Sid\": \"Route53DNS\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"route53:ListHostedZones\",\n        \"route53:GetHostedZone\",\n        \"route53:ListResourceRecordSets\",\n        \"route53:ChangeResourceRecordSets\",\n        \"route53:GetChange\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Sid\": \"ApplicationLoadBalancer\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"elasticloadbalancing:CreateLoadBalancer\",\n        \"elasticloadbalancing:DeleteLoadBalancer\",\n        \"elasticloadbalancing:DescribeLoadBalancers\",\n        \"elasticloadbalancing:DescribeLoadBalancerAttributes\",\n        \"elasticloadbalancing:ModifyLoadBalancerAttributes\",\n        \"elasticloadbalancing:CreateTargetGroup\",\n        \"elasticloadbalancing:DeleteTargetGroup\",\n        \"elasticloadbalancing:DescribeTargetGroups\",\n        \"elasticloadbalancing:DescribeTargetGroupAttributes\",\n        \"elasticloadbalancing:ModifyTargetGroupAttributes\",\n        \"elasticloadbalancing:RegisterTargets\",\n        \"elasticloadbalancing:DeregisterTargets\",\n        \"elasticloadbalancing:DescribeTargetHealth\",\n        \"elasticloadbalancing:CreateListener\",\n        \"elasticloadbalancing:DeleteListener\",\n        \"elasticloadbalancing:DescribeListeners\",\n        \"elasticloadbalancing:ModifyListener\",\n        \"elasticloadbalancing:AddTags\",\n        \"elasticloadbalancing:RemoveTags\",\n        \"elasticloadbalancing:DescribeTags\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Sid\": \"ACMCertificates\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"acm:DescribeCertificate\",\n        \"acm:ListCertificates\",\n        \"acm:GetCertificate\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Sid\": \"CloudWatchLogsAndAlarms\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:CreateLogGroup\",\n        \"logs:DeleteLogGroup\",\n        \"logs:DescribeLogGroups\",\n        \"logs:PutRetentionPolicy\",\n        \"logs:DeleteRetentionPolicy\",\n        \"logs:CreateLogStream\",\n        \"logs:DeleteLogStream\",\n        \"logs:PutLogEvents\",\n        \"logs:PutMetricFilter\",\n        \"logs:DeleteMetricFilter\",\n        \"logs:DescribeMetricFilters\",\n        \"logs:PutSubscriptionFilter\",\n        \"logs:DeleteSubscriptionFilter\",\n        \"logs:DescribeSubscriptionFilters\",\n        \"logs:TagResource\",\n        \"logs:UntagResource\",\n        \"logs:ListTagsForResource\",\n        \"cloudwatch:PutMetricAlarm\",\n        \"cloudwatch:DeleteAlarms\",\n        \"cloudwatch:DescribeAlarms\",\n        \"cloudwatch:EnableAlarmActions\",\n        \"cloudwatch:DisableAlarmActions\",\n        \"cloudwatch:TagResource\",\n        \"cloudwatch:UntagResource\",\n        \"cloudwatch:ListTagsForResource\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Sid\": \"CloudTrail\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"cloudtrail:CreateTrail\",\n        \"cloudtrail:DeleteTrail\",\n        \"cloudtrail:DescribeTrails\",\n        \"cloudtrail:GetTrailStatus\",\n        \"cloudtrail:StartLogging\",\n        \"cloudtrail:StopLogging\",\n        \"cloudtrail:UpdateTrail\",\n        \"cloudtrail:PutEventSelectors\",\n        \"cloudtrail:GetEventSelectors\",\n        \"cloudtrail:AddTags\",\n        \"cloudtrail:RemoveTags\",\n        \"cloudtrail:ListTags\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Sid\": \"CloudTrailS3Bucket\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:CreateBucket\",\n        \"s3:DeleteBucket\",\n        \"s3:PutBucketPolicy\",\n        \"s3:DeleteBucketPolicy\",\n        \"s3:GetBucketPolicy\",\n        \"s3:PutBucketAcl\",\n        \"s3:GetBucketAcl\",\n        \"s3:PutEncryptionConfiguration\",\n        \"s3:GetEncryptionConfiguration\",\n        \"s3:PutBucketVersioning\",\n        \"s3:GetBucketVersioning\",\n        \"s3:PutBucketPublicAccessBlock\",\n        \"s3:GetBucketPublicAccessBlock\",\n        \"s3:PutLifecycleConfiguration\",\n        \"s3:GetLifecycleConfiguration\",\n        \"s3:PutBucketTagging\",\n        \"s3:GetBucketTagging\",\n        \"s3:ListBucket\",\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n        \"s3:DeleteObject\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::lablink-cloudtrail-*\",\n        \"arn:aws:s3:::lablink-cloudtrail-*/*\"\n      ]\n    },\n    {\n      \"Sid\": \"Lambda\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"lambda:CreateFunction\",\n        \"lambda:DeleteFunction\",\n        \"lambda:GetFunction\",\n        \"lambda:GetFunctionConfiguration\",\n        \"lambda:UpdateFunctionCode\",\n        \"lambda:UpdateFunctionConfiguration\",\n        \"lambda:AddPermission\",\n        \"lambda:RemovePermission\",\n        \"lambda:GetPolicy\",\n        \"lambda:InvokeFunction\",\n        \"lambda:TagResource\",\n        \"lambda:UntagResource\",\n        \"lambda:ListTags\"\n      ],\n      \"Resource\": \"arn:aws:lambda:*:*:function:lablink*\"\n    },\n    {\n      \"Sid\": \"SNSNotifications\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"sns:CreateTopic\",\n        \"sns:DeleteTopic\",\n        \"sns:GetTopicAttributes\",\n        \"sns:SetTopicAttributes\",\n        \"sns:Subscribe\",\n        \"sns:Unsubscribe\",\n        \"sns:ListSubscriptionsByTopic\",\n        \"sns:Publish\",\n        \"sns:TagResource\",\n        \"sns:UntagResource\",\n        \"sns:ListTagsForResource\"\n      ],\n      \"Resource\": \"arn:aws:sns:*:*:lablink*\"\n    },\n    {\n      \"Sid\": \"Budgets\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"budgets:ViewBudget\",\n        \"budgets:CreateBudgetAction\",\n        \"budgets:DeleteBudgetAction\",\n        \"budgets:UpdateBudgetAction\",\n        \"budgets:ExecuteBudgetAction\",\n        \"budgets:ModifyBudget\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n</code></pre> <p>Note: This policy covers all AWS services used by the LabLink Terraform configuration:</p> Service Purpose S3 Terraform state storage, CloudTrail logs DynamoDB Terraform state locking EC2 Allocator and client VM instances, security groups, key pairs, EIPs IAM Instance profiles, CloudWatch agent roles, CloudTrail roles Route53 DNS records for allocator endpoints ELB Application Load Balancer for HTTPS termination ACM SSL/TLS certificates CloudWatch Logs, metric filters, alarms for monitoring CloudTrail Audit logging and compliance Lambda Log processing functions SNS Alert notifications Budgets Cost monitoring and alerts <p>Attach the custom policy:</p> <pre><code>aws iam put-role-policy \\\n  --role-name GitHubActionsLabLinkRole \\\n  --policy-name LabLinkTerraformPolicy \\\n  --policy-document file://lablink-terraform-policy.json\n</code></pre>"},{"location":"aws-setup/#option-c-aws-console","title":"Option C: AWS Console","text":"<ol> <li>Go to IAM \u2192 Roles</li> <li>Click on <code>GitHubActionsLabLinkRole</code></li> <li>Click Add permissions \u2192 Attach policies</li> <li> <p>Search for and attach each of these policies:</p> <ul> <li><code>AmazonEC2FullAccess</code></li> <li><code>AmazonS3FullAccess</code></li> <li><code>IAMFullAccess</code></li> <li><code>AmazonDynamoDBFullAccess</code></li> <li><code>CloudWatchLogsFullAccess</code></li> <li><code>AWSCloudTrail_FullAccess</code></li> <li><code>AWSLambda_FullAccess</code></li> <li><code>AmazonSNSFullAccess</code></li> </ul> </li> <li> <p>Click Add permissions after selecting each policy</p> </li> </ol>"},{"location":"aws-setup/#45-verify-role-configuration","title":"4.5: Verify Role Configuration","text":""},{"location":"aws-setup/#aws-cli_4","title":"AWS CLI","text":"<p>Check the role exists and has correct trust policy:</p> <pre><code># Get role details\naws iam get-role --role-name GitHubActionsLabLinkRole\n\n# Check trust policy\naws iam get-role --role-name GitHubActionsLabLinkRole \\\n  --query \"Role.AssumeRolePolicyDocument\" --output json\n\n# List attached policies\naws iam list-attached-role-policies --role-name GitHubActionsLabLinkRole\n\n# List inline policies\naws iam list-role-policies --role-name GitHubActionsLabLinkRole\n</code></pre> <p>Verify the trust policy includes all your deployment repositories.</p>"},{"location":"aws-setup/#aws-console_4","title":"AWS Console","text":"<ol> <li>Go to IAM \u2192 Roles \u2192 <code>GitHubActionsLabLinkRole</code></li> <li>Trust relationships tab: Verify repositories are listed</li> <li>Permissions tab: Verify required managed policies or custom policy is attached</li> <li>Copy the ARN (e.g., <code>arn:aws:iam::711387140753:role/GitHubActionsLabLinkRole</code>)</li> </ol>"},{"location":"aws-setup/#46-add-github-secrets","title":"4.6: Add GitHub Secrets","text":"<p>Four secrets are required for GitHub Actions workflows to deploy infrastructure securely.</p>"},{"location":"aws-setup/#for-template-repository-lablink-template","title":"For Template Repository (<code>lablink-template</code>)","text":"<ol> <li>Go to repository Settings \u2192 Secrets and variables \u2192 Actions</li> <li>Add AWS_ROLE_ARN secret:<ul> <li>Click New repository secret</li> <li>Name: <code>AWS_ROLE_ARN</code></li> <li>Value: <code>arn:aws:iam::YOUR_ACCOUNT_ID:role/GitHubActionsLabLinkRole</code></li> <li>Click Add secret</li> </ul> </li> <li>Add AWS_REGION secret:<ul> <li>Click New repository secret</li> <li>Name: <code>AWS_REGION</code></li> <li>Value: Your chosen region (e.g., <code>us-west-2</code>, <code>eu-west-1</code>, <code>ap-northeast-1</code>)</li> <li>Click Add secret</li> </ul> </li> <li>Add ADMIN_PASSWORD secret:<ul> <li>Click New repository secret</li> <li>Name: <code>ADMIN_PASSWORD</code></li> <li>Value: Your secure admin password (use a password manager to generate)</li> <li>Click Add secret</li> </ul> </li> <li>Add DB_PASSWORD secret: - Click New repository secret - Name: <code>DB_PASSWORD</code> - Value: Your secure database password (use a password manager to generate) - Click Add secret</li> </ol> <p>Note: The template repository can safely include these secrets because:</p> <ul> <li>Repository permissions control who can trigger workflows</li> <li>Secrets are NOT copied when creating repos from the template</li> <li>External users must configure their own AWS credentials, region, and passwords</li> </ul> <p>Security: The workflow automatically injects <code>ADMIN_PASSWORD</code> and <code>DB_PASSWORD</code> into configuration files before Terraform runs, replacing <code>PLACEHOLDER_ADMIN_PASSWORD</code> and <code>PLACEHOLDER_DB_PASSWORD</code>. This prevents passwords from appearing in Terraform logs.</p>"},{"location":"aws-setup/#for-deployment-repositories-eg-sleap-lablink","title":"For Deployment Repositories (e.g., <code>sleap-lablink</code>)","text":"<p>After creating a repository from the template:</p> <ol> <li>Go to the new repository Settings \u2192 Secrets and variables \u2192 Actions</li> <li>Add all four secrets (same process as above): - <code>AWS_ROLE_ARN</code>: Same ARN as template repository - <code>AWS_REGION</code>: Your chosen region for this deployment - <code>ADMIN_PASSWORD</code>: Your secure admin password - <code>DB_PASSWORD</code>: Your secure database password</li> </ol> <p>Important:</p> <ul> <li>Each deployment repository needs these secrets added manually after creation</li> <li>Different deployments can use different regions if needed</li> <li>Region in secret must match region in <code>config/config.yaml</code></li> <li>Use strong, unique passwords for each deployment</li> </ul>"},{"location":"aws-setup/#47-update-trust-policy-for-new-repositories","title":"4.7: Update Trust Policy for New Repositories","text":"<p>When you create new deployment repositories, update the trust policy to include them:</p>"},{"location":"aws-setup/#aws-cli_5","title":"AWS CLI","text":"<pre><code># Edit trust-policy.json to add new repository:\n# \"repo:YOUR_ORG/new-deployment:*\"\n\n# Update the role\naws iam update-assume-role-policy \\\n  --role-name GitHubActionsLabLinkRole \\\n  --policy-document file://trust-policy.json\n\n# Verify update\naws iam get-role --role-name GitHubActionsLabLinkRole \\\n  --query \"Role.AssumeRolePolicyDocument.Statement[0].Condition.StringLike\"\n</code></pre>"},{"location":"aws-setup/#aws-console_5","title":"AWS Console","text":"<ol> <li>Go to IAM \u2192 Roles \u2192 <code>GitHubActionsLabLinkRole</code></li> <li>Trust relationships tab</li> <li>Click Edit trust policy</li> <li>Add new repository to the <code>token.actions.githubusercontent.com:sub</code> array:</li> </ol> <pre><code>\"token.actions.githubusercontent.com:sub\": [\n  \"repo:YOUR_ORG/lablink:*\",\n  \"repo:YOUR_ORG/lablink-template:*\",\n  \"repo:YOUR_ORG/sleap-lablink:*\",\n  \"repo:YOUR_ORG/new-deployment:*\"\n]\n</code></pre> <ol> <li>Click Update policy</li> </ol>"},{"location":"aws-setup/#48-verify-github-actions-can-assume-role","title":"4.8: Verify GitHub Actions Can Assume Role","text":"<p>The workflows already include the OIDC authentication step:</p> <pre><code>- name: Configure AWS credentials via OIDC\n  uses: aws-actions/configure-aws-credentials@v3\n  with:\n    role-to-assume: ${{ secrets.AWS_ROLE_ARN }}\n    aws-region: us-west-2\n</code></pre> <p>Test by triggering a workflow:</p> <ol> <li>Go to Actions tab in GitHub</li> <li>Select a workflow (e.g., Terraform Deploy)</li> <li>Click Run workflow</li> <li>Check the logs for successful AWS authentication</li> </ol>"},{"location":"aws-setup/#troubleshooting-oidc-setup","title":"Troubleshooting OIDC Setup","text":""},{"location":"aws-setup/#error-not-authorized-to-perform-stsassumerolewithwebidentity","title":"Error: \"Not authorized to perform sts:AssumeRoleWithWebIdentity\"","text":"<p>Cause: Repository not in trust policy</p> <p>Solution: Add repository to trust policy (Step 4.7)</p>"},{"location":"aws-setup/#error-no-openid-connect-provider-found","title":"Error: \"No OpenID Connect provider found\"","text":"<p>Cause: OIDC provider doesn't exist</p> <p>Solution: Create OIDC provider (Step 4.2)</p>"},{"location":"aws-setup/#error-access-denied-during-deployment","title":"Error: \"Access Denied\" during deployment","text":"<p>Cause: Role lacks required permissions</p> <p>Solution: Attach required AWS managed policies or verify custom policy (Step 4.4)</p>"},{"location":"aws-setup/#verify-trust-policy-includes-repository","title":"Verify Trust Policy Includes Repository","text":"<pre><code># CLI: Check which repos can use the role\naws iam get-role --role-name GitHubActionsLabLinkRole \\\n  --query \"Role.AssumeRolePolicyDocument.Statement[0].Condition.StringLike\" \\\n  --output json\n</code></pre> <p>Console: IAM \u2192 Roles \u2192 GitHubActionsLabLinkRole \u2192 Trust relationships</p>"},{"location":"aws-setup/#step-5-find-ami-ids-for-your-region","title":"Step 5: Find AMI IDs for Your Region","text":"<p>AMI IDs are region-specific. You'll need to find the correct Ubuntu 24.04 AMI IDs for your chosen region.</p>"},{"location":"aws-setup/#find-ubuntu-2404-amis","title":"Find Ubuntu 24.04 AMIs","text":""},{"location":"aws-setup/#aws-cli-method-recommended","title":"AWS CLI Method (Recommended)","text":"<p>For Allocator (Ubuntu 24.04 with Docker):</p> <pre><code>aws ec2 describe-images \\\n  --region YOUR_REGION \\\n  --owners 099720109477 \\\n  --filters \"Name=name,Values=ubuntu/images/hvm-ssd-gp3/ubuntu-noble-24.04-amd64-server-*\" \\\n            \"Name=state,Values=available\" \\\n  --query 'sort_by(Images, &amp;CreationDate)[-1].[ImageId,Name,CreationDate]' \\\n  --output table\n</code></pre> <p>For Client VMs (Ubuntu 24.04 with Docker + NVIDIA):</p> <pre><code># First, find latest Ubuntu 24.04\naws ec2 describe-images \\\n  --region YOUR_REGION \\\n  --owners 099720109477 \\\n  --filters \"Name=name,Values=ubuntu/images/hvm-ssd-gp3/ubuntu-noble-24.04-amd64-server-*\" \\\n            \"Name=state,Values=available\" \\\n  --query 'sort_by(Images, &amp;CreationDate)[-1].[ImageId,Name]' \\\n  --output table\n\n# Note: For GPU instances, you may need to use NVIDIA's Deep Learning AMI\n# or install NVIDIA drivers via user_data\n</code></pre>"},{"location":"aws-setup/#aws-console-method","title":"AWS Console Method","text":"<ol> <li>Go to EC2 \u2192 AMI Catalog in your chosen region</li> <li>Search for \"ubuntu 24.04\"</li> <li>Select \"AWS Marketplace AMIs\" or \"Community AMIs\"</li> <li>Filter by:</li> <li>Owner: Canonical (099720109477)</li> <li>Architecture: 64-bit (x86)</li> <li>Root device type: EBS</li> <li>Choose the most recent \"ubuntu-noble-24.04\" AMI</li> <li>Copy the AMI ID (e.g., <code>ami-0bd08c9d4aa9f0bc6</code>)</li> </ol>"},{"location":"aws-setup/#update-configuration-with-ami-ids","title":"Update Configuration with AMI IDs","text":"<p>Once you have the AMI IDs for your region, update <code>config/config.yaml</code>:</p> <pre><code># lablink-infrastructure/config/config.yaml\nmachine:\n  ami_id: \"ami-XXXXXXXXX\" # Client VM AMI for your region\n  # ...\n\nallocator_instance:\n  ami_id: \"ami-YYYYYYYYY\" # Allocator AMI for your region\n</code></pre>"},{"location":"aws-setup/#lablink-custom-amis-us-west-2-only","title":"LabLink Custom AMIs (us-west-2 only)","text":"<p>LabLink maintains custom AMIs with Docker and NVIDIA drivers pre-installed, only available in us-west-2:</p> <p>Client VM AMI (Ubuntu 24.04 + Docker + NVIDIA):</p> <ul> <li>AMI ID: <code>ami-0601752c11b394251</code></li> <li>Description: Custom Ubuntu image with Docker and Nvidia GPU Driver pre-installed</li> <li>Architecture: x86_64</li> <li>Source: Ubuntu Server 24.04 LTS (HVM), SSD Volume Type</li> </ul> <p>Allocator VM AMI (Ubuntu 24.04 + Docker):</p> <ul> <li>AMI ID: <code>ami-0bd08c9d4aa9f0bc6</code></li> <li>Description: Custom Ubuntu image with Docker pre-installed</li> <li>Architecture: x86_64</li> <li>Source: Ubuntu Server 24.04 LTS (HVM), SSD Volume Type</li> </ul>"},{"location":"aws-setup/#using-lablink-in-other-regions","title":"Using LabLink in Other Regions","text":"<p>If you're deploying to a region other than <code>us-west-2</code>, you have two options:</p> <p>Option 1: Copy Custom AMIs to Your Region (Recommended)</p> <p>Copy the LabLink custom AMIs to your preferred region:</p> <pre><code># Copy Client AMI from us-west-2\naws ec2 copy-image \\\n  --source-region us-west-2 \\\n  --source-image-id ami-0601752c11b394251 \\\n  --name \"lablink-client-ubuntu-24.04-docker-nvidia\" \\\n  --description \"Custom Ubuntu image with Docker and Nvidia GPU Driver\" \\\n  --region YOUR_REGION\n\n# Copy Allocator AMI from us-west-2\naws ec2 copy-image \\\n  --source-region us-west-2 \\\n  --source-image-id ami-0bd08c9d4aa9f0bc6 \\\n  --name \"lablink-allocator-ubuntu-24.04-docker\" \\\n  --description \"Custom Ubuntu image with Docker\" \\\n  --region YOUR_REGION\n</code></pre> <p>The copy process takes 10-30 minutes. Note the new AMI IDs from the output and update your <code>config/config.yaml</code>.</p> <p>Option 2: Use Standard Ubuntu 24.04 AMIs</p> <p>Use standard Ubuntu AMIs (Docker and NVIDIA drivers will be installed via user_data):</p> <pre><code># Find latest Ubuntu 24.04 in your region\naws ec2 describe-images \\\n  --region YOUR_REGION \\\n  --owners 099720109477 \\\n  --filters \"Name=name,Values=ubuntu/images/hvm-ssd-gp3/ubuntu-noble-24.04-amd64-server-*\" \\\n            \"Name=state,Values=available\" \\\n  --query 'sort_by(Images, &amp;CreationDate)[-1].[ImageId,Name]' \\\n  --output table\n</code></pre> <p>Note: Standard AMIs require modifying user_data scripts to install Docker and NVIDIA drivers on first boot, increasing deployment time by ~5-10 minutes.</p>"},{"location":"aws-setup/#step-6-security-groups-optional-pre-creation","title":"Step 6: Security Groups (Optional Pre-Creation)","text":"<p>Terraform creates security groups automatically, but you can pre-create them for more control.</p>"},{"location":"aws-setup/#allocator-security-group","title":"Allocator Security Group","text":"<pre><code># Create security group\nALLOCATOR_SG=$(aws ec2 create-security-group \\\n  --group-name lablink-allocator-sg \\\n  --description \"LabLink Allocator Security Group\" \\\n  --vpc-id vpc-xxxxx \\\n  --output text --query 'GroupId')\n\n# Allow HTTP (port 80)\naws ec2 authorize-security-group-ingress \\\n  --group-id $ALLOCATOR_SG \\\n  --protocol tcp \\\n  --port 80 \\\n  --cidr 0.0.0.0/0\n\n# Allow SSH (port 22)\naws ec2 authorize-security-group-ingress \\\n  --group-id $ALLOCATOR_SG \\\n  --protocol tcp \\\n  --port 22 \\\n  --cidr 0.0.0.0/0\n\n# Allow PostgreSQL from VPC (port 5432)\naws ec2 authorize-security-group-ingress \\\n  --group-id $ALLOCATOR_SG \\\n  --protocol tcp \\\n  --port 5432 \\\n  --source-group $ALLOCATOR_SG\n</code></pre>"},{"location":"aws-setup/#step-6-route-53-dns-optional","title":"Step 6: Route 53 DNS (Optional)","text":"<p>Set up custom domains for your allocators.</p>"},{"location":"aws-setup/#create-hosted-zone","title":"Create Hosted Zone","text":"<pre><code>aws route53 create-hosted-zone \\\n  --name lablink.yourdomain.com \\\n  --caller-reference $(date +%s)\n</code></pre> <p>Note the hosted zone ID from the output.</p>"},{"location":"aws-setup/#create-dns-records","title":"Create DNS Records","text":"<p>After deploying allocator, create A record:</p> <pre><code># Get allocator IP\nALLOCATOR_IP=$(terraform output -raw ec2_public_ip)\n\n# Create/update DNS record\naws route53 change-resource-record-sets \\\n  --hosted-zone-id Z1234567890ABC \\\n  --change-batch '{\n    \"Changes\": [{\n      \"Action\": \"UPSERT\",\n      \"ResourceRecordSet\": {\n        \"Name\": \"lablink-test.yourdomain.com\",\n        \"Type\": \"A\",\n        \"TTL\": 300,\n        \"ResourceRecords\": [{\"Value\": \"'$ALLOCATOR_IP'\"}]\n      }\n    }]\n  }'\n</code></pre>"},{"location":"aws-setup/#update-name-servers","title":"Update Name Servers","text":"<p>Update your domain registrar with the Route 53 name servers (from hosted zone output).</p>"},{"location":"aws-setup/#step-7-secrets-manager-optional","title":"Step 7: Secrets Manager (Optional)","text":"<p>Store sensitive configuration in AWS Secrets Manager instead of config files.</p>"},{"location":"aws-setup/#create-secrets","title":"Create Secrets","text":"<pre><code># Database password\naws secretsmanager create-secret \\\n  --name lablink/db-password \\\n  --secret-string \"your-secure-db-password\" \\\n  --region us-west-2\n\n# Admin password\naws secretsmanager create-secret \\\n  --name lablink/admin-password \\\n  --secret-string \"your-secure-admin-password\" \\\n  --region us-west-2\n</code></pre>"},{"location":"aws-setup/#retrieve-in-application","title":"Retrieve in Application","text":"<p>Modify your application code to fetch secrets:</p> <pre><code>import boto3\nfrom botocore.exceptions import ClientError\n\ndef get_secret(secret_name, region_name=\"us-west-2\"):\n    \"\"\"Retrieve secret from AWS Secrets Manager.\"\"\"\n    session = boto3.session.Session()\n    client = session.client(\n        service_name='secretsmanager',\n        region_name=region_name\n    )\n\n    try:\n        response = client.get_secret_value(SecretId=secret_name)\n        return response['SecretString']\n    except ClientError as e:\n        raise e\n\n# Usage\ndb_password = get_secret(\"lablink/db-password\")\nadmin_password = get_secret(\"lablink/admin-password\")\n</code></pre>"},{"location":"aws-setup/#step-8-cloudwatch-monitoring-optional","title":"Step 8: CloudWatch Monitoring (Optional)","text":"<p>Set up monitoring and alerts for your infrastructure.</p>"},{"location":"aws-setup/#terraform-managed-cloudwatch-resources","title":"Terraform-Managed CloudWatch Resources","text":"<p>When you deploy LabLink using Terraform, the following CloudWatch resources are automatically created:</p> <p>CloudWatch Agent Role: Terraform creates an IAM role (<code>lablink_cloud_watch_agent_role_&lt;suffix&gt;</code>) that allows client VMs to send logs and metrics to CloudWatch. This role includes permissions for:</p> <ul> <li><code>logs:CreateLogGroup</code> - Create new log groups</li> <li><code>logs:CreateLogStream</code> - Create log streams within groups</li> <li><code>logs:PutLogEvents</code> - Write log entries</li> <li><code>logs:DescribeLogStreams</code> - List available log streams</li> <li><code>cloudwatch:PutMetricData</code> - Send custom metrics</li> </ul> <p>CloudTrail Logs: CloudTrail events are automatically sent to CloudWatch Logs for security monitoring.</p> <p>Metric Filters &amp; Alarms: Terraform creates CloudWatch metric filters to monitor:</p> Metric Filter Purpose Alarm Threshold <code>RunInstances</code> Detect mass instance launches &gt;10 in 5 minutes <code>LargeInstances</code> Monitor expensive instance types (p4d, p3, g5) Any launch <code>UnauthorizedCalls</code> Track API permission failures &gt;5 in 5 minutes <code>TerminateInstances</code> High termination rate detection &gt;10 in 5 minutes <p>SNS Notifications: Alarms send notifications to the <code>lablink-admin-alerts-&lt;suffix&gt;</code> SNS topic, which emails the configured admin.</p>"},{"location":"aws-setup/#manual-cloudwatch-configuration-optional","title":"Manual CloudWatch Configuration (Optional)","text":"<p>For additional monitoring beyond what Terraform provides:</p>"},{"location":"aws-setup/#enable-cloudwatch-logs","title":"Enable CloudWatch Logs","text":"<p>Update user data script to send logs to CloudWatch:</p> <pre><code>#!/bin/bash\n\n# Install CloudWatch agent\nwget https://s3.amazonaws.com/amazoncloudwatch-agent/ubuntu/amd64/latest/amazon-cloudwatch-agent.deb\ndpkg -i amazon-cloudwatch-agent.deb\n\n# Configure CloudWatch agent\ncat &gt; /opt/aws/amazon-cloudwatch-agent/etc/config.json &lt;&lt;EOF\n{\n  \"logs\": {\n    \"logs_collected\": {\n      \"files\": {\n        \"collect_list\": [\n          {\n            \"file_path\": \"/var/log/syslog\",\n            \"log_group_name\": \"/aws/ec2/lablink-allocator\",\n            \"log_stream_name\": \"{instance_id}/syslog\"\n          }\n        ]\n      }\n    }\n  }\n}\nEOF\n\n# Start agent\n/opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl \\\n  -a fetch-config -m ec2 -s -c file:/opt/aws/amazon-cloudwatch-agent/etc/config.json\n</code></pre>"},{"location":"aws-setup/#create-cloudwatch-alarms","title":"Create CloudWatch Alarms","text":"<pre><code># CPU utilization alarm\naws cloudwatch put-metric-alarm \\\n  --alarm-name lablink-allocator-cpu-high \\\n  --alarm-description \"Alert when CPU exceeds 80%\" \\\n  --metric-name CPUUtilization \\\n  --namespace AWS/EC2 \\\n  --statistic Average \\\n  --period 300 \\\n  --threshold 80 \\\n  --comparison-operator GreaterThanThreshold \\\n  --evaluation-periods 2 \\\n  --dimensions Name=InstanceId,Value=i-xxxxx\n</code></pre>"},{"location":"aws-setup/#step-9-billing-alerts","title":"Step 9: Billing Alerts","text":"<p>Set up cost monitoring to avoid unexpected charges.</p>"},{"location":"aws-setup/#enable-billing-alerts","title":"Enable Billing Alerts","text":"<pre><code>aws ce put-cost-anomaly-monitor \\\n  --anomaly-monitor file://billing-monitor.json\n</code></pre> <p><code>billing-monitor.json</code>:</p> <pre><code>{\n  \"MonitorName\": \"LabLink Cost Monitor\",\n  \"MonitorType\": \"DIMENSIONAL\",\n  \"MonitorDimension\": \"SERVICE\"\n}\n</code></pre>"},{"location":"aws-setup/#create-budget","title":"Create Budget","text":"<pre><code>aws budgets create-budget \\\n  --account-id YOUR_ACCOUNT_ID \\\n  --budget file://budget.json \\\n  --notifications-with-subscribers file://subscribers.json\n</code></pre> <p><code>budget.json</code>:</p> <pre><code>{\n  \"BudgetName\": \"LabLink Monthly Budget\",\n  \"BudgetLimit\": {\n    \"Amount\": \"100\",\n    \"Unit\": \"USD\"\n  },\n  \"TimeUnit\": \"MONTHLY\",\n  \"BudgetType\": \"COST\"\n}\n</code></pre> <p><code>subscribers.json</code>:</p> <pre><code>[\n  {\n    \"Notification\": {\n      \"NotificationType\": \"ACTUAL\",\n      \"ComparisonOperator\": \"GREATER_THAN\",\n      \"Threshold\": 80\n    },\n    \"Subscribers\": [\n      {\n        \"SubscriptionType\": \"EMAIL\",\n        \"Address\": \"your-email@example.com\"\n      }\n    ]\n  }\n]\n</code></pre>"},{"location":"aws-setup/#verification-checklist","title":"Verification Checklist","text":"<p>After completing setup, verify:</p> <p>Required Setup:</p> <ul> <li> S3 bucket created with versioning and encryption</li> <li> DynamoDB table created for state locking (if using remote state)</li> <li> Elastic IPs allocated for test and prod</li> <li> OIDC provider created</li> <li> IAM role for GitHub Actions configured with required permissions</li> <li> GitHub repository secrets configured (<code>AWS_ROLE_ARN</code>, <code>AWS_REGION</code>, <code>ADMIN_PASSWORD</code>, <code>DB_PASSWORD</code>)</li> <li> AMI IDs configured for your region</li> </ul> <p>Optional Setup:</p> <ul> <li> Route 53 hosted zone created (for custom domain)</li> <li> ACM certificate created (for HTTPS via ALB)</li> <li> Secrets Manager secrets created</li> <li> Budget alerts configured</li> </ul> <p>Terraform-Managed (Automatic):</p> <p>The following resources are created automatically by Terraform during deployment:</p> <ul> <li> Application Load Balancer (ALB) with HTTPS listener</li> <li> CloudTrail trail with S3 bucket for logs</li> <li> CloudWatch log groups and metric filters</li> <li> SNS topic for admin alerts</li> <li> IAM roles for CloudWatch agent and CloudTrail</li> <li> Security groups for allocator and ALB</li> <li> Monthly budget with alert thresholds</li> </ul>"},{"location":"aws-setup/#testing-your-setup","title":"Testing Your Setup","text":""},{"location":"aws-setup/#test-aws-cli-access","title":"Test AWS CLI Access","text":"<pre><code>aws sts get-caller-identity\naws s3 ls\naws ec2 describe-regions\n</code></pre>"},{"location":"aws-setup/#test-terraform","title":"Test Terraform","text":"<pre><code>cd lablink-allocator\nterraform init\nterraform validate\n</code></pre>"},{"location":"aws-setup/#test-github-actions","title":"Test GitHub Actions","text":"<p>Push a commit to trigger workflows:</p> <pre><code>git commit --allow-empty -m \"Test GitHub Actions\"\ngit push origin main\n</code></pre> <p>Check Actions tab in GitHub.</p>"},{"location":"aws-setup/#common-issues","title":"Common Issues","text":""},{"location":"aws-setup/#oidc-provider-already-exists","title":"OIDC Provider Already Exists","text":"<p>Error: <code>EntityAlreadyExists: Provider with URL ... already exists</code></p> <p>Solution: Use existing provider, just create new role</p>"},{"location":"aws-setup/#s3-bucket-name-taken","title":"S3 Bucket Name Taken","text":"<p>Error: <code>BucketAlreadyExists</code></p> <p>Solution: Choose a different bucket name (must be globally unique)</p>"},{"location":"aws-setup/#iam-permission-denied","title":"IAM Permission Denied","text":"<p>Error: <code>AccessDenied: User ... is not authorized to perform</code></p> <p>Solution: Ensure IAM user/role has required permissions</p>"},{"location":"aws-setup/#cost-estimation","title":"Cost Estimation","text":"<p>Estimated monthly costs for AWS resources:</p> Resource Usage Estimated Cost S3 Bucket &lt;1 GB, versioning $0.05/month Elastic IPs 2 IPs (test, prod) $0.00 (while associated) Route 53 Hosted Zone 1 zone $0.50/month Secrets Manager 2 secrets $0.80/month <p>Total AWS Setup Cost: ~$1.35/month</p> <p>Running EC2 instances cost extra. See Cost Estimation for details.</p>"},{"location":"aws-setup/#next-steps","title":"Next Steps","text":"<p>With AWS resources configured:</p> <ol> <li>Deployment: Deploy LabLink</li> <li>Security: Review security best practices</li> <li>Workflows: Understand CI/CD pipelines</li> </ol>"},{"location":"aws-setup/#cleanup","title":"Cleanup","text":"<p>To remove all AWS resources:</p> <pre><code># Delete S3 bucket (remove objects first)\naws s3 rm s3://$BUCKET_NAME --recursive\naws s3api delete-bucket --bucket $BUCKET_NAME\n\n# Release Elastic IPs\naws ec2 release-address --allocation-id eipalloc-test-xxxxx\naws ec2 release-address --allocation-id eipalloc-prod-xxxxx\n\n# Delete IAM role\naws iam delete-role-policy --role-name github-lablink-deploy --policy-name lablink-deploy-permissions\naws iam delete-role --role-name github-lablink-deploy\n\n# Delete OIDC provider\naws iam delete-open-id-connect-provider \\\n  --open-id-connect-provider-arn arn:aws:iam::YOUR_ACCOUNT_ID:oidc-provider/token.actions.githubusercontent.com\n</code></pre>"},{"location":"changelog-allocator/","title":"lablink-allocator-service Changelog","text":"<p>All notable changes to lablink-allocator-service will be documented here.</p> <p>The format is based on Keep a Changelog.</p>"},{"location":"changelog-allocator/#unreleased","title":"Unreleased","text":""},{"location":"changelog-allocator/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>fix(docs): Correct module names for API reference generation (#255)</li> </ul>"},{"location":"changelog-allocator/#features","title":"Features","text":"<ul> <li>feat: Add ci-test environment support (#227)</li> <li>feat: Add config validation CLI for allocator service (#204)</li> </ul>"},{"location":"changelog-allocator/#other-changes","title":"Other Changes","text":"<ul> <li>Feat: Add Destruction Scheduler for Client VMs (#249)</li> <li>Simplify domain and SSL configuration (#230)</li> <li>Add Configs for Monitoring (#238)</li> <li>Bug: Timing Data not Updated When Numerous VMs Running (#243)</li> <li>Change uv Installation Methods and Add More Packages to the Client (#236)</li> <li>fix ui for admin (#237)</li> <li>Remove Credentials Features for Better UX (#234)</li> <li>New Timing Measurement Methods for Client VMs (#229)</li> <li>configurable region for backend (#228)</li> <li>Add Coverage Requirement in the CI and Add More Tests (#226)</li> <li>Fix: remote-exec provisioner timeout during client VM creation (#220)</li> <li>Add Unset AWS Credentials Functionality for Admin Users (#219)</li> <li>Add Tests for <code>database.py</code> (#223)</li> <li>Add a Custom Startup Script Feature without Affecting the Core Startup script (#211)</li> <li>add ci-test backend for terraform in allocator (#214)</li> <li>Fix Bugs in Allocator Logging and Configs (#210)</li> <li>Remove remaining infrastructure code migrated to lablink-template (#208)</li> </ul>"},{"location":"changelog-allocator/#003a2","title":"0.0.3a2","text":""},{"location":"changelog-allocator/#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>fix: Sanitize URLs to remove prepended dots in allocator and client services (#199)</li> </ul>"},{"location":"changelog-allocator/#003a1","title":"0.0.3a1","text":""},{"location":"changelog-allocator/#bug-fixes_2","title":"Bug Fixes","text":"<ul> <li>fix: Add safety check for leading dots in hostname and bump version (#197)</li> <li>fix: Handle empty subdomains in get_allocator_url (#196)</li> <li>fix: update module path in Dockerfile to lablink_allocator_service (#194)</li> </ul>"},{"location":"changelog-allocator/#003a0","title":"0.0.3a0","text":""},{"location":"changelog-allocator/#other-changes_1","title":"Other Changes","text":"<ul> <li>Fix: Skip Terraform tests in publish workflow (#193)</li> <li>Restructure packages and create infrastructure template (#189)</li> </ul>"},{"location":"changelog-allocator/#002a0","title":"0.0.2a0","text":"<p>Initial release.</p> <p>For more details, see the GitHub Releases page.</p>"},{"location":"changelog-client/","title":"lablink-client-service Changelog","text":"<p>All notable changes to lablink-client-service will be documented here.</p> <p>The format is based on Keep a Changelog.</p>"},{"location":"changelog-client/#unreleased","title":"Unreleased","text":""},{"location":"changelog-client/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>fix(client): Fix uv cache permission denied error in Dockerfile (#256)</li> </ul>"},{"location":"changelog-client/#features","title":"Features","text":"<ul> <li>feat: Add ci-test environment support (#227)</li> </ul>"},{"location":"changelog-client/#other-changes","title":"Other Changes","text":"<ul> <li>Add Retry Logics to API Connections to Allocator (#235)</li> <li>Change uv Installation Methods and Add More Packages to the Client (#236)</li> <li>New Timing Measurement Methods for Client VMs (#229)</li> <li>Add Coverage Requirement in the CI and Add More Tests (#226)</li> <li>Add a Custom Startup Script Feature without Affecting the Core Startup script (#211)</li> </ul>"},{"location":"changelog-client/#008a3","title":"0.0.8a3","text":""},{"location":"changelog-client/#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>fix: Sanitize URLs to remove prepended dots in allocator and client services (#199)</li> </ul>"},{"location":"changelog-client/#008a2","title":"0.0.8a2","text":""},{"location":"changelog-client/#bug-fixes_2","title":"Bug Fixes","text":"<ul> <li>fix: Sanitize URLs to handle prepended dots in subscribe function (#198)</li> </ul>"},{"location":"changelog-client/#008a1","title":"0.0.8a1","text":""},{"location":"changelog-client/#chores","title":"Chores","text":"<ul> <li>chore: Update client package Python requirement and version (#195)</li> </ul>"},{"location":"changelog-client/#008a0","title":"0.0.8a0","text":""},{"location":"changelog-client/#other-changes_1","title":"Other Changes","text":"<ul> <li>Fix: Skip Terraform tests in publish workflow (#193)</li> <li>Restructure packages and create infrastructure template (#189)</li> </ul>"},{"location":"changelog-client/#007a0","title":"0.0.7a0","text":"<p>Initial release.</p> <p>For more details, see the GitHub Releases page.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>LabLink consists of two independently versioned packages:</p>"},{"location":"changelog/#package-changelogs","title":"Package Changelogs","text":"<ul> <li>lablink-allocator-service - VM Allocator Service</li> <li>lablink-client-service - Client Service</li> </ul> <p>For release notes and downloads, see the GitHub Releases page.</p>"},{"location":"configuration/","title":"Configuration","text":"<p>LabLink uses structured configuration files to customize behavior. This guide covers all configuration options and how to modify them.</p> <p>Infrastructure Repository</p> <p>Configuration files are located in the lablink-template repository under <code>lablink-infrastructure/config/config.yaml</code>. Clone the template repository to deploy LabLink infrastructure.</p>"},{"location":"configuration/#first-steps-change-default-passwords","title":"First Steps: Change Default Passwords","text":"<p>Critical Security Step</p> <p>Before deploying LabLink or creating any VMs, you MUST configure secure passwords!</p> <p>Configuration files use placeholder values that must be replaced with secure passwords: - Admin password placeholder: <code>PLACEHOLDER_ADMIN_PASSWORD</code> - Database password placeholder: <code>PLACEHOLDER_DB_PASSWORD</code></p>"},{"location":"configuration/#how-to-configure-passwords","title":"How to Configure Passwords","text":"<p>Method 1: GitHub Secrets (Recommended for CI/CD)</p> <p>For GitHub Actions deployments, add secrets to your repository:</p> <ol> <li>Go to repository Settings \u2192 Secrets and variables \u2192 Actions</li> <li>Click New repository secret</li> <li>Add <code>ADMIN_PASSWORD</code> with your secure admin password</li> <li>Add <code>DB_PASSWORD</code> with your secure database password</li> </ol> <p>The deployment workflow automatically replaces placeholders with these secret values before Terraform runs, preventing passwords from appearing in logs.</p> <p>Method 2: Manual Configuration</p> <p>For local deployments, edit the configuration file:</p> <pre><code># Edit allocator configuration\nvi lablink-infrastructure/config/config.yaml\n</code></pre> <p>Update these values: <pre><code>db:\n  password: \"YOUR_SECURE_DB_PASSWORD_HERE\"  # Replace PLACEHOLDER_DB_PASSWORD\n\napp:\n  admin_password: \"YOUR_SECURE_PASSWORD_HERE\"  # Replace PLACEHOLDER_ADMIN_PASSWORD\n</code></pre></p> <p>Method 3: Environment Variables</p> <pre><code>export ADMIN_PASSWORD=\"your_secure_password\"\nexport DB_PASSWORD=\"your_secure_db_password\"\n</code></pre> <p>Password requirements: - Minimum 12 characters - Mix of uppercase, lowercase, numbers, symbols - Not a dictionary word - Use a password manager to generate and store</p> <p>See Security \u2192 Change Default Passwords for detailed security guidance.</p>"},{"location":"configuration/#configuration-system","title":"Configuration System","text":"<p>LabLink uses Hydra for configuration management, which provides:</p> <ul> <li>Structured configs: Type-safe dataclass-based configuration</li> <li>Hierarchical composition: Override specific values</li> <li>Environment variables: Override via <code>ENV_VAR</code> syntax</li> <li>Command-line overrides: Pass config values as arguments</li> </ul>"},{"location":"configuration/#configuration-files","title":"Configuration Files","text":""},{"location":"configuration/#allocator-configuration","title":"Allocator Configuration","text":"<p>Location: <code>lablink-infrastructure/config/config.yaml</code></p> <pre><code>db:\n  dbname: \"lablink_db\"\n  user: \"lablink\"\n  password: \"PLACEHOLDER_DB_PASSWORD\"  # Injected from GitHub secret at deploy time\n  host: \"localhost\"\n  port: 5432\n  table_name: \"vms\"\n  message_channel: \"vm_updates\"\n\nmachine:\n  machine_type: \"g4dn.xlarge\"\n  image: \"ghcr.io/talmolab/lablink-client-base-image:linux-amd64-test\"\n  ami_id: \"ami-067cc81f948e50e06\"\n  repository: \"https://github.com/talmolab/sleap-tutorial-data.git\"\n  software: \"sleap\"\n\napp:\n  admin_user: \"admin\"\n  admin_password: \"PLACEHOLDER_ADMIN_PASSWORD\"  # Injected from GitHub secret at deploy time\n  region: \"us-west-2\"\n\nbucket_name: \"tf-state-lablink-allocator-bucket\"\n</code></pre>"},{"location":"configuration/#client-configuration","title":"Client Configuration","text":"<p>Location: <code>packages/client/src/lablink_client/conf/config.yaml</code></p> <pre><code>allocator:\n  host: \"localhost\"\n  port: 80\n\nclient:\n  software: \"sleap\"\n</code></pre>"},{"location":"configuration/#configuration-reference","title":"Configuration Reference","text":""},{"location":"configuration/#database-options-db","title":"Database Options (<code>db</code>)","text":"<p>Configuration for the PostgreSQL database.</p> Option Type Default Description <code>dbname</code> string <code>lablink_db</code> Database name <code>user</code> string <code>lablink</code> Database username <code>password</code> string <code>PLACEHOLDER_DB_PASSWORD</code> Database password (injected from GitHub secret) <code>host</code> string <code>localhost</code> Database host <code>port</code> int <code>5432</code> PostgreSQL port <code>table_name</code> string <code>vms</code> VM table name <code>message_channel</code> string <code>vm_updates</code> PostgreSQL NOTIFY channel <p>Production Security</p> <p>Configure <code>DB_PASSWORD</code> secret for GitHub Actions deployments, or manually replace the placeholder. See Security.</p>"},{"location":"configuration/#machine-options-machine","title":"Machine Options (<code>machine</code>)","text":"<p>Configuration for client VM specifications. These are the key options for adapting LabLink to your research software.</p> Option Type Default Description <code>machine_type</code> string <code>g4dn.xlarge</code> AWS EC2 instance type <code>image</code> string <code>ghcr.io/talmolab/lablink-client-base-image:latest</code> Docker image for client container <code>ami_id</code> string <code>ami-067cc81f948e50e06</code> Amazon Machine Image (Ubuntu 20.04 + Docker) <code>repository</code> string (optional) <code>https://github.com/talmolab/sleap-tutorial-data.git</code> Git repository to clone on VM <code>software</code> string <code>sleap</code> Software identifier (used by client)"},{"location":"configuration/#machine-type-options","title":"Machine Type Options","text":"<p>Common GPU instance types:</p> Instance Type GPU vCPUs Memory GPU Memory Use Case <code>g4dn.xlarge</code> NVIDIA T4 4 16 GB 16 GB Light workloads, testing <code>g4dn.2xlarge</code> NVIDIA T4 8 32 GB 16 GB Medium workloads <code>g5.xlarge</code> NVIDIA A10G 4 16 GB 24 GB Training, inference <code>g5.2xlarge</code> NVIDIA A10G 8 32 GB 24 GB Large models <code>p3.2xlarge</code> NVIDIA V100 8 61 GB 16 GB Deep learning training <p>See AWS Instance Types for complete list.</p>"},{"location":"configuration/#docker-image","title":"Docker Image","text":"<p>Default: <code>ghcr.io/talmolab/lablink-client-base-image:latest</code></p> <p>The Docker image determines what software runs on your VMs. Options:</p> <ol> <li>Use default SLEAP image (for SLEAP workflows)</li> <li>Build custom image (for your research software) - see Adapting LabLink</li> <li>Use different tag:</li> <li><code>:latest</code> - latest stable release</li> <li><code>:linux-amd64-test</code> - development version</li> <li><code>:v1.0.0</code> - specific version</li> </ol>"},{"location":"configuration/#ami-id","title":"AMI ID","text":"<p>Default: <code>ami-067cc81f948e50e06</code> (Ubuntu 20.04 + Docker in us-west-2)</p> <p>The Amazon Machine Image determines the OS and pre-installed software. You may need different AMIs for:</p> <ul> <li>Different AWS regions (AMI IDs are region-specific)</li> <li>Different OS versions</li> <li>Custom pre-configured images</li> </ul> <p>Find AMIs: <pre><code>aws ec2 describe-images \\\n  --owners 099720109477 \\\n  --filters \"Name=name,Values=ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*\" \\\n  --query 'Images[*].[ImageId,Name,CreationDate]' \\\n  --output table\n</code></pre></p>"},{"location":"configuration/#repository","title":"Repository","text":"<p>Default: <code>https://github.com/talmolab/sleap-tutorial-data.git</code></p> <p>Git repository to clone onto the client VM. Use this for:</p> <ul> <li>Custom analysis scripts</li> <li>Training data</li> <li>Configuration files</li> <li>Research code</li> </ul> <p>Set to empty string or omit if no repository needed: <pre><code>repository: \"\"\n</code></pre></p>"},{"location":"configuration/#software-identifier","title":"Software Identifier","text":"<p>Default: <code>sleap</code></p> <p>String identifier for the research software. Used by client service for software-specific logic.</p>"},{"location":"configuration/#application-options-app","title":"Application Options (<code>app</code>)","text":"<p>General application settings.</p> Option Type Default Description <code>admin_user</code> string <code>admin</code> Admin username for web UI <code>admin_password</code> string <code>PLACEHOLDER_ADMIN_PASSWORD</code> Admin password (injected from GitHub secret) <code>region</code> string <code>us-west-2</code> AWS region for deployments <p>Configure Passwords</p> <p>Configure <code>ADMIN_PASSWORD</code> secret for GitHub Actions deployments, or manually replace the placeholder. See Security.</p>"},{"location":"configuration/#allocator-options-allocator","title":"Allocator Options (<code>allocator</code>)","text":"<p>Client configuration for connecting to allocator.</p> Option Type Default Description <code>host</code> string <code>localhost</code> Allocator hostname or IP <code>port</code> int <code>80</code> Allocator port"},{"location":"configuration/#dns-options-dns","title":"DNS Options (<code>dns</code>)","text":"<p>Controls DNS configuration for allocator hostname.</p> Option Type Default Description <code>enabled</code> boolean <code>false</code> Enable DNS-based URLs <code>terraform_managed</code> boolean <code>false</code> Let Terraform manage Route 53 records <code>domain</code> string <code>\"\"</code> Your Route 53 hosted zone domain <code>zone_id</code> string <code>\"\"</code> Route 53 zone ID (optional, skips lookup if provided) <code>app_name</code> string <code>\"\"</code> Application name for auto pattern <code>pattern</code> string <code>\"auto\"</code> DNS pattern: <code>auto</code> or <code>custom</code> <code>custom_subdomain</code> string <code>\"\"</code> Custom subdomain for custom pattern <code>create_zone</code> boolean <code>false</code> Create new Route 53 zone <p>See DNS Configuration for detailed setup instructions.</p>"},{"location":"configuration/#eip-options-eip","title":"EIP Options (<code>eip</code>)","text":"<p>Controls Elastic IP allocation strategy.</p> Option Type Default Description <code>strategy</code> string <code>\"dynamic\"</code> <code>persistent</code> = reuse tagged EIP, <code>dynamic</code> = create new <code>tag_name</code> string <code>\"lablink-eip\"</code> Tag name for persistent EIP lookup"},{"location":"configuration/#ssltls-options-ssl","title":"SSL/TLS Options (<code>ssl</code>)","text":"<p>Controls HTTPS/SSL certificate management.</p> Option Type Default Description <code>provider</code> string <code>\"letsencrypt\"</code> SSL provider: <code>letsencrypt</code>, <code>cloudflare</code>, or <code>none</code> <code>email</code> string <code>\"\"</code> Email for Let's Encrypt notifications <code>staging</code> boolean <code>false</code> HTTP-only mode for testing (unlimited deployments)"},{"location":"configuration/#ssl-providers","title":"SSL Providers","text":"<p><code>letsencrypt</code> - Automatic SSL via Caddy + Let's Encrypt</p> <ul> <li>Staging mode (<code>staging: true</code>): HTTP only, unlimited deployments</li> <li>Production mode (<code>staging: false</code>): HTTPS with trusted certificates</li> </ul> <p><code>cloudflare</code> - CloudFlare proxy handles SSL</p> <ul> <li>Requires CloudFlare DNS configuration</li> <li>Not affected by <code>staging</code> setting</li> </ul> <p><code>none</code> - No SSL, HTTP only</p> <ul> <li>Similar to staging mode but explicit</li> </ul>"},{"location":"configuration/#staging-vs-production-mode","title":"Staging vs Production Mode","text":"<p>Staging Mode (<code>staging: true</code>)</p> <p>Use for testing and development:</p> <ul> <li>Serves HTTP only on port 80 (port 443 closed)</li> <li>Unlimited deployments per day</li> <li>No SSL certificate issuance delays</li> <li>No encryption - all traffic is plaintext</li> <li>Browser shows \"Not Secure\" warning</li> <li>May require clearing browser HSTS cache (see Troubleshooting)</li> </ul> <p>Configuration example: <pre><code>ssl:\n  provider: \"letsencrypt\"\n  email: \"admin@example.com\"\n  staging: true\n</code></pre></p> <p>Production Mode (<code>staging: false</code>)</p> <p>Use for production deployments:</p> <ul> <li>HTTPS with trusted Let's Encrypt certificates</li> <li>Browser shows secure padlock</li> <li>Full TLS 1.3 encryption</li> <li>Automatic HTTP \u2192 HTTPS redirects</li> <li>Rate limited (5 duplicate certificates per week)</li> <li>Certificate issuance takes 30-60 seconds</li> </ul> <p>Configuration example: <pre><code>ssl:\n  provider: \"letsencrypt\"\n  email: \"admin@example.com\"  # Receives cert expiry notifications\n  staging: false\n</code></pre></p>"},{"location":"configuration/#browser-access","title":"Browser Access","text":"<p>With staging mode:</p> <ol> <li>Type <code>http://</code> explicitly in address bar (e.g., <code>http://test.lablink.sleap.ai</code>)</li> <li>Clear HSTS cache if you previously accessed via HTTPS</li> <li>Expect \"Not Secure\" warning (this is normal)</li> </ol> <p>Alternatives: - Use incognito/private browsing - Access via IP: <code>http://&lt;allocator-ip&gt;</code> - Use curl: <code>curl http://test.lablink.sleap.ai</code></p> <p>With production mode:</p> <p>Access via <code>https://your-domain.com</code> - browser shows secure padlock.</p>"},{"location":"configuration/#lets-encrypt-rate-limits","title":"Let's Encrypt Rate Limits","text":"<p>Production mode is subject to Let's Encrypt limits:</p> <ul> <li>50 certificates per domain per week</li> <li>5 duplicate certificates per week (same hostnames)</li> <li>300 pending authorizations per account</li> </ul> <p>Use staging mode for frequent testing to avoid these limits.</p> <p>Warning: Staging mode serves unencrypted HTTP. Never use for production or sensitive data. See Security.</p>"},{"location":"configuration/#bucket-name","title":"Bucket Name","text":"<p>Option: <code>bucket_name</code> Default: <code>tf-state-lablink-allocator-bucket</code></p> <p>S3 bucket for Terraform state storage. Must be globally unique.</p>"},{"location":"configuration/#startup-script-options-startup_script","title":"Startup Script Options (<code>startup_script</code>)","text":"<p>Controls a custom startup script to be run on client VMs after the container starts.</p> Option Type Default Description <code>enabled</code> boolean <code>false</code> Enable custom startup script <code>path</code> string <code>\"\"</code> Path to the startup script file <code>on_error</code> string <code>continue</code> Behavior on script error: <code>continue</code> or <code>fail</code> <p>Example:</p> <pre><code>startup_script:\n  enabled: true\n  path: \"/path/to/your/script.sh\"\n  on_error: \"fail\"\n</code></pre> <p>When <code>enabled</code> is <code>true</code>, the content of the script specified by <code>path</code> will be executed on the client VM. - If <code>on_error</code> is <code>continue</code>, any errors in the script will be logged, but the VM will continue to run. - If <code>on_error</code> is <code>fail</code>, the VM setup will be aborted if the script returns a non-zero exit code.</p>"},{"location":"configuration/#overriding-configuration","title":"Overriding Configuration","text":""},{"location":"configuration/#method-1-edit-yaml-files","title":"Method 1: Edit YAML Files","text":"<p>Directly modify the configuration files:</p> <pre><code>nano lablink-infrastructure/config/config.yaml\n</code></pre>"},{"location":"configuration/#method-2-environment-variables","title":"Method 2: Environment Variables","text":"<p>Override specific values without modifying files:</p> <pre><code>export DB_PASSWORD=my_secure_password\nexport ADMIN_PASSWORD=my_admin_password\nexport AWS_REGION=us-east-1\n</code></pre>"},{"location":"configuration/#method-3-hydra-command-line-overrides","title":"Method 3: Hydra Command-Line Overrides","text":"<p>When running Python directly:</p> <pre><code>python main.py db.password=my_password app.region=us-east-1\n</code></pre>"},{"location":"configuration/#method-4-docker-environment-variables","title":"Method 4: Docker Environment Variables","text":"<p>Pass environment variables to Docker containers:</p> <pre><code>docker run -d \\\n  -e DB_PASSWORD=secure_password \\\n  -e ADMIN_PASSWORD=admin_password \\\n  -e AWS_REGION=us-east-1 \\\n  -p 5000:5000 \\\n  ghcr.io/talmolab/lablink-allocator-image:latest\n</code></pre> <p>Config Validation and Custom Filenames</p> <p>The allocator supports <code>CONFIG_NAME</code> environment variable to override the config filename. However, the validation CLI (<code>lablink-validate-config</code>) requires the filename to be <code>config.yaml</code> to enable strict schema checking. If you override <code>CONFIG_NAME</code> to use a different filename, validation will not perform strict schema checks and unknown keys may not be caught until runtime.</p>"},{"location":"configuration/#method-5-terraform-variables","title":"Method 5: Terraform Variables","text":"<p>Override during infrastructure deployment:</p> <pre><code>terraform apply \\\n  -var=\"allocator_image_tag=v1.0.0\" \\\n  -var=\"resource_suffix=prod\"\n</code></pre>"},{"location":"configuration/#configuration-for-different-environments","title":"Configuration for Different Environments","text":""},{"location":"configuration/#development","title":"Development","text":"<p>Use Case: Local testing, experimentation</p> <pre><code>db:\n  password: \"simple_dev_password\"\n\nmachine:\n  machine_type: \"t2.micro\"  # Cheaper for testing\n  image: \"ghcr.io/talmolab/lablink-client-base-image:linux-amd64-test\"\n\napp:\n  region: \"us-west-2\"\n</code></pre>"},{"location":"configuration/#teststaging","title":"Test/Staging","text":"<p>Use Case: Pre-production validation</p> <pre><code>db:\n  password: \"${DB_PASSWORD}\"  # From environment variable\n\nmachine:\n  machine_type: \"g4dn.xlarge\"\n  image: \"ghcr.io/talmolab/lablink-client-base-image:linux-amd64-test\"\n\napp:\n  admin_password: \"${ADMIN_PASSWORD}\"\n  region: \"us-west-2\"\n</code></pre>"},{"location":"configuration/#production","title":"Production","text":"<p>Use Case: Production workloads</p> <pre><code>db:\n  password: \"${DB_PASSWORD}\"  # From Secrets Manager\n  host: \"lablink-db.xxxxx.us-west-2.rds.amazonaws.com\"  # RDS instance\n\nmachine:\n  machine_type: \"g5.2xlarge\"\n  image: \"ghcr.io/talmolab/lablink-client-base-image:v1.0.0\"  # Pinned version\n\napp:\n  admin_password: \"${ADMIN_PASSWORD}\"  # From Secrets Manager\n  region: \"us-west-2\"\n\nbucket_name: \"tf-state-lablink-prod\"\n</code></pre>"},{"location":"configuration/#validating-configuration","title":"Validating Configuration","text":"<p>After modifying configuration, validate it:</p>"},{"location":"configuration/#schema-validation-recommended","title":"Schema Validation (Recommended)","text":"<p>Use the built-in validation CLI to check your config against the schema:</p> <pre><code># Validate config file\nlablink-validate-config lablink-infrastructure/config/config.yaml\n\n# Output on success:\n# \u2713 Config validation passed\n\n# Output on error:\n# \u2717 Config validation failed: Error merging config with schema\n#   Unknown keys found: ['unknown_section']\n</code></pre> <p>The validator checks:</p> <ul> <li>File exists and is named <code>config.yaml</code></li> <li>All keys match the structured config schema</li> <li>Required fields are present</li> <li>Type mismatches (strings vs integers, etc.)</li> <li>Unknown configuration sections</li> </ul> <p>Important: The validator requires the filename to be <code>config.yaml</code> to enable Hydra's strict schema matching. Using a different filename will bypass schema validation.</p> <p>Usage in CI/CD:</p> <pre><code># Validate before deployment\nlablink-validate-config config/config.yaml &amp;&amp; terraform apply || exit 1\n</code></pre>"},{"location":"configuration/#check-syntax","title":"Check Syntax","text":"<pre><code># YAML syntax check\npython -c \"import yaml; yaml.safe_load(open('lablink-infrastructure/config/config.yaml'))\"\n</code></pre>"},{"location":"configuration/#test-locally","title":"Test Locally","text":"<pre><code># Run allocator with custom config\ncd packages/allocator\npython src/lablink_allocator_service/main.py\n</code></pre>"},{"location":"configuration/#terraform-validation","title":"Terraform Validation","text":"<pre><code>cd lablink-infrastructure\nterraform validate\nterraform plan  # Preview changes\n</code></pre>"},{"location":"configuration/#common-configuration-patterns","title":"Common Configuration Patterns","text":""},{"location":"configuration/#use-your-own-research-software","title":"Use Your Own Research Software","text":"<pre><code>machine:\n  machine_type: \"g4dn.2xlarge\"\n  image: \"ghcr.io/yourorg/your-research-image:latest\"\n  repository: \"https://github.com/yourorg/your-research-code.git\"\n  software: \"your-software-name\"\n</code></pre> <p>See Adapting LabLink for complete guide.</p>"},{"location":"configuration/#multiple-gpu-types","title":"Multiple GPU Types","text":"<p>Create environment-specific configs:</p> <p><code>config-cpu.yaml</code> (for testing): <pre><code>machine:\n  machine_type: \"t2.medium\"\n  ami_id: \"ami-0c55b159cbfafe1f0\"\n</code></pre></p> <p><code>config-gpu.yaml</code> (for production): <pre><code>machine:\n  machine_type: \"g5.xlarge\"\n  ami_id: \"ami-067cc81f948e50e06\"\n</code></pre></p> <p>Use with Hydra: <pre><code>python main.py --config-name=config-gpu\n</code></pre></p>"},{"location":"configuration/#custom-database","title":"Custom Database","text":"<p>Use external PostgreSQL (RDS):</p> <pre><code>db:\n  dbname: \"lablink_production\"\n  user: \"lablink_admin\"\n  password: \"${DB_PASSWORD}\"\n  host: \"lablink-db.cluster-xxxxx.us-west-2.rds.amazonaws.com\"\n  port: 5432\n</code></pre>"},{"location":"configuration/#configuration-best-practices","title":"Configuration Best Practices","text":"<ol> <li>Never commit secrets: Use environment variables or AWS Secrets Manager</li> <li>Pin versions in production: Use specific image tags, not <code>:latest</code></li> <li>Document custom values: Add comments explaining non-standard configurations</li> <li>Test configuration changes: Validate with <code>terraform plan</code> before applying</li> <li>Use separate configs per environment: Don't reuse dev configs in production</li> </ol>"},{"location":"configuration/#troubleshooting-configuration","title":"Troubleshooting Configuration","text":""},{"location":"configuration/#config-not-loading","title":"Config Not Loading","text":"<p>Check file location and syntax: <pre><code>python -c \"import yaml; print(yaml.safe_load(open('conf/config.yaml')))\"\n</code></pre></p>"},{"location":"configuration/#environment-variables-not-working","title":"Environment Variables Not Working","text":"<p>Verify export and check case sensitivity: <pre><code>env | grep -i lablink\necho $DB_PASSWORD\n</code></pre></p>"},{"location":"configuration/#terraform-variables-not-applied","title":"Terraform Variables Not Applied","text":"<p>Ensure <code>-var</code> flags are passed: <pre><code>terraform plan -var=\"resource_suffix=prod\"\n</code></pre></p>"},{"location":"configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Adapting LabLink: Customize for your research software</li> <li>Deployment: Deploy with your configuration</li> <li>Security: Secure your configuration values</li> </ul>"},{"location":"contributing-docs/","title":"Contributing to Documentation","text":"<p>This guide covers how to contribute to LabLink documentation.</p>"},{"location":"contributing-docs/#documentation-system","title":"Documentation System","text":"<p>LabLink uses MkDocs with the Material theme for documentation.</p> <p>Key Features: - Markdown-based documentation - Automatic API reference generation from Python docstrings - Automatic changelog generation from git history - Version management (multiple doc versions) - Search functionality - Dark/light mode</p>"},{"location":"contributing-docs/#quick-start","title":"Quick Start","text":""},{"location":"contributing-docs/#setup","title":"Setup","text":"<p>Option 1: Using uv (Recommended)</p> <pre><code># Clone repository\ngit clone https://github.com/talmolab/lablink.git\ncd lablink\n\n# Quick test (creates temporary environment automatically)\nuv run --extra docs mkdocs serve\n\n# Or create persistent virtual environment\nuv venv .venv-docs\n# Windows\n.venv-docs\\Scripts\\activate\n# macOS/Linux\nsource .venv-docs/bin/activate\n\n# Install dependencies\nuv sync --extra docs\n</code></pre> <p>Option 2: Using pip</p> <pre><code># Clone repository\ngit clone https://github.com/talmolab/lablink.git\ncd lablink\n\n# Create virtual environment\npython -m venv .venv-docs\n# Windows\n.venv-docs\\Scripts\\activate\n# macOS/Linux\nsource .venv-docs/bin/activate\n\n# Install documentation dependencies (from pyproject.toml)\npip install -e \".[docs]\"\n</code></pre>"},{"location":"contributing-docs/#build-and-preview","title":"Build and Preview","text":"<pre><code># Serve documentation locally\nmkdocs serve\n\n# Open http://localhost:8000 in browser\n</code></pre> <p>Changes to <code>.md</code> files will auto-reload in the browser.</p>"},{"location":"contributing-docs/#build-static-site","title":"Build Static Site","text":"<pre><code># Build documentation\nmkdocs build\n\n# Output in site/ directory\n</code></pre>"},{"location":"contributing-docs/#documentation-structure","title":"Documentation Structure","text":"<pre><code>lablink/\n\u251c\u2500\u2500 mkdocs.yml              # MkDocs configuration\n\u251c\u2500\u2500 pyproject.toml          # Python dependencies (docs extra)\n\u251c\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 index.md           # Homepage\n\u2502   \u251c\u2500\u2500 prerequisites.md   # Getting Started section\n\u2502   \u251c\u2500\u2500 quickstart.md\n\u2502   \u251c\u2500\u2500 installation.md\n\u2502   \u251c\u2500\u2500 architecture.md    # User Guides section\n\u2502   \u251c\u2500\u2500 configuration.md\n\u2502   \u251c\u2500\u2500 adapting.md\n\u2502   \u251c\u2500\u2500 deployment.md\n\u2502   \u251c\u2500\u2500 workflows.md\n\u2502   \u251c\u2500\u2500 ssh-access.md\n\u2502   \u251c\u2500\u2500 database.md\n\u2502   \u251c\u2500\u2500 testing.md\n\u2502   \u251c\u2500\u2500 aws-setup.md       # AWS Setup section\n\u2502   \u251c\u2500\u2500 security.md\n\u2502   \u251c\u2500\u2500 cost-estimation.md\n\u2502   \u251c\u2500\u2500 troubleshooting.md # Reference section\n\u2502   \u251c\u2500\u2500 faq.md\n\u2502   \u251c\u2500\u2500 contributing-docs.md\n\u2502   \u251c\u2500\u2500 scripts/\n\u2502   \u2502   \u251c\u2500\u2500 gen_ref_pages.py    # Auto-generates API docs\n\u2502   \u2502   \u2514\u2500\u2500 gen_changelog.py    # Auto-generates changelog\n\u2502   \u2514\u2500\u2500 assets/            # Images, diagrams, etc.\n\u2514\u2500\u2500 .github/workflows/\n    \u2514\u2500\u2500 docs.yml           # Documentation CI/CD\n</code></pre>"},{"location":"contributing-docs/#writing-documentation","title":"Writing Documentation","text":""},{"location":"contributing-docs/#markdown-basics","title":"Markdown Basics","text":"<pre><code># Page Title (H1)\n\n## Section (H2)\n\n### Subsection (H3)\n\n**Bold text**\n*Italic text*\n`inline code`\n\n[Link text](https://example.com)\n[Internal link](other-page.md)\n\n- Bullet list\n- Item 2\n\n1. Numbered list\n2. Item 2\n</code></pre>"},{"location":"contributing-docs/#code-blocks","title":"Code Blocks","text":"<p>Use fenced code blocks with language specification:</p> <pre><code>```python\ndef hello_world():\n    print(\"Hello, World!\")\n```\n\n```bash\nterraform apply -var=\"resource_suffix=dev\"\n```\n\n```yaml\ndb:\n  host: localhost\n  port: 5432\n```\n</code></pre>"},{"location":"contributing-docs/#admonitions","title":"Admonitions","text":"<p>Use admonitions for notes, warnings, tips:</p> <pre><code>!!! note\n    This is a note.\n\n!!! warning\n    This is a warning.\n\n!!! tip\n    This is a helpful tip.\n\n!!! danger\n    This is critical information.\n</code></pre> <p>Renders as:</p> <p>Note</p> <p>This is a note.</p> <p>Warning</p> <p>This is a warning.</p>"},{"location":"contributing-docs/#tabs","title":"Tabs","text":"<p>For multi-option content:</p> <pre><code>=== \"macOS\"\n    ```bash\n    brew install terraform\n    ```\n\n=== \"Linux\"\n    ```bash\n    wget https://releases.hashicorp.com/terraform/...\n    ```\n\n=== \"Windows\"\n    Download from terraform.io\n</code></pre> <p>Renders as:</p> macOSLinuxWindows <pre><code>brew install terraform\n</code></pre> <pre><code>wget https://releases.hashicorp.com/terraform/...\n</code></pre> <p>Download from terraform.io</p>"},{"location":"contributing-docs/#tables","title":"Tables","text":"<pre><code>| Column 1 | Column 2 | Column 3 |\n|----------|----------|----------|\n| Value 1  | Value 2  | Value 3  |\n| Value 4  | Value 5  | Value 6  |\n</code></pre>"},{"location":"contributing-docs/#internal-links","title":"Internal Links","text":"<pre><code>See [Configuration](configuration.md) for details.\n\nLink to specific section: [Configuration \u2192 Database](configuration.md#database-options-db)\n</code></pre>"},{"location":"contributing-docs/#images","title":"Images","text":"<pre><code>![Alt text](assets/diagram.png)\n\n# With caption\n&lt;figure markdown&gt;\n  ![Alt text](assets/diagram.png)\n  &lt;figcaption&gt;Caption text&lt;/figcaption&gt;\n&lt;/figure&gt;\n</code></pre>"},{"location":"contributing-docs/#documentation-guidelines","title":"Documentation Guidelines","text":""},{"location":"contributing-docs/#style-guide","title":"Style Guide","text":"<ol> <li>Be concise: Short sentences, clear language</li> <li>Use active voice: \"Run the command\" not \"The command should be run\"</li> <li>Include examples: Show don't just tell</li> <li>Test commands: Verify all bash commands work</li> <li>Update dates: Use current years in examples</li> <li>Cross-reference: Link to related pages</li> <li>Use consistent terminology: \"allocator\" not \"allocator server\" or \"allocation service\"</li> </ol>"},{"location":"contributing-docs/#page-structure","title":"Page Structure","text":"<p>Every documentation page should have:</p> <ol> <li>Title (H1): Page name</li> <li>Introduction: Brief overview (1-2 sentences)</li> <li>Main content: Organized with H2/H3 sections</li> <li>Examples: Code samples and use cases</li> <li>Related links: \"Next Steps\" or \"See Also\" section</li> </ol> <p>Example Template:</p> <pre><code># Page Title\n\nBrief introduction explaining what this page covers.\n\n## Main Section\n\nContent here.\n\n### Subsection\n\nMore detailed content.\n\n## Examples\n\nPractical examples.\n\n## Troubleshooting\n\nCommon issues.\n\n## Next Steps\n\n- [Related Page 1](page1.md)\n- [Related Page 2](page2.md)\n</code></pre>"},{"location":"contributing-docs/#code-examples","title":"Code Examples","text":"<p>Good: <pre><code># Comment explaining what this does\nterraform apply -var=\"resource_suffix=dev\"\n</code></pre></p> <p>Bad: <pre><code>terraform apply\n</code></pre></p> <p>Always: - Include comments - Show complete commands - Provide context - Test before documenting</p>"},{"location":"contributing-docs/#command-documentation","title":"Command Documentation","text":"<p>When documenting commands:</p> <ol> <li>Show the command</li> <li>Explain what it does</li> <li>Show expected output (if helpful)</li> <li>Mention common errors</li> </ol> <p>Example:</p> <pre><code>### Connect to Allocator\n\nSSH into the allocator instance:\n\n\\`\\`\\`bash\nssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\n\\`\\`\\`\n\n**Expected output**:\n\\`\\`\\`\nWelcome to Ubuntu 20.04.6 LTS\n...\nubuntu@ip-xxx-xx-xx-xx:~$\n\\`\\`\\`\n\n**Common errors**: See [Troubleshooting \u2192 SSH Issues](troubleshooting.md#ssh-access-issues)\n</code></pre>"},{"location":"contributing-docs/#api-documentation","title":"API Documentation","text":""},{"location":"contributing-docs/#python-docstrings","title":"Python Docstrings","text":"<p>API documentation is auto-generated from docstrings. Use Google-style docstrings:</p> <pre><code>def request_vm(email: str, crd_command: str) -&gt; dict:\n    \"\"\"Request a VM from the allocator.\n\n    Args:\n        email: User email address\n        crd_command: Command to execute on the VM\n\n    Returns:\n        Dictionary containing VM assignment details:\n        - hostname: VM hostname\n        - status: VM status\n        - assigned_at: Assignment timestamp\n\n    Raises:\n        ValueError: If email is invalid\n        RuntimeError: If no VMs available\n\n    Example:\n        &gt;&gt;&gt; result = request_vm(\"user@example.com\", \"python train.py\")\n        &gt;&gt;&gt; print(result['hostname'])\n        i-0abc123def456\n    \"\"\"\n    # Implementation\n</code></pre>"},{"location":"contributing-docs/#documenting-new-modules","title":"Documenting New Modules","text":"<p>When adding new Python modules:</p> <ol> <li>Add docstrings to all public functions/classes</li> <li>Run docs build to see generated API docs:    <pre><code>mkdocs serve\n# Navigate to Reference \u2192 API Reference\n</code></pre></li> <li>Verify documentation is clear and complete</li> </ol>"},{"location":"contributing-docs/#configuration-reference","title":"Configuration Reference","text":"<p>When documenting configuration options:</p> <ol> <li>Use tables for option lists</li> <li>Include:</li> <li>Option name</li> <li>Type</li> <li>Default value</li> <li>Description</li> <li>Provide examples</li> </ol> <p>Example:</p> <pre><code>### Database Options (`db`)\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `dbname` | string | `lablink_db` | Database name |\n| `user` | string | `lablink` | Database username |\n| `password` | string | `lablink` | Database password |\n\n**Example**:\n\\`\\`\\`yaml\ndb:\n  dbname: \"lablink_db\"\n  user: \"lablink\"\n  password: \"secure_password\"\n\\`\\`\\`\n</code></pre>"},{"location":"contributing-docs/#updating-navigation","title":"Updating Navigation","text":"<p>Navigation is defined in <code>mkdocs.yml</code>:</p> <pre><code>nav:\n  - Home: index.md\n  - Getting Started:\n      - Prerequisites: prerequisites.md\n      - Installation: installation.md\n  - User Guides:\n      - Configuration: configuration.md\n      - Deployment: deployment.md\n</code></pre> <p>When adding a new page:</p> <ol> <li>Create the <code>.md</code> file in <code>docs/</code></li> <li>Add entry to <code>nav</code> in <code>mkdocs.yml</code></li> <li>Build docs to verify</li> </ol>"},{"location":"contributing-docs/#adding-assets","title":"Adding Assets","text":""},{"location":"contributing-docs/#images_1","title":"Images","text":"<ol> <li>Place images in <code>docs/assets/</code></li> <li>Use descriptive names: <code>architecture-diagram.png</code></li> <li>Reference in markdown:    <pre><code>![Architecture Diagram](assets/architecture-diagram.png)\n</code></pre></li> </ol>"},{"location":"contributing-docs/#diagrams","title":"Diagrams","text":"<p>Preferred: Use Mermaid diagrams for all documentation visuals. Mermaid is text-based, version-controlled, and fully supported by MkDocs Material.</p>"},{"location":"contributing-docs/#when-to-use-mermaid","title":"When to Use Mermaid","text":"<ul> <li>Flowcharts: Decision trees, process flows, CI/CD pipelines</li> <li>Sequence Diagrams: Component interactions, API flows, service communication</li> <li>State Diagrams: VM lifecycle, status transitions</li> <li>ER Diagrams: Database schemas, table relationships</li> <li>Graphs: System architecture, deployment diagrams</li> </ul>"},{"location":"contributing-docs/#mermaid-examples","title":"Mermaid Examples","text":"<p>Flowchart (Decision Tree):</p> <pre><code>\\`\\`\\`mermaid\nflowchart TD\n    Start[User Request] --&gt; Check{VM Available?}\n    Check --&gt;|Yes| Assign[Assign VM]\n    Check --&gt;|No| Error[Return Error]\n    Assign --&gt; Notify[Notify Client]\n    Notify --&gt; End[Return Hostname]\n\\`\\`\\`\n</code></pre> <p>Sequence Diagram (Component Interaction):</p> <pre><code>\\`\\`\\`mermaid\nsequenceDiagram\n    participant User\n    participant Flask as Flask App\n    participant DB as PostgreSQL\n\n    User-&gt;&gt;Flask: POST /request_vm\n    Flask-&gt;&gt;DB: SELECT available VM\n    DB--&gt;&gt;Flask: Return VM details\n    Flask--&gt;&gt;User: Return hostname\n\\`\\`\\`\n</code></pre> <p>State Diagram (Lifecycle):</p> <pre><code>\\`\\`\\`mermaid\nstateDiagram-v2\n    [*] --&gt; available: VM Created\n    available --&gt; in_use: VM Assigned\n    in_use --&gt; available: VM Released\n    in_use --&gt; failed: Health Check Failed\n\\`\\`\\`\n</code></pre> <p>ER Diagram (Database Schema):</p> <pre><code>\\`\\`\\`mermaid\nerDiagram\n    VMS {\n        int id PK\n        string hostname UK\n        string status\n    }\n\\`\\`\\`\n</code></pre>"},{"location":"contributing-docs/#mermaid-styling-guidelines","title":"Mermaid Styling Guidelines","text":"<ul> <li>Use consistent colors for similar components across diagrams</li> <li>Keep diagrams focused (max 10-15 nodes)</li> <li>Use clear, concise labels with action verbs</li> <li>Include HTTP methods for API calls (POST, GET, etc.)</li> <li>Test rendering in both light and dark modes</li> </ul>"},{"location":"contributing-docs/#resources","title":"Resources","text":"<ul> <li>Mermaid Documentation</li> <li>Mermaid Live Editor - Test diagrams before adding to docs</li> <li>MkDocs Material Diagrams</li> </ul>"},{"location":"contributing-docs/#alternative-ascii-art","title":"Alternative: ASCII Art","text":"<p>For very simple diagrams where Mermaid would be overkill, ASCII art is acceptable:</p> <pre><code>User \u2192 Allocator \u2192 Client VM\n</code></pre>"},{"location":"contributing-docs/#versioning-documentation","title":"Versioning Documentation","text":"<p>Documentation is versioned using <code>mike</code>:</p>"},{"location":"contributing-docs/#version-names","title":"Version Names","text":"<ul> <li><code>latest</code>: Latest stable release</li> <li><code>v1.0.0</code>, <code>v1.1.0</code>, etc.: Specific versions</li> <li><code>dev</code>: Development/unreleased changes</li> </ul>"},{"location":"contributing-docs/#deploy-new-version","title":"Deploy New Version","text":"<pre><code># Deploy version 1.0.0 as latest\nmike deploy 1.0.0 latest --update-aliases\nmike set-default latest\n\n# Deploy dev version\nmike deploy dev\n\n# List versions\nmike list\n\n# Delete version\nmike delete v0.9.0\n</code></pre>"},{"location":"contributing-docs/#version-workflow","title":"Version Workflow","text":"<ol> <li>On main branch push: Deploy as <code>dev</code></li> <li>On release: Deploy as version number + <code>latest</code></li> </ol>"},{"location":"contributing-docs/#cicd-workflow","title":"CI/CD Workflow","text":"<p>Documentation is built and deployed via GitHub Actions (<code>.github/workflows/docs.yml</code>).</p> <p>Triggers: - Push to <code>main</code> \u2192 Deploy <code>dev</code> docs - Release published \u2192 Deploy versioned docs - Pull request \u2192 Build only (no deploy)</p> <p>Process: 1. Checkout code 2. Setup Python 3. Install dependencies 4. Run <code>mike deploy</code> (or <code>mkdocs build</code>) 5. Push to <code>gh-pages</code> branch</p>"},{"location":"contributing-docs/#testing-documentation","title":"Testing Documentation","text":""},{"location":"contributing-docs/#before-committing","title":"Before Committing","text":"<ol> <li> <p>Build locally:    <pre><code>mkdocs build --strict\n</code></pre> <code>--strict</code> treats warnings as errors</p> </li> <li> <p>Serve locally:    <pre><code>mkdocs serve\n</code></pre>    Review changes in browser</p> </li> <li> <p>Check links:</p> </li> <li>Click through all internal links</li> <li> <p>Verify external links work</p> </li> <li> <p>Test code examples:</p> </li> <li>Copy-paste commands and verify they work</li> <li>Test on clean environment if possible</li> </ol>"},{"location":"contributing-docs/#validation-checklist","title":"Validation Checklist","text":"<ul> <li> All links work (internal and external)</li> <li> Code blocks have language specified</li> <li> Commands tested and work</li> <li> Images display correctly</li> <li> Tables render properly</li> <li> No typos or grammar errors</li> <li> Follows style guide</li> <li> Cross-references added where relevant</li> </ul>"},{"location":"contributing-docs/#common-issues","title":"Common Issues","text":""},{"location":"contributing-docs/#link-not-working","title":"Link Not Working","text":"<p>Problem: Link shows 404</p> <p>Solution: - Use relative paths: <code>[Text](other-page.md)</code> not <code>[Text](/other-page.md)</code> - For sections: <code>[Text](page.md#section-heading)</code> - Check file exists in <code>docs/</code> directory</p>"},{"location":"contributing-docs/#code-block-not-highlighting","title":"Code Block Not Highlighting","text":"<p>Problem: Code block appears as plain text</p> <p>Solution: - Specify language: <code>```python</code> not just <code>```</code> - Check language name is correct: <code>bash</code> not <code>shell</code>, <code>yaml</code> not <code>yml</code></p>"},{"location":"contributing-docs/#admonition-not-rendering","title":"Admonition Not Rendering","text":"<p>Problem: Admonition shows as plain text</p> <p>Solution: <pre><code># Correct\n!!! note\n    Content indented with 4 spaces\n\n# Wrong\n!!! note\nContent not indented\n</code></pre></p>"},{"location":"contributing-docs/#table-not-aligning","title":"Table Not Aligning","text":"<p>Problem: Table cells misaligned</p> <p>Solution: - Ensure same number of columns in header and rows - Align pipes vertically (not required but helps) - Use markdown table formatter</p>"},{"location":"contributing-docs/#contributing-workflow","title":"Contributing Workflow","text":"<ol> <li> <p>Fork repository (if not a maintainer)</p> </li> <li> <p>Create branch:    <pre><code>git checkout -b docs/improve-configuration-page\n</code></pre></p> </li> <li> <p>Make changes:</p> </li> <li>Edit markdown files</li> <li>Add/update examples</li> <li> <p>Test locally with <code>mkdocs serve</code></p> </li> <li> <p>Commit:    <pre><code>git add docs/\ngit commit -m \"docs: improve configuration examples\"\n</code></pre></p> </li> <li> <p>Push:    <pre><code>git push origin docs/improve-configuration-page\n</code></pre></p> </li> <li> <p>Open Pull Request:</p> </li> <li>Clear title describing changes</li> <li>Description explaining what and why</li> <li> <p>Screenshots if visual changes</p> </li> <li> <p>Address feedback:</p> </li> <li>Respond to review comments</li> <li>Make requested changes</li> <li> <p>Push updates</p> </li> <li> <p>Merge:</p> </li> <li>Once approved, PR will be merged</li> <li>Documentation will auto-deploy</li> </ol>"},{"location":"contributing-docs/#getting-help","title":"Getting Help","text":"<ul> <li>Questions: Open GitHub issue with <code>documentation</code> label</li> <li>Suggestions: Open GitHub discussion</li> <li>Bugs in docs: Open GitHub issue</li> </ul>"},{"location":"contributing-docs/#resources_1","title":"Resources","text":"<ul> <li>MkDocs Documentation</li> <li>Material for MkDocs</li> <li>Python Markdown</li> <li>mkdocstrings</li> </ul>"},{"location":"contributing-docs/#quick-reference","title":"Quick Reference","text":"<pre><code># Install dependencies (uv)\nuv sync --extra docs\n\n# Install dependencies (pip)\npip install -e \".[docs]\"\n\n# Serve locally\nmkdocs serve\n\n# Build documentation\nmkdocs build --strict\n\n# Deploy version\nmike deploy 1.0.0 latest\n\n# View deployed versions\nmike list\n</code></pre>"},{"location":"contributing/","title":"Contributing to LabLink","text":"<p>Thank you for your interest in contributing to LabLink! This guide will help you get started.</p>"},{"location":"contributing/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Code of Conduct</li> <li>Getting Started</li> <li>Development Setup</li> <li>How to Contribute</li> <li>Contribution Workflow</li> <li>Coding Standards</li> <li>Testing</li> <li>Documentation</li> <li>Pull Request Process</li> <li>Getting Help</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>This project adheres to a code of conduct that we expect all contributors to follow. Please be respectful and constructive in all interactions.</p> <p>Expected Behavior: - Be respectful and inclusive - Welcome newcomers and help them learn - Focus on what is best for the community - Show empathy towards other community members</p> <p>Unacceptable Behavior: - Harassment, discrimination, or offensive comments - Trolling or insulting/derogatory comments - Public or private harassment - Publishing others' private information without permission</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":""},{"location":"contributing/#ways-to-contribute","title":"Ways to Contribute","text":"<ul> <li>\ud83d\udc1b Report bugs: Open an issue describing the problem</li> <li>\u2728 Suggest features: Open an issue with enhancement label</li> <li>\ud83d\udcdd Improve documentation: Fix typos, add examples, clarify instructions</li> <li>\ud83d\udd27 Fix bugs: Submit pull requests for open issues</li> <li>\ud83d\ude80 Add features: Implement new functionality</li> <li>\ud83e\uddea Write tests: Improve test coverage</li> <li>\ud83d\udcac Help others: Answer questions in issues and discussions</li> </ul>"},{"location":"contributing/#before-you-start","title":"Before You Start","text":"<ol> <li>Check existing issues: Someone may already be working on it</li> <li>Open an issue first: For major changes, discuss before implementing</li> <li>Read the docs: Familiarize yourself with LabLink architecture</li> <li>Review CLAUDE.md: Developer-focused project overview</li> </ol>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9 or higher</li> <li>Docker and Docker Desktop running</li> <li>AWS CLI (for testing infrastructure)</li> <li>Terraform 1.6.6+ (for testing infrastructure)</li> <li>Git</li> </ul>"},{"location":"contributing/#local-setup","title":"Local Setup","text":"<pre><code># Fork and clone the repository\ngit clone https://github.com/YOUR_USERNAME/lablink.git\ncd lablink\n\n# Install uv (recommended Python package manager)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Setup allocator service\ncd packages/allocator\nuv sync --extra dev\n\n# Setup client service\ncd ../client\nuv sync --extra dev\n\n# Return to root\ncd ../..\n</code></pre>"},{"location":"contributing/#verify-setup","title":"Verify Setup","text":"<pre><code># Run allocator tests\ncd packages/allocator\nPYTHONPATH=. pytest\n\n# Run client tests\ncd ../client\nPYTHONPATH=. pytest\n\n# Run linting\nruff check .\n\n# Build Docker images (development)\ndocker build -t lablink-allocator:dev -f packages/allocator/Dockerfile.dev .\ndocker build -t lablink-client:dev -f packages/client/Dockerfile.dev .\n</code></pre>"},{"location":"contributing/#how-to-contribute","title":"How to Contribute","text":""},{"location":"contributing/#reporting-bugs","title":"Reporting Bugs","text":"<p>Before reporting: 1. Check if the bug has already been reported 2. Try to reproduce on the latest version 3. Check the Troubleshooting Guide</p> <p>When reporting, include: - Clear, descriptive title - Steps to reproduce the issue - Expected behavior vs actual behavior - Error messages and logs - Environment details (OS, Python version, Docker version) - Screenshots (if applicable)</p> <p>Example bug report:</p> <p><pre><code>**Title**: PostgreSQL connection fails after deployment\n\n**Description**:\nAfter deploying the allocator to AWS, cannot connect to PostgreSQL database.\n\n**Steps to Reproduce**:\n1. Deploy allocator with `terraform apply`\n2. SSH into instance\n3. Try to access database: `psql -U lablink -d lablink_db`\n\n**Expected**: Successfully connect to database\n\n**Actual**: Connection refused error\n\n**Environment**:\n- OS: Ubuntu 20.04\n- Terraform: 1.6.6\n- Image tag: linux-amd64-latest\n\n**Logs**:\n</code></pre> [error logs here] <pre><code>\n</code></pre></p>"},{"location":"contributing/#suggesting-features","title":"Suggesting Features","text":"<p>Before suggesting: 1. Check if it's already suggested or implemented 2. Consider if it fits the project scope 3. Think about backwards compatibility</p> <p>When suggesting, include: - Clear, descriptive title - Use case and motivation - Proposed solution - Alternative solutions considered - Impact on existing functionality</p> <p>Example feature request:</p> <pre><code>**Title**: Add support for Azure cloud provider\n\n**Use Case**:\nSome research institutions use Azure instead of AWS and would benefit from LabLink.\n\n**Proposed Solution**:\n- Add Azure provider to Terraform configurations\n- Support Azure VMs alongside EC2\n- Document Azure-specific setup\n\n**Alternatives**:\n- Create separate fork for Azure\n- Use abstraction layer for multi-cloud support\n\n**Impact**:\n- Requires significant changes to infrastructure code\n- Need Azure-specific configuration options\n- May need separate documentation\n</code></pre>"},{"location":"contributing/#contribution-workflow","title":"Contribution Workflow","text":""},{"location":"contributing/#1-fork-the-repository","title":"1. Fork the Repository","text":"<p>Click \"Fork\" button on GitHub to create your copy.</p>"},{"location":"contributing/#2-create-a-branch","title":"2. Create a Branch","text":"<p>Use descriptive branch names:</p> <pre><code>git checkout -b feature/add-spot-instance-support\ngit checkout -b fix/postgresql-connection-issue\ngit checkout -b docs/improve-configuration-guide\n</code></pre> <p>Branch naming conventions: - <code>feature/</code> - New features - <code>fix/</code> - Bug fixes - <code>docs/</code> - Documentation changes - <code>refactor/</code> - Code refactoring - <code>test/</code> - Test additions or modifications</p>"},{"location":"contributing/#3-make-your-changes","title":"3. Make Your Changes","text":"<p>Follow the Coding Standards below.</p>"},{"location":"contributing/#4-test-your-changes","title":"4. Test Your Changes","text":"<pre><code># Run tests\nPYTHONPATH=. pytest\n\n# Run linting\nruff check .\n\n# Auto-fix linting issues\nruff check --fix .\n\n# Format code\nruff format .\n\n# Run type checking (if applicable)\nmypy .\n</code></pre>"},{"location":"contributing/#5-commit-your-changes","title":"5. Commit Your Changes","text":"<p>Use clear, descriptive commit messages following Conventional Commits:</p> <pre><code># Format: &lt;type&gt;(&lt;scope&gt;): &lt;description&gt;\n\ngit commit -m \"feat(allocator): add support for Spot Instances\"\ngit commit -m \"fix(database): resolve PostgreSQL connection timeout\"\ngit commit -m \"docs(security): add section on OIDC setup\"\ngit commit -m \"test(api): add tests for VM request endpoint\"\n</code></pre> <p>Commit types: - <code>feat</code> - New feature - <code>fix</code> - Bug fix - <code>docs</code> - Documentation changes - <code>style</code> - Code style changes (formatting, etc.) - <code>refactor</code> - Code refactoring - <code>test</code> - Adding or updating tests - <code>chore</code> - Maintenance tasks</p>"},{"location":"contributing/#6-push-to-your-fork","title":"6. Push to Your Fork","text":"<pre><code>git push origin feature/add-spot-instance-support\n</code></pre>"},{"location":"contributing/#7-open-a-pull-request","title":"7. Open a Pull Request","text":"<ol> <li>Go to the original repository</li> <li>Click \"New Pull Request\"</li> <li>Select your fork and branch</li> <li>Fill in the PR template (see below)</li> </ol>"},{"location":"contributing/#coding-standards","title":"Coding Standards","text":""},{"location":"contributing/#python-style","title":"Python Style","text":"<ul> <li>Follow PEP 8 style guide</li> <li>Use <code>ruff</code> for linting and formatting</li> <li>Maximum line length: 88 characters (Black default)</li> <li>Use type hints for function parameters and return values</li> <li>Write docstrings for all public functions and classes</li> </ul> <p>Example:</p> <pre><code>def request_vm(email: str, crd_command: str) -&gt; dict[str, str]:\n    \"\"\"Request a VM from the allocator.\n\n    Args:\n        email: User email address for VM assignment.\n        crd_command: Command to execute on the VM.\n\n    Returns:\n        Dictionary containing VM details:\n        - hostname: VM hostname\n        - status: Current VM status\n        - assigned_at: Assignment timestamp\n\n    Raises:\n        ValueError: If email format is invalid.\n        RuntimeError: If no VMs are available.\n\n    Example:\n        &gt;&gt;&gt; result = request_vm(\"user@example.com\", \"python train.py\")\n        &gt;&gt;&gt; print(result['hostname'])\n        i-0abc123def456\n    \"\"\"\n    if not validate_email(email):\n        raise ValueError(f\"Invalid email format: {email}\")\n\n    vm = get_available_vm()\n    if not vm:\n        raise RuntimeError(\"No VMs available\")\n\n    return assign_vm(vm, email, crd_command)\n</code></pre>"},{"location":"contributing/#terraform-style","title":"Terraform Style","text":"<ul> <li>Use descriptive resource names</li> <li>Add comments for complex logic</li> <li>Tag all resources with <code>Name</code>, <code>Project</code>, <code>Environment</code></li> <li>Use variables for configurable values</li> <li>Include outputs for important values</li> </ul> <p>Example:</p> <pre><code>resource \"aws_instance\" \"lablink_allocator\" {\n  ami           = var.ami_id\n  instance_type = var.instance_type\n\n  tags = {\n    Name        = \"lablink-allocator-${var.environment}\"\n    Project     = \"LabLink\"\n    Environment = var.environment\n    ManagedBy   = \"Terraform\"\n  }\n\n  # Security group allowing HTTP and SSH\n  vpc_security_group_ids = [aws_security_group.lablink.id]\n}\n</code></pre>"},{"location":"contributing/#documentation-style","title":"Documentation Style","text":"<ul> <li>Use clear, concise language</li> <li>Include code examples</li> <li>Test all commands before documenting</li> <li>Use consistent terminology</li> <li>Link to related documentation</li> </ul> <p>See Contributing to Documentation for detailed guidelines.</p>"},{"location":"contributing/#testing","title":"Testing","text":""},{"location":"contributing/#writing-tests","title":"Writing Tests","text":"<ul> <li>Write tests for all new functionality</li> <li>Maintain or improve code coverage</li> <li>Use descriptive test names</li> <li>Test both success and failure cases</li> <li>Mock external dependencies (AWS, database)</li> </ul> <p>Example test:</p> <pre><code>import pytest\nfrom unittest.mock import MagicMock, patch\n\ndef test_request_vm_success():\n    \"\"\"Test successful VM request.\"\"\"\n    mock_db = MagicMock()\n    mock_db.get_available_vm.return_value = {\n        'hostname': 'i-12345',\n        'status': 'available'\n    }\n\n    result = request_vm(\"user@example.com\", \"echo test\", db=mock_db)\n\n    assert result['hostname'] == 'i-12345'\n    assert result['status'] == 'in-use'\n    mock_db.update_vm_status.assert_called_once()\n\ndef test_request_vm_no_vms_available():\n    \"\"\"Test VM request when no VMs available.\"\"\"\n    mock_db = MagicMock()\n    mock_db.get_available_vm.return_value = None\n\n    with pytest.raises(RuntimeError, match=\"No VMs available\"):\n        request_vm(\"user@example.com\", \"echo test\", db=mock_db)\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\nPYTHONPATH=. pytest\n\n# Run specific test file\nPYTHONPATH=. pytest tests/test_api_calls.py\n\n# Run specific test\nPYTHONPATH=. pytest tests/test_api_calls.py::test_request_vm_success\n\n# Run with coverage\nPYTHONPATH=. pytest --cov=lablink_allocator --cov-report=html\n\n# View coverage report\nopen htmlcov/index.html  # macOS\nxdg-open htmlcov/index.html  # Linux\nstart htmlcov/index.html  # Windows\n</code></pre>"},{"location":"contributing/#test-requirements","title":"Test Requirements","text":"<ul> <li>All new features must have tests</li> <li>Bug fixes should include regression tests</li> <li>Tests must pass in CI before merging</li> <li>Aim for &gt;80% code coverage</li> </ul>"},{"location":"contributing/#documentation","title":"Documentation","text":""},{"location":"contributing/#updating-documentation","title":"Updating Documentation","text":"<p>When making changes that affect users:</p> <ol> <li>Update relevant docs in <code>docs/</code> directory</li> <li>Update docstrings for API changes</li> <li>Update README.md if adding major features</li> <li>Update CLAUDE.md if changing architecture</li> <li>Test documentation locally with <code>mkdocs serve</code></li> </ol>"},{"location":"contributing/#documentation-checklist","title":"Documentation Checklist","text":"<ul> <li> Docstrings updated for changed functions</li> <li> Relevant documentation pages updated</li> <li> Examples provided for new features</li> <li> Links to related documentation added</li> <li> Tested all commands/examples</li> <li> Screenshots added (if UI changes)</li> </ul>"},{"location":"contributing/#building-documentation-locally","title":"Building Documentation Locally","text":"<p>Using uv (Recommended):</p> <pre><code># Quick test (creates temporary environment automatically)\nuv run --extra docs mkdocs serve\n\n# Or create persistent virtual environment\nuv venv .venv-docs\n# Windows\n.venv-docs\\Scripts\\activate\n# macOS/Linux\nsource .venv-docs/bin/activate\n\n# Install dependencies\nuv sync --extra docs\n\n# Serve documentation\nmkdocs serve\n\n# Open http://localhost:8000\n\n# Build static site\nmkdocs build\n</code></pre> <p>Using pip:</p> <pre><code># Install docs dependencies\npip install -e \".[docs]\"\n\n# Serve documentation\nmkdocs serve\n\n# Open http://localhost:8000\n\n# Build static site\nmkdocs build\n</code></pre>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":""},{"location":"contributing/#pr-template","title":"PR Template","text":"<p>When opening a PR, include:</p> <pre><code>## Description\nBrief description of changes\n\n## Type of Change\n- [ ] Bug fix (non-breaking change fixing an issue)\n- [ ] New feature (non-breaking change adding functionality)\n- [ ] Breaking change (fix or feature causing existing functionality to change)\n- [ ] Documentation update\n\n## Related Issue\nFixes #(issue number)\n\n## Changes Made\n- Bullet list of changes\n- Be specific\n\n## Testing\n- [ ] Tests added/updated\n- [ ] All tests pass locally\n- [ ] Documentation updated\n\n## Screenshots (if applicable)\nAdd screenshots for UI changes\n\n## Checklist\n- [ ] Code follows project style guidelines\n- [ ] Self-review completed\n- [ ] Comments added for complex code\n- [ ] Documentation updated\n- [ ] No new warnings generated\n- [ ] Tests added and passing\n- [ ] Branch is up to date with main\n</code></pre>"},{"location":"contributing/#pr-review-process","title":"PR Review Process","text":"<ol> <li>Automated checks run (CI tests, linting)</li> <li>Maintainer review for code quality and design</li> <li>Address feedback by pushing new commits</li> <li>Approval from at least one maintainer</li> <li>Merge by maintainer (squash merge preferred)</li> </ol>"},{"location":"contributing/#pr-guidelines","title":"PR Guidelines","text":"<p>Do: - Keep PRs focused on a single concern - Write clear PR descriptions - Respond to feedback promptly - Keep PRs up to date with main branch - Be respectful of reviewers' time</p> <p>Don't: - Mix multiple unrelated changes - Submit huge PRs (&gt;500 lines if possible) - Make breaking changes without discussion - Merge without approval - Force push after review starts</p>"},{"location":"contributing/#release-process","title":"Release Process","text":"<p>LabLink uses independent versioning for its two packages. Maintainers follow this process for releases:</p>"},{"location":"contributing/#package-versions","title":"Package Versions","text":"<ul> <li>lablink-allocator-service: VM Allocator Service</li> <li>lablink-client-service: Client Service</li> </ul> <p>Each package is versioned and released independently following Semantic Versioning.</p>"},{"location":"contributing/#release-workflow","title":"Release Workflow","text":""},{"location":"contributing/#1-prepare-the-release","title":"1. Prepare the Release","text":"<pre><code># Update version in pyproject.toml\ncd packages/allocator  # or packages/client\n# Edit pyproject.toml: version = \"0.3.0\"\n\n# Commit the version bump\ngit add pyproject.toml\ngit commit -m \"chore: bump lablink-allocator-service to 0.3.0\"\ngit push origin main\n</code></pre>"},{"location":"contributing/#2-test-with-dry-run-recommended","title":"2. Test with Dry Run (Recommended)","text":"<p>Before creating a release, test the build process:</p> <pre><code># For allocator\ngh workflow run \"Publish Python Packages\" \\\n  -f package=lablink-allocator-service \\\n  -f dry_run=true\n\n# For client\ngh workflow run \"Publish Python Packages\" \\\n  -f package=lablink-client-service \\\n  -f dry_run=true\n\n# Monitor the workflow\ngh run watch\n</code></pre> <p>The dry run will verify: - \u2705 Package metadata is correct - \u2705 Linting passes - \u2705 Tests pass - \u2705 Package builds successfully</p>"},{"location":"contributing/#3-create-github-release","title":"3. Create GitHub Release","text":"<p>Once the dry run passes, create the release:</p> <pre><code># For lablink-allocator-service\ngh release create lablink-allocator-service_v0.3.0 \\\n  --title \"lablink-allocator-service v0.3.0\" \\\n  --notes \"## Changes\n\n### Features\n- New feature X (#123)\n- Enhancement Y (#124)\n\n### Bug Fixes\n- Fixed issue Z (#125)\n\n### Documentation\n- Updated configuration docs\n\n## Installation\n\\`\\`\\`bash\npip install lablink-allocator-service==0.3.0\n\\`\\`\\`\n\"\n\n# For lablink-client-service\ngh release create lablink-client-service_v0.1.5 \\\n  --title \"lablink-client-service v0.1.5\" \\\n  --notes \"## Changes\n\n### Features\n- New feature A (#130)\n\n### Bug Fixes\n- Fixed bug B (#131)\n\n## Installation\n\\`\\`\\`bash\npip install lablink-client-service==0.1.5\n\\`\\`\\`\n\"\n</code></pre>"},{"location":"contributing/#4-automated-publishing","title":"4. Automated Publishing","text":"<p>When you create the GitHub Release, the <code>Publish Python Packages</code> workflow automatically:</p> <ol> <li>Verifies the release is from the <code>main</code> branch</li> <li>Checks the tag version matches <code>pyproject.toml</code></li> <li>Validates package metadata</li> <li>Runs linting checks</li> <li>Executes test suite</li> <li>Builds the package</li> <li>Publishes to PyPI using OIDC (no API token needed)</li> <li>Displays Docker build command for creating production images</li> </ol>"},{"location":"contributing/#5-build-production-docker-images","title":"5. Build Production Docker Images","text":"<p>After publishing to PyPI, manually trigger Docker image builds to create production images with version tags.</p> <p>Using GitHub CLI: <pre><code># Build both images with their respective versions\ngh workflow run lablink-images.yml \\\n  -f environment=prod \\\n  -f allocator_version=0.3.0 \\\n  -f client_version=0.1.5\n\n# Monitor the build\ngh run watch\n</code></pre></p> <p>Using GitHub UI: 1. Go to Actions \u2192 Build and Push Docker Images 2. Click \"Run workflow\" 3. Fill in:    - Branch: <code>main</code>    - Environment: <code>prod</code>    - Allocator version: <code>0.3.0</code>    - Client version: <code>0.1.5</code> 4. Click \"Run workflow\"</p> <p>This creates Docker images tagged with: - <code>ghcr.io/talmolab/lablink-allocator-image:0.3.0</code> (version tag) - <code>ghcr.io/talmolab/lablink-client-base-image:0.1.5</code> (version tag) - <code>latest</code> tags for both images - Plus platform-specific and metadata tags</p> <p>See Image Tagging Strategy for complete tag details.</p>"},{"location":"contributing/#release-guardrails","title":"Release Guardrails","text":"<p>The workflow includes several safety checks:</p> Check Purpose Failure Action Branch verification Ensures releases only from <code>main</code> Blocks publish Version match Tag must equal <code>pyproject.toml</code> Blocks publish Metadata validation All required fields present Blocks publish Linting Code quality standards Blocks publish Test suite Functionality verification Blocks publish Build verification Package builds successfully Blocks publish"},{"location":"contributing/#tag-naming-convention","title":"Tag Naming Convention","text":"<p>Format: <code>&lt;package-name&gt;_v&lt;version&gt;</code></p> <p>Examples: - <code>lablink-allocator-service_v0.3.0</code> - <code>lablink-allocator-service_v1.0.0-rc1</code> - <code>lablink-client-service_v0.1.5</code> - <code>lablink-client-service_v0.2.0-beta1</code></p>"},{"location":"contributing/#versioning-guidelines","title":"Versioning Guidelines","text":"<p>Follow Semantic Versioning:</p> <ul> <li>MAJOR (1.0.0): Breaking changes</li> <li>MINOR (0.1.0): New features, backwards compatible</li> <li>PATCH (0.0.1): Bug fixes, backwards compatible</li> </ul> <p>Pre-release identifiers: - <code>0.3.0-alpha1</code>: Alpha release - <code>0.3.0-beta1</code>: Beta release - <code>0.3.0-rc1</code>: Release candidate</p>"},{"location":"contributing/#post-release","title":"Post-Release","text":"<p>After publishing and building Docker images:</p> <ol> <li> <p>Verify on PyPI: Check package appears on PyPI <pre><code>pip install lablink-allocator-service==0.3.0\n</code></pre></p> </li> <li> <p>Verify Docker images: Check images on GHCR    <pre><code>docker pull ghcr.io/talmolab/lablink-allocator-image:0.3.0\ndocker pull ghcr.io/talmolab/lablink-client-base-image:0.1.5\n</code></pre></p> </li> <li> <p>Test installation: Install and verify the package works    <pre><code>pip install lablink-allocator-service==0.3.0\npython -c \"from lablink_allocator.main import main; print('OK')\"\n</code></pre></p> </li> <li> <p>Check documentation: Verify docs at https://talmolab.github.io/lablink/</p> </li> <li> <p>Announce: Post release announcement (if major version)</p> </li> </ol>"},{"location":"contributing/#troubleshooting-releases","title":"Troubleshooting Releases","text":"<p>Version mismatch error: <pre><code># Ensure pyproject.toml version matches tag\ngrep '^version = ' packages/allocator/pyproject.toml\n# Should output: version = \"0.3.0\"\n</code></pre></p> <p>Tests failing: <pre><code># Run tests locally first\ncd packages/allocator\nuv sync --extra dev\nsource .venv/bin/activate  # or .venv\\Scripts\\activate on Windows\nuv run pytest tests\n</code></pre></p> <p>Build failing: <pre><code># Test build locally\ncd packages/allocator\nuv build\nls -lh dist/\n</code></pre></p>"},{"location":"contributing/#rolling-back-a-release","title":"Rolling Back a Release","text":"<p>If a release has issues:</p> <ol> <li>Delete the GitHub Release (does not delete the tag)</li> <li>Delete the tag: <code>gh release delete lablink-allocator-service_v0.3.0 --yes</code></li> <li>Delete from PyPI: Contact PyPI support (cannot delete via API)</li> <li>Fix the issue and release as a new patch version (e.g., 0.3.1)</li> </ol> <p>Note: PyPI does not allow re-uploading the same version. Always increment the version number.</p>"},{"location":"contributing/#getting-help","title":"Getting Help","text":""},{"location":"contributing/#resources","title":"Resources","text":"<ul> <li>\ud83d\udcd6 Documentation: https://talmolab.github.io/lablink/</li> <li>\ud83d\udc1b Issues: https://github.com/talmolab/lablink/issues</li> <li>\ud83d\udcac Discussions: https://github.com/talmolab/lablink/discussions</li> <li>\ud83d\udce7 Developer Guide: CLAUDE.md</li> </ul>"},{"location":"contributing/#questions","title":"Questions?","text":"<ul> <li>Check FAQ</li> <li>Search existing issues</li> <li>Open a new discussion</li> <li>Open an issue if bug/feature</li> </ul>"},{"location":"contributing/#communication","title":"Communication","text":"<ul> <li>Be patient: Maintainers are volunteers</li> <li>Be clear: Provide context and details</li> <li>Be respectful: Follow code of conduct</li> <li>Be helpful: Help others when you can</li> </ul>"},{"location":"contributing/#recognition","title":"Recognition","text":"<p>Contributors are recognized in: - Git commit history - GitHub contributors page - Release notes (for significant contributions)</p>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the same BSD-3-Clause License that covers the project.</p>"},{"location":"contributing/#thank-you","title":"Thank You!","text":"<p>Your contributions make LabLink better for the research community. We appreciate your time and effort! \ud83c\udf89</p> <p>Questions about contributing? Open a discussion or reach out in an issue.</p>"},{"location":"cost-estimation/","title":"Cost Estimation","text":"<p>This guide helps you understand and estimate AWS costs for running LabLink.</p>"},{"location":"cost-estimation/#cost-overview","title":"Cost Overview","text":"<p>LabLink costs consist of:</p> <ol> <li>Infrastructure costs (one-time or monthly)</li> <li>Compute costs (per hour, based on usage)</li> <li>Storage costs (monthly)</li> <li>Data transfer costs (per GB)</li> </ol>"},{"location":"cost-estimation/#aws-pricing-calculator","title":"AWS Pricing Calculator","text":"<p>For exact pricing, use the AWS Pricing Calculator.</p> <p>Note</p> <p>Prices shown are for us-west-2 region as of January 2025. Check current AWS pricing for your region.</p>"},{"location":"cost-estimation/#infrastructure-costs-minimal","title":"Infrastructure Costs (Minimal)","text":""},{"location":"cost-estimation/#s3-bucket-terraform-state","title":"S3 Bucket (Terraform State)","text":"<p>Purpose: Store Terraform state files</p> Item Usage Monthly Cost Storage &lt; 1 GB $0.02 Requests ~ 100/month $0.01 Versioning Enabled Included <p>Estimated Monthly Cost: $0.05</p>"},{"location":"cost-estimation/#elastic-ips","title":"Elastic IPs","text":"<p>Purpose: Static IP addresses for allocators</p> Item Quantity Monthly Cost Elastic IP (associated) 2 (test, prod) $0.00 Elastic IP (unassociated) 0 $0.00 <p>Warning</p> <p>Unassociated Elastic IPs cost $0.005/hour ($3.60/month). Always associate or release unused IPs.</p> <p>Estimated Monthly Cost: $0.00 (when associated)</p>"},{"location":"cost-estimation/#route-53-optional","title":"Route 53 (Optional)","text":"<p>Purpose: DNS management for custom domains</p> Item Quantity Monthly Cost Hosted Zone 1 $0.50 Queries 1M $0.40 <p>Estimated Monthly Cost: $0.90</p>"},{"location":"cost-estimation/#total-infrastructure-cost","title":"Total Infrastructure Cost","text":"<p>Without Route 53: ~$0.05/month With Route 53: ~$0.95/month</p>"},{"location":"cost-estimation/#compute-costs-variable","title":"Compute Costs (Variable)","text":""},{"location":"cost-estimation/#allocator-instance","title":"Allocator Instance","text":"<p>Costs for running the allocator EC2 instance.</p>"},{"location":"cost-estimation/#instance-type-options","title":"Instance Type Options","text":"Instance Type vCPUs RAM Price (On-Demand) Monthly (24/7) t2.micro 1 1 GB $0.0116/hour $8.50 t2.small 1 2 GB $0.023/hour $16.79 t2.medium 2 4 GB $0.0464/hour $33.87 t3.micro 2 1 GB $0.0104/hour $7.59 t3.small 2 2 GB $0.0208/hour $15.18 <p>Recommended: t2.micro for dev/test, t2.small for production</p> <p>Estimated Monthly Cost: $8.50 - $17 (if running 24/7)</p>"},{"location":"cost-estimation/#cost-optimization","title":"Cost Optimization","text":"<p>Option 1: Terminate When Not Needed - Stop allocator during off-hours - Cost: Only when running - Example: 8 hours/day \u00d7 20 days = 160 hours = $1.86/month (t2.micro)</p> <p>Option 2: Reserved Instances (1-year commitment) - Up to 75% savings - t2.micro: $5.03/month (vs $8.50)</p> <p>Option 3: Savings Plans - Flexible commitment - Similar savings to Reserved Instances</p>"},{"location":"cost-estimation/#client-vm-instances","title":"Client VM Instances","text":"<p>Costs for running research workload VMs.</p>"},{"location":"cost-estimation/#gpu-instance-types","title":"GPU Instance Types","text":"Instance Type GPU vCPUs RAM GPU Memory Price/Hour Monthly (24/7) g4dn.xlarge T4 4 16 GB 16 GB $0.526 $384 g4dn.2xlarge T4 8 32 GB 16 GB $0.752 $549 g4dn.4xlarge T4 16 64 GB 16 GB $1.204 $879 g5.xlarge A10G 4 16 GB 24 GB $1.006 $735 g5.2xlarge A10G 8 32 GB 24 GB $1.212 $885 p3.2xlarge V100 8 61 GB 16 GB $3.06 $2,234 <p>Most Common: g4dn.xlarge (good balance of performance and cost)</p>"},{"location":"cost-estimation/#cost-optimization-strategies","title":"Cost Optimization Strategies","text":"<p>Option 1: Spot Instances (Up to 90% savings)</p> <pre><code># terraform/main.tf\nresource \"aws_instance\" \"client\" {\n  instance_market_options {\n    market_type = \"spot\"\n    spot_options {\n      max_price = \"0.20\"  # Max price you're willing to pay\n    }\n  }\n}\n</code></pre> <ul> <li>g4dn.xlarge Spot: ~$0.158/hour (vs $0.526 on-demand)</li> <li>Savings: 70%</li> <li>Risk: Can be terminated if capacity needed</li> </ul> <p>Option 2: Terminate After Use - Only run VMs when actively working - Cost: Per-hour usage only</p> <p>Example: 10 VMs \u00d7 8 hours = 80 hours \u00d7 $0.526 = $42.08</p> <p>Option 3: Right-Size Instance Types - Use smallest instance that meets requirements - Test on smaller instances first</p>"},{"location":"cost-estimation/#cpu-only-instance-types-non-gpu","title":"CPU-Only Instance Types (Non-GPU)","text":"<p>For non-GPU workloads:</p> Instance Type vCPUs RAM Price/Hour Monthly (24/7) c5.xlarge 4 8 GB $0.17 $124 c5.2xlarge 8 16 GB $0.34 $248 c6i.xlarge 4 8 GB $0.17 $124 <p>Use Case: Data processing without GPU requirements</p>"},{"location":"cost-estimation/#storage-costs","title":"Storage Costs","text":""},{"location":"cost-estimation/#ebs-volumes-ec2-storage","title":"EBS Volumes (EC2 Storage)","text":"Volume Type Allocator Client VM Price/GB-Month gp3 30 GB 100 GB $0.08 <p>Allocator: 30 GB \u00d7 $0.08 = $2.40/month Client VM: 100 GB \u00d7 $0.08 = $8.00/month per VM</p> <p>Note: EBS charges apply even for stopped instances. Terminate to avoid charges.</p>"},{"location":"cost-estimation/#s3-storage-backups","title":"S3 Storage (Backups)","text":"Item Usage Monthly Cost Standard Storage 10 GB $0.23 Glacier (Archive) 100 GB $0.40 <p>Use Case: Database backups, logs, artifacts</p>"},{"location":"cost-estimation/#data-transfer-costs","title":"Data Transfer Costs","text":""},{"location":"cost-estimation/#inbound-free","title":"Inbound (Free)","text":"<ul> <li>Data transfer into AWS is free</li> <li>Pulling Docker images: Free</li> <li>SSH/API calls into instances: Free</li> </ul>"},{"location":"cost-estimation/#outbound","title":"Outbound","text":"Destination Price per GB First 100 GB/month Free Next 10 TB/month $0.09 Internet (general) $0.09 <p>Typical Usage: &lt; 100 GB/month (covered by free tier)</p>"},{"location":"cost-estimation/#inter-region-transfer","title":"Inter-Region Transfer","text":"<p>If using resources across regions:</p> Transfer Type Price per GB Cross-region $0.02 <p>Avoid: Keep all resources in same region</p>"},{"location":"cost-estimation/#example-cost-scenarios","title":"Example Cost Scenarios","text":""},{"location":"cost-estimation/#scenario-1-developmenttesting","title":"Scenario 1: Development/Testing","text":"<p>Setup: - 1 allocator (t2.micro) - 2 client VMs (g4dn.xlarge) - Running 40 hours/month</p> <p>Costs: - Infrastructure: $0.05/month - Allocator: 40 hours \u00d7 $0.0116 = $0.46 - Client VMs: 2 \u00d7 40 hours \u00d7 $0.526 = $42.08 - Storage: $2.40 (allocator) + $16.00 (2 clients) = $18.40</p> <p>Total: ~$61/month</p>"},{"location":"cost-estimation/#scenario-2-light-production-use","title":"Scenario 2: Light Production Use","text":"<p>Setup: - 1 allocator (t2.small, 24/7) - 5 client VMs (g4dn.xlarge) - VMs running 160 hours/month each</p> <p>Costs: - Infrastructure: $0.95/month (with Route 53) - Allocator: $16.79/month - Client VMs: 5 \u00d7 160 hours \u00d7 $0.526 = $420.80 - Storage: $2.40 + (5 \u00d7 $8.00) = $42.40</p> <p>Total: ~$481/month</p>"},{"location":"cost-estimation/#scenario-3-heavy-production-use","title":"Scenario 3: Heavy Production Use","text":"<p>Setup: - 1 allocator (t2.small, 24/7, Reserved Instance) - 20 client VMs (g4dn.xlarge, Spot Instances) - VMs running 320 hours/month each</p> <p>Costs: - Infrastructure: $0.95/month - Allocator: $5.03/month (Reserved Instance) - Client VMs: 20 \u00d7 320 hours \u00d7 $0.158 (Spot) = $1,011.20 - Storage: $2.40 + (20 \u00d7 $8.00) = $162.40</p> <p>Total: ~$1,182/month</p> <p>Savings vs On-Demand: ~$3,200/month (70% reduction)</p>"},{"location":"cost-estimation/#scenario-4-minimal-cost-conscious","title":"Scenario 4: Minimal (Cost-Conscious)","text":"<p>Setup: - 1 allocator (t2.micro) - 3 client VMs (g4dn.xlarge, Spot) - Only running when actively working (40 hours/month)</p> <p>Costs: - Infrastructure: $0.05/month - Allocator: 40 hours \u00d7 $0.0116 = $0.46 - Client VMs: 3 \u00d7 40 hours \u00d7 $0.158 (Spot) = $18.96 - Storage: Minimal (terminate when done) = $0.50</p> <p>Total: ~$20/month</p>"},{"location":"cost-estimation/#cost-monitoring","title":"Cost Monitoring","text":""},{"location":"cost-estimation/#set-up-billing-alerts","title":"Set Up Billing Alerts","text":"<p>Step 1: Create SNS Topic <pre><code>aws sns create-topic --name lablink-billing-alerts\n</code></pre></p> <p>Step 2: Subscribe Email <pre><code>aws sns subscribe \\\n  --topic-arn arn:aws:sns:us-west-2:ACCOUNT_ID:lablink-billing-alerts \\\n  --protocol email \\\n  --notification-endpoint your-email@example.com\n</code></pre></p> <p>Step 3: Create Budget <pre><code>aws budgets create-budget --account-id ACCOUNT_ID --budget file://budget.json\n</code></pre></p> <p><code>budget.json</code>: <pre><code>{\n  \"BudgetName\": \"LabLink Monthly Budget\",\n  \"BudgetLimit\": {\n    \"Amount\": \"100\",\n    \"Unit\": \"USD\"\n  },\n  \"TimeUnit\": \"MONTHLY\",\n  \"BudgetType\": \"COST\"\n}\n</code></pre></p>"},{"location":"cost-estimation/#view-current-costs","title":"View Current Costs","text":"<p>AWS Console: 1. Navigate to Billing Dashboard 2. View Cost Explorer 3. Filter by tag <code>Project: LabLink</code></p> <p>AWS CLI: <pre><code>aws ce get-cost-and-usage \\\n  --time-period Start=2025-01-01,End=2025-01-31 \\\n  --granularity MONTHLY \\\n  --metrics UnblendedCost \\\n  --filter file://filter.json\n</code></pre></p> <p><code>filter.json</code>: <pre><code>{\n  \"Tags\": {\n    \"Key\": \"Project\",\n    \"Values\": [\"LabLink\"]\n  }\n}\n</code></pre></p>"},{"location":"cost-estimation/#tag-resources","title":"Tag Resources","text":"<p>Tag all resources for cost tracking:</p> <pre><code># terraform/main.tf\nresource \"aws_instance\" \"lablink_allocator\" {\n  # ... other config\n\n  tags = {\n    Name    = \"lablink-allocator-${var.environment}\"\n    Project = \"LabLink\"\n    Environment = var.environment\n    ManagedBy = \"Terraform\"\n  }\n}\n</code></pre> <p>View costs by tag in Cost Explorer.</p>"},{"location":"cost-estimation/#cost-optimization-checklist","title":"Cost Optimization Checklist","text":"<ul> <li> Use Spot Instances for client VMs (70-90% savings)</li> <li> Terminate VMs when not in use</li> <li> Use Reserved Instances for always-on allocators (75% savings)</li> <li> Right-size instance types (don't over-provision)</li> <li> Use gp3 volumes instead of gp2 (20% cheaper)</li> <li> Set up billing alerts</li> <li> Monitor costs weekly in Cost Explorer</li> <li> Tag all resources for cost attribution</li> <li> Use Lifecycle Policies to delete old S3 backups</li> <li> Terminate (not stop) unused instances</li> <li> Release unused Elastic IPs</li> <li> Clean up old EBS snapshots</li> </ul>"},{"location":"cost-estimation/#free-tier","title":"Free Tier","text":"<p>New AWS accounts get 12 months of free tier:</p> Service Free Tier (Monthly) EC2 (t2.micro) 750 hours EBS (gp2/gp3) 30 GB S3 5 GB storage Data Transfer 100 GB out <p>Note: GPU instances (g4dn, g5, p3) are not included in free tier.</p>"},{"location":"cost-estimation/#hidden-costs-to-watch","title":"Hidden Costs to Watch","text":"<ol> <li>Unassociated Elastic IPs: $3.60/month each</li> <li>Stopped instances with EBS: Storage charges still apply</li> <li>Old EBS snapshots: Accumulate over time</li> <li>Unused load balancers: $16-18/month</li> <li>NAT Gateways: $32/month + data transfer</li> <li>Idle RDS instances: $15-200/month</li> </ol> <p>Solution: Regular cleanup and monitoring</p>"},{"location":"cost-estimation/#cost-comparison","title":"Cost Comparison","text":""},{"location":"cost-estimation/#lablink-vs-self-managed","title":"LabLink vs Self-Managed","text":"Aspect LabLink Self-Managed Infrastructure setup $0-1/month $0 Allocator runtime $8-17/month $0 (your time) Client VMs Same Same Management time Minimal Significant <p>LabLink advantage: Time savings outweigh small infrastructure costs</p>"},{"location":"cost-estimation/#lablink-vs-managed-services","title":"LabLink vs Managed Services","text":"Service Monthly Cost (5 VMs) Setup Complexity LabLink ~$481 Moderate AWS Batch ~$500+ High SageMaker ~$600+ Moderate Cloud GPUs (vast.ai) ~$200-400 Low <p>LabLink advantage: Balance of cost, features, and control</p>"},{"location":"cost-estimation/#budget-recommendations","title":"Budget Recommendations","text":""},{"location":"cost-estimation/#by-use-case","title":"By Use Case","text":"Use Case Recommended Monthly Budget Individual researcher (occasional) $50-100 Individual researcher (regular) $200-500 Small research group $500-1,500 Large research group $1,500-5,000+"},{"location":"cost-estimation/#next-steps","title":"Next Steps","text":"<ul> <li>AWS Setup: Set up billing alerts</li> <li>Configuration: Choose cost-effective instance types</li> <li>Deployment: Deploy with cost optimization</li> </ul>"},{"location":"cost-estimation/#questions-about-costs","title":"Questions About Costs?","text":"<ul> <li>Check AWS Pricing</li> <li>Use AWS Pricing Calculator</li> <li>Contact AWS Support for enterprise pricing</li> </ul>"},{"location":"database/","title":"Database Management","text":"<p>This guide covers the PostgreSQL database used by LabLink, including schema, management tasks, and troubleshooting.</p>"},{"location":"database/#database-overview","title":"Database Overview","text":"<p>LabLink uses PostgreSQL for:</p> <ul> <li>Tracking VM states (available, in-use, failed)</li> <li>Storing user assignments</li> <li>Real-time notifications (LISTEN/NOTIFY)</li> <li>Audit logging</li> </ul> <p>Version: PostgreSQL 13+ Location: Runs in allocator Docker container Access: Port 5432 (internal)</p>"},{"location":"database/#database-schema","title":"Database Schema","text":""},{"location":"database/#schema-overview","title":"Schema Overview","text":"erDiagram     VMS {         varchar_1024 HostName PK \"VM hostname/instance ID\"         varchar_1024 Pin \"VM pin/identifier\"         varchar_1024 CrdCommand \"Command to execute\"         varchar_1024 UserEmail \"User email address\"         boolean InUse \"Whether software is running\"         varchar_1024 Healthy \"Health status\"         varchar_1024 Status \"VM status\"         text Logs \"VM logs\"         timestamp TerraformApplyStartTime \"Terraform start\"         timestamp TerraformApplyEndTime \"Terraform end\"         float TerraformApplyDurationSeconds \"Terraform duration\"         timestamp CloudInitStartTime \"Cloud-init start\"         timestamp CloudInitEndTime \"Cloud-init end\"         float CloudInitDurationSeconds \"Cloud-init duration\"         timestamp ContainerStartTime \"Container start\"         timestamp ContainerEndTime \"Container end\"         float ContainerStartupDurationSeconds \"Container startup duration\"         float TotalStartupDurationSeconds \"Total startup duration\"         timestamp CreatedAt \"Creation timestamp\"     }      VMS ||--o{ trigger_crd_command_insert_or_update : \"fires on notify_crd_command_update()\""},{"location":"database/#tables","title":"Tables","text":""},{"location":"database/#vms-table","title":"<code>vms</code> Table","text":"<p>Primary table tracking all VM instances with comprehensive timing metrics.</p> Column Type Constraints Description <code>HostName</code> VARCHAR(1024) PRIMARY KEY VM hostname/instance ID <code>Pin</code> VARCHAR(1024) VM pin/identifier for access <code>CrdCommand</code> VARCHAR(1024) Command to execute on VM <code>UserEmail</code> VARCHAR(1024) User email address <code>InUse</code> BOOLEAN NOT NULL Whether configured software is running (default: FALSE) <code>Healthy</code> VARCHAR(1024) Health status of the VM <code>Status</code> VARCHAR(1024) VM status <code>Logs</code> TEXT VM logs <code>TerraformApplyStartTime</code> TIMESTAMP When Terraform apply started <code>TerraformApplyEndTime</code> TIMESTAMP When Terraform apply completed <code>TerraformApplyDurationSeconds</code> FLOAT Duration of Terraform apply in seconds <code>CloudInitStartTime</code> TIMESTAMP When cloud-init started <code>CloudInitEndTime</code> TIMESTAMP When cloud-init completed <code>CloudInitDurationSeconds</code> FLOAT Duration of cloud-init in seconds <code>ContainerStartTime</code> TIMESTAMP When container startup started <code>ContainerEndTime</code> TIMESTAMP When container became ready <code>ContainerStartupDurationSeconds</code> FLOAT Duration of container startup in seconds <code>TotalStartupDurationSeconds</code> FLOAT Total VM startup duration in seconds <code>CreatedAt</code> TIMESTAMP DEFAULT NOW() Creation timestamp <p>InUse Status:</p> <p>The <code>InUse</code> column indicates whether the configured software (e.g., SLEAP) is actively running on the VM, not just whether a user has been assigned. This is monitored by the <code>update_inuse_status</code> service running on the client VM.</p> <ul> <li><code>FALSE</code>: Software process not running</li> <li><code>TRUE</code>: Software process actively running</li> </ul> <p>Timing Metrics:</p> <p>The table tracks three phases of VM startup:</p> <ol> <li>Terraform Apply: Infrastructure provisioning (EC2 instance creation)</li> <li>Cloud-init: OS-level initialization and Docker setup</li> <li>Container Startup: Application container becoming ready</li> </ol> <p>These metrics help identify bottlenecks in the VM creation process.</p> <p>Example Row:</p> <pre><code>HostName          | UserEmail        | InUse | CrdCommand       | TotalStartupDurationSeconds | CreatedAt\n------------------+------------------+-------+------------------+-----------------------------+---------------------\ni-0abc123def456   | user@example.com | true  | python train.py  | 245.67                      | 2025-01-15 10:30:00\n</code></pre>"},{"location":"database/#triggers","title":"Triggers","text":""},{"location":"database/#notify_vm_update","title":"<code>notify_vm_update</code>","text":"<p>Sends PostgreSQL NOTIFY when VM table changes.</p> <p>Purpose: Real-time updates to client VMs</p> <p>Definition:</p> <pre><code>CREATE OR REPLACE FUNCTION notify_vm_changes()\nRETURNS trigger AS $$\nBEGIN\n  PERFORM pg_notify('vm_updates', row_to_json(NEW)::text);\n  RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER vm_update_trigger\nAFTER INSERT OR UPDATE ON vms\nFOR EACH ROW\nEXECUTE FUNCTION notify_vm_changes();\n</code></pre> <p>How it works:</p> <ol> <li> <p>Client VM starts up and sends HTTP POST to <code>/vm_startup</code> endpoint</p> </li> <li> <p>Allocator validates VM exists in database and establishes <code>LISTEN vm_updates</code> connection</p> </li> <li> <p>HTTP request blocks while allocator waits for VM assignment</p> </li> <li> <p>User requests a VM via web UI (POST <code>/api/request_vm</code>)</p> </li> <li> <p>Allocator assigns first available VM by updating database with <code>CrdCommand</code> and <code>Pin</code></p> </li> <li> <p>Database trigger fires on UPDATE and sends <code>pg_notify()</code> with JSON payload to <code>vm_updates</code> channel</p> </li> <li> <p>Allocator receives notification via PostgreSQL LISTEN connection</p> </li> <li> <p>Allocator parses notification and matches hostname to the pending <code>/vm_startup</code> request</p> </li> <li> <p>HTTP response returns to client with command and pin</p> </li> <li> <p>Client executes the CrdCommand to connect to the configured software</p> </li> </ol>"},{"location":"database/#notifylisten-flow-diagram","title":"NOTIFY/LISTEN Flow Diagram","text":"sequenceDiagram     participant User     participant Client as Client VM     participant Flask as Flask App&lt;br/&gt;(Allocator)     participant DB as PostgreSQL     participant Trigger as notify_crd_command_update&lt;br/&gt;Trigger      Note over Client: Client VM starts up      Client-&gt;&gt;Flask: POST /vm_startup&lt;br/&gt;{hostname: \"vm-123\"}&lt;br/&gt;      Note over Flask: Allocator validates VM&lt;br/&gt;and establishes LISTEN      Flask-&gt;&gt;DB: SELECT * WHERE hostname='vm-123'     DB--&gt;&gt;Flask: VM found      Flask-&gt;&gt;DB: LISTEN vm_updates      Note over Flask,DB: Allocator waits for notification&lt;br/&gt;(HTTP response blocked)      Note over User: User requests a VM      User-&gt;&gt;Flask: POST /api/request_vm&lt;br/&gt;{email: \"user@example.com\",&lt;br/&gt;crd_command: \"...\"}      Note over Flask: Allocator assigns first&lt;br/&gt;available VM      Flask-&gt;&gt;DB: UPDATE vms&lt;br/&gt;SET UserEmail='user@example.com',&lt;br/&gt;CrdCommand='...', Pin='...'&lt;br/&gt;WHERE hostname='vm-123'      DB-&gt;&gt;Trigger: Row modified&lt;br/&gt;(AFTER INSERT/UPDATE trigger)      Trigger-&gt;&gt;Trigger: Build JSON payload&lt;br/&gt;{HostName, CrdCommand, Pin}      Trigger-&gt;&gt;DB: pg_notify('vm_updates',&lt;br/&gt;JSON payload)      DB--&gt;&gt;Flask: NOTIFY event received&lt;br/&gt;(on LISTEN connection)      Flask-&gt;&gt;Flask: Parse notification&lt;br/&gt;Check if HostName matches&lt;br/&gt;pending vm_startup request      alt Hostname matches         Flask--&gt;&gt;Client: HTTP 200 OK&lt;br/&gt;{status: \"success\",&lt;br/&gt;command: \"...\", pin: \"...\"}         Flask--&gt;&gt;User: Success page&lt;br/&gt;with VM hostname         Client-&gt;&gt;Client: Execute CrdCommand&lt;br/&gt;(connect to software)     else Hostname doesn't match         Note over Flask: Continue waiting for&lt;br/&gt;matching notification     end"},{"location":"database/#accessing-the-database","title":"Accessing the Database","text":""},{"location":"database/#via-ssh-and-psql","title":"Via SSH and psql","text":"<pre><code># SSH into allocator\nssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\n\n# Get container ID\nCONTAINER_ID=$(sudo docker ps --filter \"ancestor=ghcr.io/talmolab/lablink-allocator-image\" --format \"{{.ID}}\")\n\n# Access PostgreSQL\nsudo docker exec -it $CONTAINER_ID psql -U lablink -d lablink_db\n</code></pre>"},{"location":"database/#connection-parameters","title":"Connection Parameters","text":"<p>From config (<code>lablink-infrastructure/config/config.yaml</code>):</p> <pre><code>db:\n  dbname: \"lablink_db\"\n  user: \"lablink\"\n  password: \"lablink\" # Change in production!\n  host: \"localhost\"\n  port: 5432\n</code></pre>"},{"location":"database/#from-python-inside-container","title":"From Python (Inside Container)","text":"<pre><code>import psycopg2\n\nconn = psycopg2.connect(\n    dbname=\"lablink_db\",\n    user=\"lablink\",\n    password=\"lablink\",\n    host=\"localhost\",\n    port=5432\n)\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT * FROM vms;\")\nrows = cursor.fetchall()\n\nfor row in rows:\n    print(row)\n\nconn.close()\n</code></pre>"},{"location":"database/#common-database-operations","title":"Common Database Operations","text":""},{"location":"database/#view-all-vms","title":"View All VMs","text":"<pre><code>SELECT * FROM vms;\n</code></pre>"},{"location":"database/#view-available-vms","title":"View Available VMs","text":"<pre><code>SELECT hostname, status, created_at\nFROM vms\nWHERE status = 'available'\nORDER BY created_at;\n</code></pre>"},{"location":"database/#view-in-use-vms","title":"View In-Use VMs","text":"<pre><code>SELECT hostname, email, status, crd_command, updated_at\nFROM vms\nWHERE status = 'in-use'\nORDER BY updated_at DESC;\n</code></pre>"},{"location":"database/#count-vms-by-status","title":"Count VMs by Status","text":"<pre><code>SELECT status, COUNT(*) as count\nFROM vms\nGROUP BY status;\n</code></pre> <p>Expected output:</p> <pre><code> status    | count\n-----------+-------\n available |     5\n in-use    |     3\n failed    |     1\n</code></pre>"},{"location":"database/#find-vm-by-email","title":"Find VM by Email","text":"<pre><code>SELECT hostname, status, crd_command\nFROM vms\nWHERE email = 'user@example.com';\n</code></pre>"},{"location":"database/#update-vm-status","title":"Update VM Status","text":"<pre><code>-- Mark VM as available\nUPDATE vms\nSET status = 'available', email = NULL, crd_command = NULL, updated_at = NOW()\nWHERE hostname = 'i-0abc123def456';\n\n-- Mark VM as failed\nUPDATE vms\nSET status = 'failed', updated_at = NOW()\nWHERE hostname = 'i-0abc123def456';\n</code></pre>"},{"location":"database/#delete-vm-record","title":"Delete VM Record","text":"<pre><code>DELETE FROM vms WHERE hostname = 'i-0abc123def456';\n</code></pre> <p>Warning</p> <p>Only delete after VM instance is terminated in AWS.</p>"},{"location":"database/#clear-all-vms","title":"Clear All VMs","text":"<pre><code>-- Use with caution!\nTRUNCATE TABLE vms;\n</code></pre>"},{"location":"database/#monitoring-listennotify","title":"Monitoring LISTEN/NOTIFY","text":""},{"location":"database/#listen-for-vm-updates","title":"Listen for VM Updates","text":"<pre><code>-- In psql session\nLISTEN vm_updates;\n\n-- In another session, make a change:\nUPDATE vms SET status = 'in-use' WHERE id = 1;\n\n-- First session receives:\nAsynchronous notification \"vm_updates\" received from server process with PID 12345.\n</code></pre>"},{"location":"database/#listen-from-python","title":"Listen from Python","text":"<pre><code>import psycopg2\nimport select\n\nconn = psycopg2.connect(\n    dbname=\"lablink_db\",\n    user=\"lablink\",\n    password=\"lablink\",\n    host=\"localhost\"\n)\n\nconn.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)\ncursor = conn.cursor()\ncursor.execute(\"LISTEN vm_updates;\")\n\nprint(\"Waiting for notifications...\")\n\nwhile True:\n    if select.select([conn], [], [], 5) == ([], [], []):\n        print(\"Timeout\")\n    else:\n        conn.poll()\n        while conn.notifies:\n            notify = conn.notifies.pop(0)\n            print(f\"Notification: {notify.payload}\")\n</code></pre>"},{"location":"database/#database-backup","title":"Database Backup","text":""},{"location":"database/#manual-backup","title":"Manual Backup","text":"<pre><code># SSH into allocator\nssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\n\n# Backup database\nsudo docker exec &lt;container-id&gt; pg_dump -U lablink lablink_db &gt; lablink_backup.sql\n\n# Download backup\nscp -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;:~/lablink_backup.sql ./\n</code></pre>"},{"location":"database/#automated-backup-script","title":"Automated Backup Script","text":"<p><code>backup.sh</code>:</p> <pre><code>#!/bin/bash\n\nCONTAINER_ID=$(sudo docker ps --filter \"ancestor=ghcr.io/talmolab/lablink-allocator-image\" --format \"{{.ID}}\")\nBACKUP_DIR=\"/home/ubuntu/backups\"\nDATE=$(date +%Y%m%d_%H%M%S)\n\nmkdir -p $BACKUP_DIR\n\nsudo docker exec $CONTAINER_ID pg_dump -U lablink lablink_db &gt; $BACKUP_DIR/lablink_$DATE.sql\n\n# Upload to S3\naws s3 cp $BACKUP_DIR/lablink_$DATE.sql s3://lablink-backups/\n\n# Keep only last 7 days locally\nfind $BACKUP_DIR -name \"lablink_*.sql\" -mtime +7 -delete\n\necho \"Backup complete: lablink_$DATE.sql\"\n</code></pre> <p>Setup cron job:</p> <pre><code># Edit crontab\ncrontab -e\n\n# Add daily backup at 2 AM\n0 2 * * * /home/ubuntu/backup.sh &gt;&gt; /var/log/lablink-backup.log 2&gt;&amp;1\n</code></pre>"},{"location":"database/#restore-from-backup","title":"Restore from Backup","text":"<pre><code># SSH into allocator\nssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\n\n# Copy backup to instance\nscp -i ~/lablink-key.pem lablink_backup.sql ubuntu@&lt;allocator-ip&gt;:~/\n\n# Restore database\nsudo docker exec -i &lt;container-id&gt; psql -U lablink lablink_db &lt; lablink_backup.sql\n</code></pre>"},{"location":"database/#database-maintenance","title":"Database Maintenance","text":""},{"location":"database/#vacuum-database","title":"Vacuum Database","text":"<p>Remove dead tuples and reclaim space:</p> <pre><code>-- Analyze and vacuum\nVACUUM ANALYZE vms;\n\n-- Full vacuum (more aggressive, requires exclusive lock)\nVACUUM FULL vms;\n</code></pre>"},{"location":"database/#reindex","title":"Reindex","text":"<p>Rebuild indexes for performance:</p> <pre><code>REINDEX TABLE vms;\n</code></pre>"},{"location":"database/#check-database-size","title":"Check Database Size","text":"<pre><code>SELECT pg_size_pretty(pg_database_size('lablink_db'));\n</code></pre>"},{"location":"database/#check-table-size","title":"Check Table Size","text":"<pre><code>SELECT\n  schemaname,\n  tablename,\n  pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size\nFROM pg_tables\nWHERE schemaname = 'public'\nORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;\n</code></pre>"},{"location":"database/#view-active-connections","title":"View Active Connections","text":"<pre><code>SELECT\n  pid,\n  usename,\n  application_name,\n  client_addr,\n  state,\n  query\nFROM pg_stat_activity\nWHERE datname = 'lablink_db';\n</code></pre>"},{"location":"database/#kill-idle-connections","title":"Kill Idle Connections","text":"<pre><code>SELECT pg_terminate_backend(pid)\nFROM pg_stat_activity\nWHERE datname = 'lablink_db'\n  AND state = 'idle'\n  AND state_change &lt; NOW() - INTERVAL '10 minutes';\n</code></pre>"},{"location":"database/#migrating-to-rds-production","title":"Migrating to RDS (Production)","text":"<p>For production, consider Amazon RDS for managed PostgreSQL.</p>"},{"location":"database/#benefits","title":"Benefits","text":"<ul> <li>Automated backups</li> <li>Multi-AZ high availability</li> <li>Automatic failover</li> <li>Automated patching</li> <li>Monitoring and metrics</li> <li>Point-in-time recovery</li> </ul>"},{"location":"database/#setup-rds-instance","title":"Setup RDS Instance","text":"<pre><code># terraform/rds.tf\n\nresource \"aws_db_instance\" \"lablink\" {\n  identifier        = \"lablink-db-${var.environment}\"\n  engine            = \"postgres\"\n  engine_version    = \"13.7\"\n  instance_class    = \"db.t3.micro\"\n  allocated_storage = 20\n\n  db_name  = \"lablink_db\"\n  username = \"lablink\"\n  password = var.db_password  # From Secrets Manager\n\n  vpc_security_group_ids = [aws_security_group.rds.id]\n  db_subnet_group_name   = aws_db_subnet_group.lablink.name\n\n  backup_retention_period = 7\n  backup_window          = \"03:00-04:00\"\n  maintenance_window     = \"mon:04:00-mon:05:00\"\n\n  storage_encrypted      = true\n  skip_final_snapshot   = false\n  final_snapshot_identifier = \"lablink-final-${var.environment}\"\n\n  tags = {\n    Name = \"lablink-db-${var.environment}\"\n  }\n}\n\noutput \"rds_endpoint\" {\n  value = aws_db_instance.lablink.endpoint\n}\n</code></pre>"},{"location":"database/#update-application-configuration","title":"Update Application Configuration","text":"<pre><code>db:\n  dbname: \"lablink_db\"\n  user: \"lablink\"\n  password: \"${DB_PASSWORD}\" # From Secrets Manager\n  host: \"lablink-db-prod.xxxxx.us-west-2.rds.amazonaws.com\"\n  port: 5432\n</code></pre>"},{"location":"database/#migrate-data","title":"Migrate Data","text":"<pre><code># Dump from container database\nsudo docker exec &lt;container-id&gt; pg_dump -U lablink lablink_db &gt; dump.sql\n\n# Restore to RDS\npsql -h lablink-db-prod.xxxxx.us-west-2.rds.amazonaws.com -U lablink -d lablink_db &lt; dump.sql\n</code></pre>"},{"location":"database/#troubleshooting","title":"Troubleshooting","text":""},{"location":"database/#postgresql-wont-start","title":"PostgreSQL Won't Start","text":"<p>Check logs:</p> <pre><code>sudo docker exec &lt;container-id&gt; tail -f /var/log/postgresql/postgresql-13-main.log\n</code></pre> <p>Common issues:</p> <ol> <li>Port already in use:</li> </ol> <pre><code>sudo netstat -tulpn | grep 5432\n# Kill process using port\n</code></pre> <ol> <li>Disk full:</li> </ol> <pre><code>df -h\n# Clean up space\n</code></pre> <ol> <li>Corrupt data files:    <pre><code># Stop container, remove volume, restart\nsudo docker stop &lt;container-id&gt;\nsudo docker rm &lt;container-id&gt;\n# Redeploy with fresh database\n</code></pre></li> </ol>"},{"location":"database/#cannot-connect-to-database","title":"Cannot Connect to Database","text":"<p>Check connection from allocator:</p> <pre><code>sudo docker exec &lt;container-id&gt; pg_isready -U lablink\n</code></pre> <p>Test connection:</p> <pre><code>sudo docker exec &lt;container-id&gt; psql -U lablink -d lablink_db -c \"SELECT 1;\"\n</code></pre> <p>Check pg_hba.conf:</p> <pre><code>sudo docker exec &lt;container-id&gt; cat /etc/postgresql/13/main/pg_hba.conf\n</code></pre> <p>Should include:</p> <pre><code>host    all             all             0.0.0.0/0            md5\n</code></pre>"},{"location":"database/#database-performance-issues","title":"Database Performance Issues","text":"<p>Check slow queries:</p> <pre><code>SELECT\n  pid,\n  now() - pg_stat_activity.query_start AS duration,\n  query\nFROM pg_stat_activity\nWHERE state = 'active'\n  AND now() - pg_stat_activity.query_start &gt; interval '5 seconds'\nORDER BY duration DESC;\n</code></pre> <p>Enable query logging:</p> <pre><code># In postgresql.conf\nlog_min_duration_statement = 1000  # Log queries &gt; 1 second\n</code></pre> <p>Add indexes:</p> <pre><code>-- Index on email for faster lookups\nCREATE INDEX idx_vms_email ON vms(email);\n\n-- Index on status\nCREATE INDEX idx_vms_status ON vms(status);\n\n-- Composite index\nCREATE INDEX idx_vms_status_email ON vms(status, email);\n</code></pre>"},{"location":"database/#restart-postgresql","title":"Restart PostgreSQL","text":"<p>Known issue requiring manual restart after first boot:</p> <pre><code># SSH into allocator\nssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\n\n# Access container\nsudo docker exec -it &lt;container-id&gt; bash\n\n# Inside container\n/etc/init.d/postgresql restart\n\n# Verify\npg_isready -U lablink\n</code></pre>"},{"location":"database/#security-best-practices","title":"Security Best Practices","text":"<ol> <li>Change default password: See Security</li> <li>Use SSL connections: Configure <code>sslmode=require</code></li> <li>Restrict pg_hba.conf: Limit to specific IPs/VPCs</li> <li>Regular backups: Automate daily backups</li> <li>Monitor access logs: Review connection attempts</li> <li>Use RDS for production: Better security and management</li> </ol>"},{"location":"database/#performance-tuning","title":"Performance Tuning","text":""},{"location":"database/#configuration-recommendations","title":"Configuration Recommendations","text":"<p>For allocator with 2GB RAM:</p> <pre><code># postgresql.conf\nshared_buffers = 512MB\neffective_cache_size = 1GB\nmaintenance_work_mem = 128MB\ncheckpoint_completion_target = 0.9\nwal_buffers = 16MB\ndefault_statistics_target = 100\nrandom_page_cost = 1.1\neffective_io_concurrency = 200\nwork_mem = 5MB\nmin_wal_size = 1GB\nmax_wal_size = 4GB\n</code></pre>"},{"location":"database/#connection-pooling","title":"Connection Pooling","text":"<p>For high-concurrency, use pgBouncer:</p> <pre><code># docker-compose.yml\nservices:\n  pgbouncer:\n    image: pgbouncer/pgbouncer\n    environment:\n      DATABASES_HOST: localhost\n      DATABASES_PORT: 5432\n      DATABASES_DBNAME: lablink_db\n    ports:\n      - \"6432:6432\"\n</code></pre>"},{"location":"database/#next-steps","title":"Next Steps","text":"<ul> <li>SSH Access: Connect to database via SSH</li> <li>Troubleshooting: Fix database issues</li> <li>Security: Secure database access</li> <li>Architecture: Understand database role</li> </ul>"},{"location":"database/#quick-reference","title":"Quick Reference","text":"<pre><code>-- View all VMs\nSELECT * FROM vms;\n\n-- Count by status\nSELECT status, COUNT(*) FROM vms GROUP BY status;\n\n-- Find available VMs\nSELECT * FROM vms WHERE status = 'available';\n\n-- Update VM status\nUPDATE vms SET status = 'available' WHERE hostname = 'i-xxxxx';\n\n-- Backup\npg_dump -U lablink lablink_db &gt; backup.sql\n\n-- Restore\npsql -U lablink lablink_db &lt; backup.sql\n\n-- Vacuum\nVACUUM ANALYZE vms;\n</code></pre>"},{"location":"deployment/","title":"Deployment","text":"<p>This guide covers deploying LabLink to AWS using both automated (GitHub Actions) and manual (Terraform CLI) methods.</p>"},{"location":"deployment/#deployment-overview","title":"Deployment Overview","text":"<p>LabLink supports four deployment environments:</p> Environment Purpose Trigger Image Tag dev Local/personal development Manual <code>*-test</code> test Staging, pre-production testing Push to <code>test</code> branch <code>*-test</code> ci-test CI testing with S3 backend Manual workflow dispatch <code>*-test</code> prod Production workloads Manual workflow dispatch Pinned version tags"},{"location":"deployment/#choosing-a-deployment-method","title":"Choosing a Deployment Method","text":"flowchart TD     Start{What are you deploying?}      Start --&gt;|Quick testing&lt;br/&gt;or development| DevEnv[Local Dev Environment]     Start --&gt;|Team staging| TestEnv[Test Environment]     Start --&gt;|Production| ProdEnv[Production Environment]      DevEnv --&gt; DevMethod{Preferred method?}     TestEnv --&gt; AutoDeploy[GitHub Actions&lt;br/&gt;Recommended]     ProdEnv --&gt; ProdMethod{Infrastructure exists?}      DevMethod --&gt;|Quick &amp; easy| ManualDev[Manual Terraform&lt;br/&gt;Local state]     DevMethod --&gt;|CI/CD practice| GHActionsDev[GitHub Actions&lt;br/&gt;workflow_dispatch]      ProdMethod --&gt;|First time| ManualProd[Manual Terraform&lt;br/&gt;Careful setup]     ProdMethod --&gt;|Updates| GHActionsProd[GitHub Actions&lt;br/&gt;workflow_dispatch]      ManualDev --&gt; DevNotes[\"\u2713 No S3 bucket needed&lt;br/&gt;\u2713 Fast iteration&lt;br/&gt;\u2717 State not shared\"]     GHActionsDev --&gt; DevGHNotes[\"\u2713 Practice CI/CD&lt;br/&gt;\u2713 Shared state&lt;br/&gt;\u26a0 Requires S3 setup\"]      AutoDeploy --&gt; TestNotes[\"\u2713 Automatic on push&lt;br/&gt;\u2713 Team accessible&lt;br/&gt;\u2713 S3 state storage\"]      ManualProd --&gt; ProdManual[\"\u2713 Full control&lt;br/&gt;\u2713 Step-by-step&lt;br/&gt;\u26a0 Manual process\"]     GHActionsProd --&gt; ProdGH[\"\u2713 Consistent deploys&lt;br/&gt;\u2713 Audit trail&lt;br/&gt;\u2713 Rollback support\"]      style DevEnv fill:#e3f2fd     style TestEnv fill:#fff3e0     style ProdEnv fill:#ffebee     style AutoDeploy fill:#c8e6c9     style ManualProd fill:#fff9c4"},{"location":"deployment/#prerequisites","title":"Prerequisites","text":"<p>Before deploying, ensure you have:</p> <ul> <li> AWS account configured (see Prerequisites)</li> <li> Terraform installed (see Prerequisites)</li> <li> S3 bucket for Terraform state (see AWS Setup)</li> <li> Elastic IP allocated for test/prod (see AWS Setup)</li> <li> IAM roles configured for GitHub Actions (see AWS Setup)</li> </ul>"},{"location":"deployment/#method-1-github-actions-recommended","title":"Method 1: GitHub Actions (Recommended)","text":"<p>Automated deployment via CI/CD pipelines.</p>"},{"location":"deployment/#initial-setup","title":"Initial Setup","text":"<ol> <li>Configure GitHub Secrets</li> </ol> <p>Navigate to Settings \u2192 Secrets and variables \u2192 Actions in your GitHub repository.</p> <p>Required secrets:    - <code>AWS_ROLE_ARN</code>: IAM role ARN for GitHub Actions authentication      Example: <code>arn:aws:iam::711387140753:role/GitHubActionsLabLinkRole</code>    - <code>AWS_REGION</code>: AWS region for deployment      Example: <code>us-west-2</code>, <code>eu-west-1</code>, <code>ap-northeast-1</code> Note: Must match region in <code>config/config.yaml</code>    - <code>ADMIN_PASSWORD</code>: Admin password for allocator web interface      Example: Generate a secure password using a password manager    - <code>DB_PASSWORD</code>: Database password for PostgreSQL      Example: Generate a secure password using a password manager</p> <p>Security Note: The workflow automatically replaces <code>PLACEHOLDER_ADMIN_PASSWORD</code> and <code>PLACEHOLDER_DB_PASSWORD</code> in config files with these secret values before Terraform runs, preventing passwords from appearing in logs. If these secrets are not set, the workflow uses temporary <code>CHANGEME_*</code> defaults and displays a warning.</p> <ol> <li>Verify OIDC Configuration</li> </ol> <p>Ensure AWS IAM role exists and trusts your GitHub repository:</p> <ul> <li>OIDC provider exists: <code>token.actions.githubusercontent.com</code></li> <li>IAM role trust policy includes your repository: <code>repo:YOUR_ORG/YOUR_REPO:*</code></li> <li>Role has PowerUserAccess or equivalent permissions</li> </ul> <p>See detailed setup instructions: AWS Setup \u2192 OIDC Configuration</p>"},{"location":"deployment/#deploy-to-test-environment","title":"Deploy to Test Environment","text":"<p>Trigger: Push to <code>test</code> branch</p> <pre><code>git checkout -b test\ngit push origin test\n</code></pre> <p>This automatically: 1. Builds Docker images with <code>-test</code> tags 2. Runs Terraform init with <code>backend-test.hcl</code> 3. Deploys to test environment 4. Outputs allocator URL and SSH key</p> <p>Monitor Progress: - Go to Actions tab in GitHub - Watch <code>Terraform Deploy</code> workflow - Check logs for any errors</p> <p>Access Deployment: - Allocator URL: Available in workflow output - SSH Key: Download from workflow artifacts</p>"},{"location":"deployment/#deploy-to-production","title":"Deploy to Production","text":"<p>Trigger: Manual workflow dispatch</p> <ol> <li> <p>Navigate to Actions tab in GitHub</p> </li> <li> <p>Select \"Terraform Deploy\" workflow</p> </li> <li> <p>Click \"Run workflow\"</p> </li> <li> <p>Fill in parameters:</p> </li> <li>Environment: <code>prod</code></li> <li> <p>Image tag: Specific version (e.g., <code>v1.0.0</code> or commit SHA)</p> </li> <li> <p>Click \"Run workflow\"</p> </li> </ol> <p>Why Manual? Production deployments use pinned image tags for reproducibility and require explicit approval.</p> <p>Production Image Tags</p> <p>Never use <code>:latest</code> or <code>-test</code> tags in production. Always use specific version tags or commit SHAs.</p>"},{"location":"deployment/#deployment-outputs","title":"Deployment Outputs","text":"<p>After successful deployment, the workflow provides:</p> <ul> <li>Allocator FQDN: DNS name for your allocator</li> <li>EC2 Public IP: IP address of the allocator instance</li> <li>EC2 Key Name: Name of the SSH key pair</li> <li>Private Key: Downloaded as artifact (expires in 1 day)</li> </ul>"},{"location":"deployment/#deployment-workflow-details","title":"Deployment Workflow Details","text":"<p>The GitHub Actions workflow (<code>.github/workflows/lablink-allocator-terraform.yml</code>) performs:</p> <ol> <li>Checkout code from repository</li> <li>Configure AWS credentials via OIDC</li> <li>Setup Terraform (version 1.6.6)</li> <li>Determine environment from trigger</li> <li>Inject password secrets - Replace placeholders in config files with GitHub secrets</li> <li>Initialize Terraform with environment-specific backend</li> <li>Validate Terraform configuration</li> <li>Plan infrastructure changes</li> <li>Apply changes to AWS</li> <li>Save SSH key as artifact</li> <li>Output deployment details</li> <li>Destroy on failure (if apply fails)</li> </ol> <p>Password Injection Step: Before Terraform runs, the workflow finds the config file and uses <code>sed</code> to replace <code>PLACEHOLDER_ADMIN_PASSWORD</code> and <code>PLACEHOLDER_DB_PASSWORD</code> with values from GitHub secrets. This ensures passwords never appear in Terraform logs while maintaining secure configuration.</p>"},{"location":"deployment/#method-2-manual-terraform-deployment","title":"Method 2: Manual Terraform Deployment","text":"<p>Deploy directly from your local machine using Terraform CLI.</p>"},{"location":"deployment/#step-1-clone-template-repository","title":"Step 1: Clone Template Repository","text":"<pre><code>git clone https://github.com/talmolab/lablink-template.git\ncd lablink-template/lablink-infrastructure\n</code></pre> <p>All infrastructure configurations are in the <code>lablink-infrastructure/</code> directory within the template repository.</p>"},{"location":"deployment/#step-2-configure-aws-credentials","title":"Step 2: Configure AWS Credentials","text":"<p>Option A: AWS CLI Profiles <pre><code>export AWS_PROFILE=your-profile\naws configure --profile your-profile\n</code></pre></p> <p>Option B: Environment Variables <pre><code>export AWS_ACCESS_KEY_ID=your-access-key\nexport AWS_SECRET_ACCESS_KEY=your-secret-key\nexport AWS_REGION=us-west-2\n</code></pre></p> <p>Option C: AWS SSO <pre><code>aws sso login --profile your-sso-profile\nexport AWS_PROFILE=your-sso-profile\n</code></pre></p>"},{"location":"deployment/#step-3-initialize-terraform","title":"Step 3: Initialize Terraform","text":"<p>For dev environment (local state): <pre><code>terraform init\n</code></pre></p> <p>For test/prod (remote state): <pre><code># Test\nterraform init -backend-config=backend-test.hcl\n\n# Production\nterraform init -backend-config=backend-prod.hcl\n</code></pre></p>"},{"location":"deployment/#step-4-plan-deployment","title":"Step 4: Plan Deployment","text":"<p>Preview infrastructure changes:</p> <pre><code>terraform plan \\\n  -var=\"resource_suffix=dev\" \\\n  -var=\"allocator_image_tag=linux-amd64-latest-test\"\n</code></pre> <p>Review the plan output carefully. Terraform will show: - Resources to be created - Resources to be modified - Resources to be destroyed</p>"},{"location":"deployment/#step-5-apply-deployment","title":"Step 5: Apply Deployment","text":"<p>Deploy the infrastructure:</p> <pre><code>terraform apply \\\n  -var=\"resource_suffix=dev\" \\\n  -var=\"allocator_image_tag=linux-amd64-latest-test\"\n</code></pre> <p>Type <code>yes</code> when prompted to confirm.</p> <p>Deployment time: ~5-10 minutes</p>"},{"location":"deployment/#step-6-get-outputs","title":"Step 6: Get Outputs","text":"<p>After deployment completes:</p> <pre><code># Get allocator URL\nterraform output allocator_fqdn\n\n# Get public IP\nterraform output ec2_public_ip\n\n# Save SSH key\nterraform output -raw private_key_pem &gt; ~/lablink-dev-key.pem\nchmod 600 ~/lablink-dev-key.pem\n</code></pre>"},{"location":"deployment/#step-7-verify-deployment","title":"Step 7: Verify Deployment","text":"<p>Test the allocator:</p> <pre><code># Get the IP\nALLOCATOR_IP=$(terraform output -raw ec2_public_ip)\n\n# Test web interface\ncurl http://$ALLOCATOR_IP:80\n\n# SSH into instance\nssh -i ~/lablink-dev-key.pem ubuntu@$ALLOCATOR_IP\n</code></pre>"},{"location":"deployment/#terraform-variables","title":"Terraform Variables","text":"<p>Key variables for customizing deployment:</p> Variable Description Default Example <code>resource_suffix</code> Environment suffix for resource names <code>dev</code> <code>prod</code>, <code>test</code> <code>allocator_image_tag</code> Docker image tag for allocator (required) <code>v1.0.0</code>, <code>linux-amd64-latest-test</code> <code>instance_type</code> EC2 instance type for allocator <code>t2.micro</code> <code>t2.small</code>, <code>t3.medium</code> <code>allocated_eip</code> Pre-allocated Elastic IP (test/prod) None <code>eipalloc-xxxxx</code> <p>Usage: <pre><code>terraform apply \\\n  -var=\"resource_suffix=prod\" \\\n  -var=\"allocator_image_tag=v1.0.0\" \\\n  -var=\"instance_type=t2.small\" \\\n  -var=\"allocated_eip=eipalloc-xxxxx\"\n</code></pre></p>"},{"location":"deployment/#environment-specific-configurations","title":"Environment-Specific Configurations","text":""},{"location":"deployment/#development","title":"Development","text":"<p>Purpose: Local testing, rapid iteration</p> <p>Configuration: - Terraform state: Local file - Image tag: <code>-test</code> versions - Instance type: <code>t2.micro</code> (cheapest) - No Elastic IP (dynamic)</p> <p>Deploy: <pre><code>terraform init\nterraform apply \\\n  -var=\"resource_suffix=dev\" \\\n  -var=\"allocator_image_tag=linux-amd64-latest-test\"\n</code></pre></p>"},{"location":"deployment/#teststaging","title":"Test/Staging","text":"<p>Purpose: Pre-production validation, integration testing</p> <p>Configuration: - Terraform state: S3 bucket (<code>backend-test.hcl</code>) - Image tag: <code>-test</code> versions - Instance type: Same as production - Elastic IP: Pre-allocated</p> <p>Deploy: <pre><code>terraform init -backend-config=backend-test.hcl\nterraform apply \\\n  -var=\"resource_suffix=test\" \\\n  -var=\"allocator_image_tag=linux-amd64-latest-test\" \\\n  -var=\"allocated_eip=eipalloc-test\"\n</code></pre></p>"},{"location":"deployment/#production","title":"Production","text":"<p>Purpose: Live workloads, stable releases</p> <p>Configuration: - Terraform state: S3 bucket (<code>backend-prod.hcl</code>) - Image tag: Pinned versions (<code>v1.0.0</code>) - Instance type: Appropriately sized - Elastic IP: Pre-allocated - Monitoring and backups enabled</p> <p>Deploy: <pre><code>terraform init -backend-config=backend-prod.hcl\nterraform apply \\\n  -var=\"resource_suffix=prod\" \\\n  -var=\"allocator_image_tag=v1.0.0\" \\\n  -var=\"allocated_eip=eipalloc-prod\"\n</code></pre></p>"},{"location":"deployment/#post-deployment-tasks","title":"Post-Deployment Tasks","text":"<p>After deploying the allocator:</p>"},{"location":"deployment/#1-configure-dns-optional","title":"1. Configure DNS (Optional)","text":"<p>Point a custom domain to your allocator for easier access.</p>"},{"location":"deployment/#using-aws-route-53","title":"Using AWS Route 53","text":"<p>Quick Update: <pre><code># Get IP\nALLOCATOR_IP=$(terraform output -raw ec2_public_ip)\n\n# Update Route 53 A record\naws route53 change-resource-record-sets \\\n  --hosted-zone-id Z1234567890ABC \\\n  --change-batch '{\n    \"Changes\": [{\n      \"Action\": \"UPSERT\",\n      \"ResourceRecordSet\": {\n        \"Name\": \"lablink.yourdomain.com\",\n        \"Type\": \"A\",\n        \"TTL\": 300,\n        \"ResourceRecords\": [{\"Value\": \"'$ALLOCATOR_IP'\"}]\n      }\n    }]\n  }'\n</code></pre></p>"},{"location":"deployment/#example-talmo-lab-dns-configuration","title":"Example: Talmo Lab DNS Configuration","text":"<p>The Talmo Lab LabLink deployment uses the <code>sleap.ai</code> domain with environment-specific subdomains:</p> Environment Subdomain IP Address Purpose Production <code>lablink.sleap.ai</code> <code>44.247.165.126</code> Production allocator Test <code>test.lablink.sleap.ai</code> <code>100.20.149.17</code> Testing environment Dev <code>dev.lablink.sleap.ai</code> <code>34.208.206.60</code> Development environment <p>DNS Configuration: - Type: A Records - TTL: 300 seconds - Managed via: AWS Route 53 - Name Servers:   - <code>ns-158.awsdns-19.com</code>   - <code>ns-697.awsdns-23.net</code>   - <code>ns-1839.awsdns-37.co.uk</code>   - <code>ns-1029.awsdns-00.org</code></p> <p>To replicate this setup:</p> <ol> <li> <p>Create hosted zone in Route 53:    <pre><code>aws route53 create-hosted-zone \\\n  --name sleap.ai \\\n  --caller-reference $(date +%s)\n</code></pre></p> </li> <li> <p>Add A records for each environment:    <pre><code># Production\naws route53 change-resource-record-sets \\\n  --hosted-zone-id YOUR_ZONE_ID \\\n  --change-batch '{\n    \"Changes\": [{\n      \"Action\": \"UPSERT\",\n      \"ResourceRecordSet\": {\n        \"Name\": \"lablink.sleap.ai\",\n        \"Type\": \"A\",\n        \"TTL\": 300,\n        \"ResourceRecords\": [{\"Value\": \"44.247.165.126\"}]\n      }\n    }]\n  }'\n\n# Test environment\naws route53 change-resource-record-sets \\\n  --hosted-zone-id YOUR_ZONE_ID \\\n  --change-batch '{\n    \"Changes\": [{\n      \"Action\": \"UPSERT\",\n      \"ResourceRecordSet\": {\n        \"Name\": \"test.lablink.sleap.ai\",\n        \"Type\": \"A\",\n        \"TTL\": 300,\n        \"ResourceRecords\": [{\"Value\": \"100.20.149.17\"}]\n      }\n    }]\n  }'\n\n# Dev environment\naws route53 change-resource-record-sets \\\n  --hosted-zone-id YOUR_ZONE_ID \\\n  --change-batch '{\n    \"Changes\": [{\n      \"Action\": \"UPSERT\",\n      \"ResourceRecordSet\": {\n        \"Name\": \"dev.lablink.sleap.ai\",\n        \"Type\": \"A\",\n        \"TTL\": 300,\n        \"ResourceRecords\": [{\"Value\": \"34.208.206.60\"}]\n      }\n    }]\n  }'\n</code></pre></p> </li> <li> <p>Verify DNS propagation:    <pre><code># Check if DNS is resolving\nnslookup lablink.sleap.ai\ndig lablink.sleap.ai\n\n# Test all environments\ncurl http://lablink.sleap.ai\ncurl http://test.lablink.sleap.ai\ncurl http://dev.lablink.sleap.ai\n</code></pre></p> </li> </ol> <p>DNS Best Practices</p> <ul> <li>Use environment-specific subdomains (e.g., <code>prod.</code>, <code>test.</code>, <code>dev.</code>)</li> <li>Keep TTL low (300s) for easier updates during initial setup</li> <li>Increase TTL (3600s+) once stable to reduce DNS query costs</li> <li>Use Elastic IPs for production to avoid DNS updates on instance replacement</li> </ul>"},{"location":"deployment/#2-change-default-passwords","title":"2. Change Default Passwords","text":"<p>Security Critical</p> <p>Change default passwords before creating any VMs!</p> <p>SSH into allocator and update configuration: <pre><code>ssh -i ~/lablink-key.pem ubuntu@$ALLOCATOR_IP\nsudo docker exec -it &lt;container&gt; bash\n# Edit config and restart container\n</code></pre></p> <p>See Security \u2192 Change Default Passwords.</p>"},{"location":"deployment/#3-test-vm-creation","title":"3. Test VM Creation","text":"<p>Via web interface: 1. Navigate to <code>http://&lt;allocator-ip&gt;:80</code> 2. Login with admin credentials 3. Go to Admin \u2192 Create Instances 4. Enter number of VMs to create 5. Submit and monitor creation</p> <p>Via API: <pre><code>curl -X POST http://&lt;allocator-ip&gt;:80/request_vm \\\n  -d \"email=test@example.com\" \\\n  -d \"crd_command=test_command\"\n</code></pre></p>"},{"location":"deployment/#4-monitor-logs","title":"4. Monitor Logs","text":"<pre><code># SSH into allocator\nssh -i ~/lablink-key.pem ubuntu@$ALLOCATOR_IP\n\n# Check Docker containers\nsudo docker ps\n\n# View allocator logs\nsudo docker logs &lt;allocator-container-id&gt;\n\n# View PostgreSQL logs\nsudo docker exec -it &lt;allocator-container-id&gt; \\\n  tail -f /var/log/postgresql/postgresql-13-main.log\n</code></pre>"},{"location":"deployment/#updating-a-deployment","title":"Updating a Deployment","text":"<p>To update an existing deployment with new configuration or image:</p> <pre><code># Pull latest code\ngit pull origin main\n\n# Re-initialize if needed\nterraform init -reconfigure\n\n# Plan changes\nterraform plan \\\n  -var=\"resource_suffix=dev\" \\\n  -var=\"allocator_image_tag=new-version\"\n\n# Apply changes\nterraform apply \\\n  -var=\"resource_suffix=dev\" \\\n  -var=\"allocator_image_tag=new-version\"\n</code></pre> <p>Note: Changing the image tag will replace the EC2 instance.</p>"},{"location":"deployment/#destroying-a-deployment","title":"Destroying a Deployment","text":""},{"location":"deployment/#via-github-actions","title":"Via GitHub Actions","text":"<p>Use the destroy workflow:</p> <ol> <li>Go to Actions \u2192 Allocator Master Destroy</li> <li>Click Run workflow</li> <li>Select environment</li> <li>Confirm destruction</li> </ol>"},{"location":"deployment/#via-terraform-cli","title":"Via Terraform CLI","text":"<pre><code>cd lablink-infrastructure\n\nterraform destroy \\\n  -var=\"resource_suffix=dev\" \\\n  -var=\"allocator_image_tag=dummy\"  # Still required\n\n# Type 'yes' to confirm\n</code></pre> <p>Warning: This destroys: - EC2 instance - Security group - SSH key pair - All associated resources</p> <p>Not destroyed: - S3 bucket (Terraform state) - Elastic IPs (must be released manually) - Any client VMs created by the allocator</p>"},{"location":"deployment/#troubleshooting-deployments","title":"Troubleshooting Deployments","text":""},{"location":"deployment/#terraform-init-fails","title":"Terraform Init Fails","text":"<p>Error: <code>Backend configuration changed</code></p> <p>Solution: <pre><code>terraform init -reconfigure\n</code></pre></p>"},{"location":"deployment/#apply-fails-resource-already-exists","title":"Apply Fails: Resource Already Exists","text":"<p>Error: <code>Error creating security group: ... already exists</code></p> <p>Solution: Import existing resource or destroy manually: <pre><code>terraform import aws_security_group.lablink sg-xxxxx\n</code></pre></p>"},{"location":"deployment/#ssh-key-not-working","title":"SSH Key Not Working","text":"<p>Error: <code>Permission denied (publickey)</code></p> <p>Check: <pre><code># Verify key permissions\nls -l ~/lablink-key.pem\n# Should show: -rw------- (600)\n\n# Fix permissions\nchmod 600 ~/lablink-key.pem\n</code></pre></p>"},{"location":"deployment/#instance-not-accessible","title":"Instance Not Accessible","text":"<p>Check: 1. Security group allows port 80 from your IP 2. Instance has public IP 3. Instance is running (<code>aws ec2 describe-instances</code>)</p>"},{"location":"deployment/#terraform-state-locked","title":"Terraform State Locked","text":"<p>Error: <code>Error acquiring the state lock</code></p> <p>Solution: <pre><code># If no other Terraform process is running:\nterraform force-unlock &lt;lock-id&gt;\n</code></pre></p>"},{"location":"deployment/#best-practices","title":"Best Practices","text":"<ol> <li>Use version control: Always commit Terraform configs before applying</li> <li>Review plans: Always run <code>terraform plan</code> before <code>apply</code></li> <li>Pin versions: Use specific image tags in production</li> <li>Separate environments: Never share state between dev/test/prod</li> <li>Backup state: Enable S3 versioning for Terraform state</li> <li>Monitor costs: Set up AWS billing alerts</li> <li>Document changes: Use descriptive commit messages</li> </ol>"},{"location":"deployment/#next-steps","title":"Next Steps","text":"<ul> <li>Workflows: Understand the CI/CD pipeline</li> <li>SSH Access: Connect to your deployed instances</li> <li>Database Management: Manage the allocator database</li> <li>Troubleshooting: Fix common deployment issues</li> </ul>"},{"location":"deployment/#cost-management","title":"Cost Management","text":"<p>See Cost Estimation for expected AWS costs and how to monitor spending.</p>"},{"location":"dns-configuration/","title":"DNS Configuration Guide","text":""},{"location":"dns-configuration/#overview","title":"Overview","text":"<p>LabLink uses AWS Route53 for DNS management with a delegated subdomain structure. This document describes the DNS architecture and configuration for internal reference.</p>"},{"location":"dns-configuration/#dns-architecture","title":"DNS Architecture","text":""},{"location":"dns-configuration/#domain-structure","title":"Domain Structure","text":"<ul> <li>Parent Domain: <code>sleap.ai</code> (managed in Cloudflare)</li> <li>Delegated Subdomain: <code>lablink.sleap.ai</code> (managed in AWS Route53)</li> <li>Example Deployment: <code>test.lablink.sleap.ai</code> \u2192 allocator instance</li> </ul>"},{"location":"dns-configuration/#why-delegated-subdomain","title":"Why Delegated Subdomain?","text":"<p>The main <code>sleap.ai</code> domain is managed in Cloudflare for the primary website. To avoid conflicts and allow AWS-based automation, we use NS delegation to hand off the <code>lablink.sleap.ai</code> subdomain to AWS Route53.</p> <p>Benefits: - Terraform can manage DNS records automatically - No Cloudflare API credentials needed - Separation of concerns (website vs LabLink infrastructure) - Let's Encrypt can validate domain ownership via DNS</p>"},{"location":"dns-configuration/#route53-setup","title":"Route53 Setup","text":""},{"location":"dns-configuration/#hosted-zone-configuration","title":"Hosted Zone Configuration","text":"<p>Zone Name: <code>lablink.sleap.ai</code> Zone ID: <code>Z010760118DSWF5IYKMOM</code> Type: Public hosted zone</p> <p>Nameservers (AWS-assigned): <pre><code>ns-158.awsdns-19.com\nns-697.awsdns-23.net\nns-1839.awsdns-37.co.uk\nns-1029.awsdns-00.org\n</code></pre></p>"},{"location":"dns-configuration/#cloudflare-ns-delegation","title":"Cloudflare NS Delegation","text":"<p>In Cloudflare DNS for <code>sleap.ai</code>, the following NS records delegate the subdomain to AWS:</p> <p>Record Type: NS Name: <code>lablink</code> Content (4 records): <pre><code>ns-158.awsdns-19.com\nns-697.awsdns-23.net\nns-1839.awsdns-37.co.uk\nns-1029.awsdns-00.org\n</code></pre></p> <p>TTL: Auto (or 300 seconds)</p>"},{"location":"dns-configuration/#verification","title":"Verification","text":"<pre><code># Query AWS nameservers directly\ndig @ns-158.awsdns-19.com lablink.sleap.ai\n\n# Check NS delegation\ndig NS lablink.sleap.ai\n\n# Should show AWS nameservers, not Cloudflare\n</code></pre>"},{"location":"dns-configuration/#lablink-dns-configuration","title":"LabLink DNS Configuration","text":""},{"location":"dns-configuration/#configuration-file","title":"Configuration File","text":"<p>Location: <code>lablink-infrastructure/config/config.yaml</code></p> <pre><code>dns:\n  enabled: true\n  domain: \"lablink.sleap.ai\"\n  zone_id: \"Z010760118DSWF5IYKMOM\"  # Hardcoded to avoid zone lookup issues\n  app_name: \"\"                       # Empty when using custom pattern\n  pattern: \"custom\"                  # Use custom subdomain\n  custom_subdomain: \"test\"           # Creates test.lablink.sleap.ai\n  create_zone: false                 # Zone already exists, don't create\n</code></pre>"},{"location":"dns-configuration/#dns-patterns","title":"DNS Patterns","text":"<p>LabLink supports multiple DNS naming patterns:</p>"},{"location":"dns-configuration/#1-custom-pattern-recommended","title":"1. Custom Pattern (Recommended)","text":"<pre><code>pattern: \"custom\"\ncustom_subdomain: \"test\"\n# Result: test.lablink.sleap.ai\n</code></pre>"},{"location":"dns-configuration/#2-auto-pattern-environment-based","title":"2. Auto Pattern (environment-based)","text":"<pre><code>pattern: \"auto\"\n# Result: {environment}.lablink.sleap.ai\n# Examples: dev.lablink.sleap.ai, test.lablink.sleap.ai, prod.lablink.sleap.ai\n</code></pre>"},{"location":"dns-configuration/#3-timestamp-pattern","title":"3. Timestamp Pattern","text":"<pre><code>pattern: \"timestamp\"\n# Result: lablink-20251004-143022.lablink.sleap.ai\n</code></pre>"},{"location":"dns-configuration/#zone-id-configuration","title":"Zone ID Configuration","text":"<p>Why hardcode zone_id?</p> <p>The Terraform data source <code>aws_route53_zone</code> can incorrectly match parent zones when searching by domain name. To ensure the correct zone is always used, we hardcode the zone ID in the config:</p> <pre><code>dns:\n  zone_id: \"Z010760118DSWF5IYKMOM\"  # Forces use of lablink.sleap.ai zone\n</code></pre> <p>How to find your zone ID: <pre><code>aws route53 list-hosted-zones --query \"HostedZones[?Name=='lablink.sleap.ai.'].Id\" --output text\n</code></pre></p>"},{"location":"dns-configuration/#terraform-dns-management","title":"Terraform DNS Management","text":""},{"location":"dns-configuration/#main-configuration","title":"Main Configuration","text":"<p>Location: <code>lablink-infrastructure/main.tf</code></p> <pre><code># DNS configuration from config.yaml\nlocals {\n  dns_enabled          = try(local.config_file.dns.enabled, false)\n  dns_domain           = try(local.config_file.dns.domain, \"\")\n  dns_zone_id          = try(local.config_file.dns.zone_id, \"\")\n  dns_app_name         = try(local.config_file.dns.app_name, \"lablink\")\n  dns_pattern          = try(local.config_file.dns.pattern, \"auto\")\n  dns_custom_subdomain = try(local.config_file.dns.custom_subdomain, \"\")\n  dns_create_zone      = try(local.config_file.dns.create_zone, false)\n}\n\n# Zone selection priority: hardcoded zone_id &gt; create new &gt; lookup existing\nlocals {\n  zone_id = local.dns_enabled ? (\n    local.dns_zone_id != \"\" ? local.dns_zone_id : (\n      local.dns_create_zone ? aws_route53_zone.new[0].zone_id : data.aws_route53_zone.existing[0].zone_id\n    )\n  ) : \"\"\n}\n</code></pre>"},{"location":"dns-configuration/#dns-record-creation","title":"DNS Record Creation","text":"<pre><code>resource \"aws_route53_record\" \"allocator\" {\n  count   = local.dns_enabled ? 1 : 0\n  zone_id = local.zone_id\n  name    = local.fqdn\n  type    = \"A\"\n  ttl     = 300\n  records = [local.allocator_public_ip]\n}\n</code></pre>"},{"location":"dns-configuration/#ssltls-configuration","title":"SSL/TLS Configuration","text":""},{"location":"dns-configuration/#lets-encrypt-integration","title":"Let's Encrypt Integration","text":"<p>LabLink uses Caddy for automatic SSL certificate acquisition from Let's Encrypt.</p> <p>Prerequisites: - Valid DNS record pointing to allocator IP - DNS propagated to public resolvers (Google DNS 8.8.8.8, Cloudflare DNS 1.1.1.1) - Port 80 and 443 accessible</p> <p>Configuration: <pre><code>ssl:\n  provider: \"letsencrypt\"\n  email: \"admin@example.com\"\n</code></pre></p> <p>Caddy Configuration (<code>lablink-infrastructure/user_data.sh</code>): <pre><code>cat &gt; /etc/caddy/Caddyfile &lt;&lt;EOF\n${fqdn} {\n    reverse_proxy localhost:5000\n}\nEOF\n</code></pre></p> <p>Caddy automatically: 1. Requests certificate from Let's Encrypt 2. Validates domain ownership via HTTP-01 challenge 3. Renews certificates before expiration 4. Redirects HTTP to HTTPS</p>"},{"location":"dns-configuration/#ssl-troubleshooting","title":"SSL Troubleshooting","text":"<p>Check Caddy logs: <pre><code>ssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\nsudo journalctl -u caddy -f\n</code></pre></p> <p>Common issues: - DNS not propagated \u2192 Wait 5-10 minutes - Port 80/443 blocked \u2192 Check security group rules - Invalid domain \u2192 Verify DNS record exists</p>"},{"location":"dns-configuration/#dns-troubleshooting","title":"DNS Troubleshooting","text":""},{"location":"dns-configuration/#issue-record-created-in-wrong-zone","title":"Issue: Record Created in Wrong Zone","text":"<p>Symptom: DNS record appears in <code>sleap.ai</code> zone instead of <code>lablink.sleap.ai</code> zone</p> <p>Cause: Terraform data source matched parent zone</p> <p>Solution: Add <code>zone_id</code> to config.yaml: <pre><code>dns:\n  zone_id: \"Z010760118DSWF5IYKMOM\"\n</code></pre></p>"},{"location":"dns-configuration/#issue-dns-not-resolving","title":"Issue: DNS Not Resolving","text":"<p>Check DNS propagation: <pre><code># Check Google DNS\nnslookup test.lablink.sleap.ai 8.8.8.8\n\n# Check Cloudflare DNS\nnslookup test.lablink.sleap.ai 1.1.1.1\n\n# Check authoritative nameservers\nnslookup test.lablink.sleap.ai ns-158.awsdns-19.com\n</code></pre></p> <p>Verify Route53 record: <pre><code>aws route53 list-resource-record-sets \\\n  --hosted-zone-id Z010760118DSWF5IYKMOM \\\n  --query \"ResourceRecordSets[?Name=='test.lablink.sleap.ai.']\"\n</code></pre></p> <p>Check NS delegation: <pre><code>dig NS lablink.sleap.ai\n# Should return AWS nameservers, not Cloudflare\n</code></pre></p>"},{"location":"dns-configuration/#issue-ns-delegation-not-working","title":"Issue: NS Delegation Not Working","text":"<p>Symptom: DNS queries return NXDOMAIN even though record exists in Route53</p> <p>Cause: NS records not properly configured in Cloudflare</p> <p>Solution: 1. Log into Cloudflare 2. Go to DNS settings for <code>sleap.ai</code> 3. Add/verify NS records for <code>lablink</code> pointing to all 4 AWS nameservers 4. Wait 5-15 minutes for propagation</p>"},{"location":"dns-configuration/#issue-multiple-hosted-zones-conflict","title":"Issue: Multiple Hosted Zones Conflict","text":"<p>Symptom: Both <code>sleap.ai</code> and <code>lablink.sleap.ai</code> zones exist in Route53</p> <p>Solution: Delete the parent zone from Route53 if it's managed elsewhere (Cloudflare) <pre><code># List zones\naws route53 list-hosted-zones\n\n# Delete parent zone (if managed in Cloudflare)\naws route53 delete-hosted-zone --id &lt;zone-id&gt;\n</code></pre></p>"},{"location":"dns-configuration/#dns-verification-script","title":"DNS Verification Script","text":"<p>Use the deployment verification script to check DNS:</p> <pre><code>cd lablink-infrastructure\n./verify-deployment.sh test.lablink.sleap.ai 52.40.142.146\n</code></pre> <p>This checks: 1. DNS resolution via Google/Cloudflare DNS 2. HTTP connectivity 3. HTTPS/SSL certificate (if enabled)</p>"},{"location":"dns-configuration/#configuration-templates","title":"Configuration Templates","text":""},{"location":"dns-configuration/#development-environment","title":"Development Environment","text":"<pre><code>dns:\n  enabled: true\n  domain: \"lablink.sleap.ai\"\n  zone_id: \"Z010760118DSWF5IYKMOM\"\n  pattern: \"custom\"\n  custom_subdomain: \"dev\"\n  create_zone: false\n\nssl:\n  provider: \"letsencrypt\"\n  email: \"dev@example.com\"\n</code></pre>"},{"location":"dns-configuration/#test-environment","title":"Test Environment","text":"<pre><code>dns:\n  enabled: true\n  domain: \"lablink.sleap.ai\"\n  zone_id: \"Z010760118DSWF5IYKMOM\"\n  pattern: \"custom\"\n  custom_subdomain: \"test\"\n  create_zone: false\n\nssl:\n  provider: \"letsencrypt\"\n  email: \"test@example.com\"\n</code></pre>"},{"location":"dns-configuration/#production-environment","title":"Production Environment","text":"<pre><code>dns:\n  enabled: true\n  domain: \"lablink.sleap.ai\"\n  zone_id: \"Z010760118DSWF5IYKMOM\"\n  pattern: \"custom\"\n  custom_subdomain: \"app\"  # or just use root: lablink.sleap.ai\n  create_zone: false\n\nssl:\n  provider: \"letsencrypt\"\n  email: \"admin@example.com\"\n</code></pre>"},{"location":"dns-configuration/#ip-only-deployment-no-dns","title":"IP-Only Deployment (No DNS)","text":"<pre><code>dns:\n  enabled: false\n\nssl:\n  provider: \"none\"\n</code></pre>"},{"location":"dns-configuration/#best-practices","title":"Best Practices","text":"<ol> <li>Always hardcode zone_id - Prevents zone lookup issues</li> <li>Use custom pattern - Explicit control over subdomain names</li> <li>Verify NS delegation - Check before first deployment</li> <li>Wait for DNS propagation - Allow 5-15 minutes after changes</li> <li>Test with verification script - Automate DNS/SSL checks</li> <li>Document all changes - Record zone IDs and nameservers</li> </ol>"},{"location":"dns-configuration/#security-considerations","title":"Security Considerations","text":""},{"location":"dns-configuration/#dns-security","title":"DNS Security","text":"<ul> <li>Route53 hosted zones are public by default</li> <li>Use IAM policies to restrict who can modify DNS records</li> <li>Enable CloudTrail logging for DNS changes</li> <li>Consider DNSSEC for additional security (advanced)</li> </ul>"},{"location":"dns-configuration/#ssltls-security","title":"SSL/TLS Security","text":"<ul> <li>Let's Encrypt certificates are valid for 90 days</li> <li>Caddy handles automatic renewal</li> <li>Monitor certificate expiration in Caddy logs</li> <li>Use strong cipher suites (Caddy defaults are secure)</li> </ul>"},{"location":"dns-configuration/#monitoring-and-maintenance","title":"Monitoring and Maintenance","text":""},{"location":"dns-configuration/#regular-checks","title":"Regular Checks","text":"<ul> <li>Verify DNS records exist in correct zone</li> <li>Check SSL certificate expiration</li> <li>Monitor Caddy logs for renewal issues</li> <li>Review Route53 query metrics</li> </ul>"},{"location":"dns-configuration/#backup-information","title":"Backup Information","text":"<p>Keep this information documented: - Zone ID: <code>Z010760118DSWF5IYKMOM</code> - Nameservers: (listed above) - Parent domain registrar: Cloudflare - SSL provider: Let's Encrypt via Caddy</p>"},{"location":"dns-configuration/#references","title":"References","text":"<ul> <li>AWS Route53 Documentation</li> <li>Cloudflare DNS Documentation</li> <li>Let's Encrypt Documentation</li> <li>Caddy Documentation</li> <li>LabLink Configuration Guide</li> <li>LabLink Deployment Guide</li> </ul>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":"<p>Common questions and answers about LabLink.</p>"},{"location":"faq/#general","title":"General","text":""},{"location":"faq/#what-is-lablink","title":"What is LabLink?","text":"<p>LabLink is a dynamic VM allocation and management system designed for computational research. It automates the deployment and management of cloud-based VMs for running research software like SLEAP or custom tools.</p>"},{"location":"faq/#who-is-lablink-for","title":"Who is LabLink for?","text":"<ul> <li>Research labs needing on-demand GPU compute resources</li> <li>Scientists running batch computational workloads</li> <li>Teams training machine learning models on cloud infrastructure</li> <li>Anyone needing automated VM management for containerized software</li> </ul>"},{"location":"faq/#what-cloud-providers-does-lablink-support","title":"What cloud providers does LabLink support?","text":"<p>Currently, LabLink supports AWS (Amazon Web Services) only. The architecture uses AWS-specific services (EC2, S3, IAM).</p>"},{"location":"faq/#is-lablink-free","title":"Is LabLink free?","text":"<p>LabLink itself is open-source and free. However, you'll pay for AWS resources you use (EC2 instances, S3 storage, etc.). See Cost Estimation.</p>"},{"location":"faq/#installation-setup","title":"Installation &amp; Setup","text":""},{"location":"faq/#do-i-need-an-aws-account","title":"Do I need an AWS account?","text":"<p>Yes. LabLink deploys to AWS and requires an AWS account with permissions to create EC2 instances, security groups, and other resources.</p>"},{"location":"faq/#how-long-does-setup-take","title":"How long does setup take?","text":"<ul> <li>Initial AWS setup: 1-2 hours (first time)</li> <li>Local testing: 5-10 minutes</li> <li>First deployment: 10-15 minutes</li> </ul>"},{"location":"faq/#can-i-run-lablink-locally-without-aws","title":"Can I run LabLink locally without AWS?","text":"<p>You can run the allocator locally with Docker for testing, but creating client VMs requires AWS.</p>"},{"location":"faq/#configuration","title":"Configuration","text":""},{"location":"faq/#how-do-i-change-the-gpu-type","title":"How do I change the GPU type?","text":"<p>Edit the allocator configuration:</p> <pre><code># lablink-infrastructure/config/config.yaml\nmachine:\n  machine_type: \"g5.xlarge\"  # Change to desired instance type\n</code></pre> <p>See Configuration \u2192 Machine Type Options for available types.</p>"},{"location":"faq/#how-do-i-use-my-own-research-software","title":"How do I use my own research software?","text":"<ol> <li>Create a Docker image with your software</li> <li>Push to a container registry (e.g., ghcr.io)</li> <li>Update configuration with your image URL</li> <li>Optionally specify your code repository</li> </ol> <p>See Adapting LabLink for detailed guide.</p>"},{"location":"faq/#can-i-use-a-different-aws-region","title":"Can I use a different AWS region?","text":"<p>Yes. Update the region in configuration:</p> <pre><code>app:\n  region: \"us-east-1\"  # Change to your preferred region\n</code></pre> <p>Important: AMI IDs are region-specific. You'll need to find the appropriate AMI for your region.</p>"},{"location":"faq/#how-do-i-change-default-passwords","title":"How do I change default passwords?","text":"<p>Critical for production!</p> <pre><code># In config.yaml\napp:\n  admin_user: \"admin\"\n  admin_password: \"YOUR_SECURE_PASSWORD\"\n\ndb:\n  password: \"YOUR_SECURE_DB_PASSWORD\"\n</code></pre> <p>Or use environment variables. See Security \u2192 Change Default Passwords.</p>"},{"location":"faq/#why-does-my-browser-say-not-secure","title":"Why does my browser say \"Not Secure\"?","text":"<p>You're using staging mode (<code>ssl.staging: true</code>), which serves HTTP only (no encryption). This is expected for testing.</p> <p>To get a secure HTTPS connection with browser padlock, set <code>ssl.staging: false</code> in your configuration and redeploy.</p> <p>See Configuration \u2192 SSL Options.</p>"},{"location":"faq/#why-cant-i-access-the-allocator-in-my-browser","title":"Why can't I access the allocator in my browser?","text":"<p>If your browser cannot connect to <code>http://your-domain.com</code>:</p> <ol> <li>Make sure you explicitly type <code>http://</code> (not <code>https://</code>)</li> <li>Clear your browser's HSTS cache (see Troubleshooting \u2192 Browser HSTS)</li> <li>Try incognito/private browsing mode</li> <li>Try accessing via IP address: <code>http://&lt;allocator-ip&gt;</code></li> </ol>"},{"location":"faq/#should-i-use-staging-or-production-mode","title":"Should I use staging or production mode?","text":"<p>Use staging mode (<code>ssl.staging: true</code>) for:</p> <ul> <li>Initial testing and development</li> <li>Frequent deployments (unlimited)</li> <li>Testing infrastructure changes</li> <li>CI/CD automated tests</li> </ul> <p>Use production mode (<code>ssl.staging: false</code>) for:</p> <ul> <li>Production deployments</li> <li>Internet-accessible allocators</li> <li>Handling sensitive data</li> <li>Long-running deployments</li> </ul> <p>Key difference: Staging = HTTP only (fast, unlimited, no encryption). Production = HTTPS with trusted certificates (secure, rate limited).</p> <p>See Configuration \u2192 Staging vs Production.</p>"},{"location":"faq/#how-many-times-can-i-deploy-with-staging-mode","title":"How many times can I deploy with staging mode?","text":"<p>Unlimited. Staging mode uses HTTP only, so there are no Let's Encrypt rate limits.</p> <p>With production mode, you're limited to 5 duplicate certificates per week.</p>"},{"location":"faq/#can-i-switch-from-staging-to-production-mode","title":"Can I switch from staging to production mode?","text":"<p>Yes. Change the configuration and redeploy:</p> <pre><code>ssl:\n  staging: false\n</code></pre> <p>Then run: <pre><code>terraform apply\n</code></pre></p> <p>The allocator will obtain a trusted Let's Encrypt certificate and start serving HTTPS. You may need to clear your browser's HSTS cache.</p>"},{"location":"faq/#deployment","title":"Deployment","text":""},{"location":"faq/#whats-the-difference-between-dev-test-and-prod-environments","title":"What's the difference between dev, test, and prod environments?","text":"Environment Purpose Image Tags Terraform State dev Local development <code>-test</code> Local file test Staging/pre-prod <code>-test</code> S3 bucket prod Production Pinned versions S3 bucket <p>See Deployment \u2192 Environment-Specific Configurations.</p>"},{"location":"faq/#how-do-i-deploy-to-production","title":"How do I deploy to production?","text":"<ol> <li>Navigate to Actions tab in GitHub</li> <li>Select \"Terraform Deploy\" workflow</li> <li>Click \"Run workflow\"</li> <li>Select <code>prod</code> environment</li> <li>Enter specific image tag (e.g., <code>v1.0.0</code>)</li> <li>Click \"Run workflow\"</li> </ol> <p>Never use <code>:latest</code> in production!</p>"},{"location":"faq/#can-i-deploy-without-github-actions","title":"Can I deploy without GitHub Actions?","text":"<p>Yes, using Terraform CLI:</p> <pre><code>cd lablink-allocator\nterraform init\nterraform apply -var=\"resource_suffix=prod\" -var=\"allocator_image_tag=v1.0.0\"\n</code></pre> <p>See Deployment \u2192 Method 2: Manual Terraform.</p>"},{"location":"faq/#how-do-i-update-an-existing-deployment","title":"How do I update an existing deployment?","text":"<pre><code># Pull latest code\ngit pull\n\n# Re-apply Terraform with new image tag\nterraform apply -var=\"resource_suffix=prod\" -var=\"allocator_image_tag=v1.1.0\"\n</code></pre> <p>This will replace the EC2 instance with the new image.</p>"},{"location":"faq/#operations","title":"Operations","text":""},{"location":"faq/#how-do-i-create-client-vms","title":"How do I create client VMs?","text":"<p>Via Web UI: 1. Navigate to allocator web interface 2. Login with admin credentials 3. Go to Admin \u2192 Create Instances 4. Enter number of VMs 5. Submit</p> <p>Via API: <pre><code>curl -X POST http://&lt;allocator-ip&gt;:80/admin/create \\\n  -u admin:password \\\n  -d \"instance_count=5\"\n</code></pre></p>"},{"location":"faq/#how-do-i-check-vm-status","title":"How do I check VM status?","text":"<p>Via Web UI: - Navigate to Admin \u2192 View Instances</p> <p>Via Database: <pre><code>ssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\nsudo docker exec &lt;container-id&gt; psql -U lablink -d lablink_db -c \"SELECT hostname, status, email FROM vms;\"\n</code></pre></p>"},{"location":"faq/#how-do-i-destroy-a-deployment","title":"How do I destroy a deployment?","text":"<p>Via GitHub Actions: 1. Actions \u2192 \"Allocator Master Destroy\" 2. Run workflow 3. Select environment</p> <p>Via Terraform CLI: <pre><code>terraform destroy -var=\"resource_suffix=dev\"\n</code></pre></p>"},{"location":"faq/#what-happens-if-i-destroy-the-allocator","title":"What happens if I destroy the allocator?","text":"<ul> <li>Allocator EC2 instance terminated</li> <li>Database data lost (unless backed up)</li> <li>Client VMs remain running (must be destroyed separately)</li> </ul> <p>Always backup database before destroying!</p>"},{"location":"faq/#troubleshooting","title":"Troubleshooting","text":""},{"location":"faq/#postgresql-wont-start-after-deployment","title":"PostgreSQL won't start after deployment","text":"<p>Known issue. Solution:</p> <pre><code>ssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\nsudo docker exec -it &lt;container-id&gt; bash\n/etc/init.d/postgresql restart\n</code></pre> <p>See Troubleshooting \u2192 PostgreSQL Issues.</p>"},{"location":"faq/#i-cant-ssh-into-the-instance","title":"I can't SSH into the instance","text":"<p>Check: 1. Key permissions: <code>chmod 600 ~/lablink-key.pem</code> 2. Security group allows port 22 3. Using correct IP address 4. Using correct user (<code>ubuntu</code>)</p> <p>See Troubleshooting \u2192 SSH Access Issues.</p>"},{"location":"faq/#client-vms-arent-being-created","title":"Client VMs aren't being created","text":"<p>Check: 1. AWS credentials configured in allocator 2. Allocator container logs for errors 3. IAM permissions for EC2 operations</p> <p>See Troubleshooting \u2192 VM Spawning Issues.</p>"},{"location":"faq/#im-getting-billed-unexpectedly","title":"I'm getting billed unexpectedly","text":"<ul> <li>Check for running EC2 instances you forgot to terminate</li> <li>Set up billing alerts (see AWS Setup \u2192 Billing Alerts)</li> <li>Review Cost Estimation guide</li> </ul>"},{"location":"faq/#costs","title":"Costs","text":""},{"location":"faq/#how-much-does-lablink-cost-to-run","title":"How much does LabLink cost to run?","text":"<p>AWS infrastructure: - S3 bucket: ~$0.05/month - Elastic IPs: Free while associated</p> <p>Running costs (per hour): - Allocator (t2.micro): $0.0116/hour (~$8.50/month if running 24/7) - Client VM (g4dn.xlarge): $0.526/hour</p> <p>See Cost Estimation for detailed breakdown.</p>"},{"location":"faq/#how-can-i-reduce-costs","title":"How can I reduce costs?","text":"<ol> <li>Terminate VMs when not in use</li> <li>Use Spot Instances for client VMs (up to 90% savings)</li> <li>Use smaller instance types for testing</li> <li>Set up billing alerts to monitor spending</li> <li>Use Reserved Instances for long-running allocators (up to 75% savings)</li> </ol>"},{"location":"faq/#do-i-get-charged-for-stopped-instances","title":"Do I get charged for stopped instances?","text":"<ul> <li>EC2 instances: No compute charges, but EBS storage charges apply</li> <li>Elastic IPs: Free while associated, $0.005/hour if unassociated</li> </ul> <p>Best practice: Terminate (not stop) instances when done.</p>"},{"location":"faq/#advanced","title":"Advanced","text":""},{"location":"faq/#can-i-use-a-custom-ami","title":"Can I use a custom AMI?","text":"<p>Yes. Create an AMI with your software pre-installed:</p> <pre><code># Create AMI from running instance\naws ec2 create-image --instance-id i-xxxxx --name \"my-custom-ami\"\n\n# Use in configuration\nmachine:\n  ami_id: \"ami-your-custom-ami-id\"\n</code></pre> <p>See Adapting LabLink \u2192 Custom AMI.</p>"},{"location":"faq/#can-i-use-rds-instead-of-postgresql-in-docker","title":"Can I use RDS instead of PostgreSQL in Docker?","text":"<p>Yes, for production. See Database \u2192 Migrating to RDS.</p> <p>Benefits: - Automated backups - High availability - Managed updates</p>"},{"location":"faq/#can-i-use-lablink-with-multiple-aws-accounts","title":"Can I use LabLink with multiple AWS accounts?","text":"<p>Yes. Deploy separate instances with different AWS credentials/roles for each account.</p>"},{"location":"faq/#can-i-add-my-own-api-endpoints","title":"Can I add my own API endpoints?","text":"<p>Yes. Edit the allocator service in <code>packages/allocator/src/lablink_allocator/main.py</code>:</p> <pre><code>@app.route('/my-custom-endpoint', methods=['POST'])\ndef my_custom_endpoint():\n    # Your code here\n    return jsonify({'status': 'success'})\n</code></pre> <p>Rebuild the Docker image and redeploy.</p>"},{"location":"faq/#how-do-i-enable-https","title":"How do I enable HTTPS?","text":"<ol> <li>Get SSL certificate (e.g., Let's Encrypt)</li> <li>Configure nginx or use AWS Application Load Balancer</li> <li>Update security groups to allow port 443</li> </ol> <p>See Security \u2192 Encryption in Transit.</p>"},{"location":"faq/#security","title":"Security","text":""},{"location":"faq/#is-it-safe-to-use-default-passwords","title":"Is it safe to use default passwords?","text":"<p>No! Change them immediately for any non-local deployment.</p> <p>See Security \u2192 Change Default Passwords.</p>"},{"location":"faq/#how-are-aws-credentials-stored","title":"How are AWS credentials stored?","text":"<p>For GitHub Actions: OIDC (no stored credentials)</p> <p>For local: AWS credentials file or environment variables</p> <p>Never commit credentials to version control!</p>"},{"location":"faq/#how-are-ssh-keys-managed","title":"How are SSH keys managed?","text":"<ul> <li>Terraform generates unique keys per environment</li> <li>Keys stored in Terraform state</li> <li>GitHub Actions exposes keys as temporary artifacts (1 day expiration)</li> <li>Rotate keys by destroying and recreating infrastructure</li> </ul> <p>See SSH Access \u2192 Key Management.</p>"},{"location":"faq/#contributing","title":"Contributing","text":""},{"location":"faq/#can-i-contribute-to-lablink","title":"Can I contribute to LabLink?","text":"<p>Yes! LabLink is open-source. Contributions welcome:</p> <ol> <li>Fork repository</li> <li>Create feature branch</li> <li>Make changes</li> <li>Add tests</li> <li>Submit pull request</li> </ol>"},{"location":"faq/#how-do-i-report-bugs","title":"How do I report bugs?","text":"<p>Open an issue on GitHub with: - Description of the bug - Steps to reproduce - Expected vs actual behavior - Logs/error messages - Environment details</p>"},{"location":"faq/#where-can-i-ask-questions","title":"Where can I ask questions?","text":"<ul> <li>GitHub Issues: For bugs and feature requests</li> <li>GitHub Discussions: For questions and general discussion</li> </ul>"},{"location":"faq/#comparison","title":"Comparison","text":""},{"location":"faq/#how-is-lablink-different-from-aws-batch","title":"How is LabLink different from AWS Batch?","text":"Feature LabLink AWS Batch Setup complexity Moderate High Custom software Easy (Docker) Easy (Docker) GPU support Yes Yes Cost Pay for VMs Pay for VMs + Batch overhead Web UI Included Requires building VM management Automated Automated Learning curve Moderate Steep <p>LabLink advantage: Simpler setup, included web UI, research-focused</p>"},{"location":"faq/#how-is-lablink-different-from-kubernetes","title":"How is LabLink different from Kubernetes?","text":"<p>LabLink is simpler and more focused:</p> <ul> <li>LabLink: VM allocation for research workloads</li> <li>Kubernetes: General-purpose container orchestration</li> </ul> <p>If you need simple VM management for research, use LabLink. If you need complex microservices orchestration, use Kubernetes.</p>"},{"location":"faq/#still-have-questions","title":"Still Have Questions?","text":"<ul> <li>Check Documentation</li> <li>Search GitHub Issues</li> <li>Open new issue with your question</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker 20.10+</li> <li>Python 3.9+ (for development)</li> <li>8GB RAM, 10GB disk space</li> </ul>"},{"location":"installation/#install-via-docker","title":"Install via Docker","text":"AllocatorClient <pre><code>docker pull ghcr.io/talmolab/lablink-allocator-image:latest\ndocker run -d -p 5000:5000 --name lablink-allocator \\\n  ghcr.io/talmolab/lablink-allocator-image:latest\n</code></pre> <p>Access at http://localhost:5000</p> <pre><code>docker pull ghcr.io/talmolab/lablink-client-base-image:latest\ndocker run -d --name lablink-client \\\n  -e ALLOCATOR_HOST=&lt;allocator_ip&gt; \\\n  -e ALLOCATOR_PORT=80 \\\n  ghcr.io/talmolab/lablink-client-base-image:latest\n</code></pre>"},{"location":"installation/#install-from-source","title":"Install from source","text":"CloneBuild allocatorBuild client <pre><code>git clone https://github.com/talmolab/lablink.git\ncd lablink\n</code></pre> <pre><code>docker build -t lablink-allocator \\\n  -f packages/allocator/Dockerfile .\ndocker run -d -p 5000:5000 lablink-allocator\n</code></pre> <pre><code>docker build -t lablink-client \\\n  -f packages/client/Dockerfile .\n</code></pre>"},{"location":"installation/#python-development","title":"Python development","text":"Install uvAllocatorClient <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <pre><code>cd packages/allocator\nuv sync\nuv run lablink-allocator\n</code></pre> <pre><code>cd packages/client\nuv sync\nuv run subscribe\n</code></pre>"},{"location":"installation/#configuration","title":"Configuration","text":"<p>See Configuration for environment variables and config files.</p>"},{"location":"installation/#next-steps","title":"Next steps","text":"<ul> <li>Quickstart - Deploy to AWS</li> <li>Configuration - Customize settings</li> <li>Troubleshooting - Common issues</li> </ul>"},{"location":"prerequisites/","title":"Prerequisites","text":"<p>Before deploying LabLink, you'll need to set up several tools and accounts. This guide covers everything you need to get started.</p>"},{"location":"prerequisites/#required-tools","title":"Required Tools","text":""},{"location":"prerequisites/#1-aws-account","title":"1. AWS Account","text":"<p>You'll need an AWS account with appropriate permissions to create:</p> <ul> <li>EC2 instances</li> <li>Security groups</li> <li>Elastic IPs</li> <li>S3 buckets (for Terraform state)</li> <li>IAM roles and policies</li> </ul> <p>Cost Considerations: See the Cost Estimation guide for expected AWS costs.</p>"},{"location":"prerequisites/#2-aws-cli","title":"2. AWS CLI","text":"<p>Install the AWS Command Line Interface:</p> macOSLinuxWindows <pre><code>brew install awscli\n</code></pre> <pre><code>curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install\n</code></pre> <p>Download and run the AWS CLI MSI installer</p> <p>Verify installation: <pre><code>aws --version\n</code></pre></p>"},{"location":"prerequisites/#configure-aws-credentials","title":"Configure AWS Credentials","text":"<p>You have two options:</p> <p>Option 1: AWS Access Keys (Local Development) <pre><code>aws configure\n</code></pre></p> <p>Enter your:</p> <ul> <li>AWS Access Key ID</li> <li>AWS Secret Access Key</li> <li>Default region (e.g., <code>us-west-2</code>)</li> <li>Default output format (<code>json</code>)</li> </ul> <p>Option 2: OIDC (GitHub Actions)</p> <p>For automated deployments, you'll configure OpenID Connect (OIDC) to allow GitHub Actions to assume an IAM role without storing credentials. See AWS Setup from Scratch for details.</p>"},{"location":"prerequisites/#3-terraform","title":"3. Terraform","text":"<p>Install Terraform for infrastructure provisioning:</p> macOSLinuxWindows <pre><code>brew tap hashicorp/tap\nbrew install hashicorp/tap/terraform\n</code></pre> <pre><code>wget https://releases.hashicorp.com/terraform/1.6.6/terraform_1.6.6_linux_amd64.zip\nunzip terraform_1.6.6_linux_amd64.zip\nsudo mv terraform /usr/local/bin/\n</code></pre> <p>Download from Terraform Downloads and add to PATH</p> <p>Verify installation: <pre><code>terraform version\n</code></pre></p> <p>Version Requirement: LabLink uses Terraform 1.6.6 (as specified in the CI workflow).</p>"},{"location":"prerequisites/#4-docker","title":"4. Docker","text":"<p>Install Docker for local testing and development:</p> macOSLinuxWindows <p>Download Docker Desktop for Mac</p> <pre><code>curl -fsSL https://get.docker.com -o get-docker.sh\nsudo sh get-docker.sh\n</code></pre> <p>Download Docker Desktop for Windows</p> <p>Verify installation: <pre><code>docker --version\ndocker ps\n</code></pre></p> <p>Docker Permissions (Linux)</p> <p>If you encounter permission errors: <pre><code>sudo usermod -aG docker $USER\nnewgrp docker\n</code></pre></p>"},{"location":"prerequisites/#5-git","title":"5. Git","text":"<p>Git should already be installed on most systems. Verify: <pre><code>git --version\n</code></pre></p> <p>If not installed:</p> macOSLinuxWindows <pre><code>brew install git\n</code></pre> <pre><code>sudo apt-get install git  # Debian/Ubuntu\nsudo yum install git      # RHEL/CentOS\n</code></pre> <p>Download from git-scm.com</p>"},{"location":"prerequisites/#optional-tools","title":"Optional Tools","text":""},{"location":"prerequisites/#github-cli-gh","title":"GitHub CLI (gh)","text":"<p>Useful for managing releases and workflows:</p> <pre><code># macOS\nbrew install gh\n\n# Linux\nsudo apt install gh\n\n# Windows\nwinget install GitHub.cli\n</code></pre>"},{"location":"prerequisites/#python-and-uv","title":"Python and uv","text":"<p>If you want to run the services locally or contribute to development:</p> <pre><code># Install uv (recommended Python package manager)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Install Python 3.9+\nuv python install 3.11\n</code></pre>"},{"location":"prerequisites/#aws-resource-requirements","title":"AWS Resource Requirements","text":"<p>Before deploying, ensure you have or will create:</p> <ol> <li> <p>S3 Bucket: For Terraform state storage</p> <ul> <li>Naming: <code>tf-state-lablink-allocator-bucket</code> (configurable)</li> <li>Versioning enabled recommended</li> </ul> </li> <li> <p>Elastic IPs: Pre-allocated for each environment</p> <ul> <li>1 for dev</li> <li>1 for test</li> <li>1 for prod</li> </ul> </li> <li> <p>IAM Roles: For OIDC authentication (GitHub Actions)</p> <ul> <li>Trust relationship with GitHub</li> <li>Permissions for EC2, S3, Route53</li> </ul> </li> <li> <p>Route 53 Hosted Zone (Optional): For custom DNS</p> <ul> <li>Example: <code>lablink.yourdomain.com</code></li> </ul> </li> </ol> <p>See the AWS Setup from Scratch guide for detailed setup instructions.</p>"},{"location":"prerequisites/#ssh-key-pair","title":"SSH Key Pair","text":"<p>LabLink automatically generates SSH key pairs via Terraform, but you should be familiar with:</p> <ul> <li>SSH key management</li> <li>File permissions (chmod 600)</li> <li>Connecting to EC2 instances via SSH</li> </ul>"},{"location":"prerequisites/#next-steps","title":"Next Steps","text":"<p>Once you have these prerequisites installed:</p> <ol> <li>Installation: Set up LabLink locally</li> <li>AWS Setup: Configure AWS resources from scratch</li> <li>Configuration: Customize LabLink for your needs</li> </ol>"},{"location":"quickstart/","title":"Quickstart","text":"<p>Get LabLink running in 15 minutes.</p> <p>Recommended: GitHub Actions Deployment</p> <p>For production deployments, we recommend using GitHub Actions. See the Deployment Guide for the full workflow with automated CI/CD.</p> <p>This quickstart covers local deployment for testing and development.</p>"},{"location":"quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>AWS Account with admin access (setup guide)</li> <li>AWS CLI configured locally (setup guide)</li> <li>Terraform installed (setup guide)</li> <li>S3 Bucket for Terraform state (setup guide)</li> </ul>"},{"location":"quickstart/#step-1-create-your-repository","title":"Step 1: Create Your Repository","text":"<p>Click the \"Use this template\" button on the lablink-template repository to create your own deployment repository.</p> <p>Then clone your new repository:</p> <pre><code>git clone https://github.com/YOUR_ORG/YOUR_REPO.git\ncd YOUR_REPO/lablink-infrastructure\n</code></pre>"},{"location":"quickstart/#step-2-configure-settings","title":"Step 2: Configure Settings","text":"<p>Edit <code>config/config.yaml</code>:</p> <pre><code># Minimal configuration for quick start\ndns:\n  enabled: false  # Start without DNS, use IP address\n\nssl:\n  provider: \"none\"  # Start without HTTPS\n\nmachine:\n  ami_id: \"ami-067cc81f948e50e06\"  # Ubuntu 22.04 LTS (us-west-2)\n  machine_type: \"t3.medium\"\n  gpu_support: false\n</code></pre> <p>Note on SSL: This configuration uses <code>provider: \"none\"</code> for simplicity. For testing with DNS, you can use: <pre><code>ssl:\n  provider: \"letsencrypt\"\n  email: \"your-email@example.com\"\n  staging: true  # HTTP only, unlimited deployments\n</code></pre></p> <p>Staging mode serves HTTP only. Your browser will show \"Not Secure\" - this is expected for testing. For production with HTTPS, set <code>staging: false</code>. See Configuration - SSL Options.</p> <p>Set passwords in <code>config/config.yaml</code>:</p> <pre><code>app:\n  admin_password: \"your-secure-admin-password\"  # Replace placeholder\n\ndb:\n  password: \"your-secure-db-password\"  # Replace placeholder\n</code></pre> <p>For GitHub Actions Deployment</p> <p>When using GitHub Actions, set <code>ADMIN_PASSWORD</code> and <code>DB_PASSWORD</code> as repository secrets instead of editing the config file directly. See AWS Setup - Add GitHub Secrets.</p>"},{"location":"quickstart/#step-3-initialize-and-deploy","title":"Step 3: Initialize and Deploy","text":"<pre><code># Initialize Terraform with backend configuration\n../scripts/init-terraform.sh test\n\n# Deploy (will prompt for confirmation)\nterraform apply -var=\"resource_suffix=test\"\n</code></pre> <p>Deployment time: ~5 minutes</p> <p>Creates: - EC2 instance running allocator (Flask app + PostgreSQL) - Security groups (HTTP port 80, SSH port 22) - SSH key pair</p>"},{"location":"quickstart/#step-4-access-your-allocator","title":"Step 4: Access Your Allocator","text":"<pre><code># Get public IP\nterraform output ec2_public_ip\n# Output: 52.10.123.456\n\n# Save SSH key\nterraform output -raw private_key_pem &gt; ~/lablink-key.pem\nchmod 600 ~/lablink-key.pem\n</code></pre> <p>Web interface: <code>http://&lt;ec2_public_ip&gt;</code></p> <p>Admin login:</p> <ul> <li>Username: <code>admin</code></li> <li>Password: The password you set in Step 2</li> </ul>"},{"location":"quickstart/#step-5-create-client-vms","title":"Step 5: Create Client VMs","text":"<ol> <li>Navigate to <code>http://&lt;ec2_public_ip&gt;/admin</code></li> <li>Log in with admin credentials</li> <li>Click \"Create VMs\"</li> <li>Enter number of VMs (try 2)</li> <li>Click \"Launch VMs\"</li> </ol> <p>VM creation time: ~5 minutes</p>"},{"location":"quickstart/#step-6-verify","title":"Step 6: Verify","text":"<pre><code># SSH into allocator\nssh -i ~/lablink-key.pem ubuntu@&lt;ec2_public_ip&gt;\n\n# Check allocator is running\nsudo docker ps\n\n# Check VMs in database\nsudo docker exec $(sudo docker ps -q) psql -U lablink -d lablink_db -c \"SELECT hostname FROM vms;\"\n</code></pre>"},{"location":"quickstart/#step-7-cleanup","title":"Step 7: Cleanup","text":"<pre><code># Destroy all resources\nterraform destroy -var=\"resource_suffix=test\"\n</code></pre> <p>AWS Costs</p> <p>EC2 instances cost ~$0.04/hour (t3.medium). Always destroy test resources.</p>"},{"location":"quickstart/#troubleshooting","title":"Troubleshooting","text":"<p>Can't access web interface? <pre><code>curl http://$(terraform output -raw ec2_public_ip)\n# If this fails, check security group allows port 80\n</code></pre></p> <p>VMs not appearing? See VM Registration Issue</p> <p>SSH permission denied? <pre><code>chmod 600 ~/lablink-key.pem\n</code></pre></p>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>GitHub Actions: Deployment Guide for automated CI/CD deployments (recommended for production)</li> <li>AWS Setup: AWS Setup Guide for IAM roles, OIDC, and GitHub secrets</li> <li>Add DNS: DNS Configuration Guide for custom domains and HTTPS</li> <li>Customize: Configuration Reference for all options</li> <li>Secure: Security Guide before going to production</li> </ul>"},{"location":"security/","title":"Security","text":"<p>This guide covers security considerations, best practices, and how to secure your LabLink deployment.</p>"},{"location":"security/#security-overview","title":"Security Overview","text":"<p>LabLink implements multiple security layers:</p> <ul> <li>Authentication: Admin interface password protection</li> <li>Authorization: OIDC for GitHub Actions, IAM roles for AWS</li> <li>Encryption: HTTPS (optional), encrypted Terraform state</li> <li>Network: Security groups restrict access</li> <li>Secrets: Environment variables, AWS Secrets Manager</li> </ul>"},{"location":"security/#threat-model","title":"Threat Model","text":""},{"location":"security/#assets-to-protect","title":"Assets to Protect","text":"<ol> <li>Allocator Server: Controls infrastructure</li> <li>Client VMs: Run research workloads</li> <li>Database: Contains VM assignments and user data</li> <li>AWS Credentials: Access to cloud resources</li> <li>SSH Keys: Access to EC2 instances</li> <li>Admin Credentials: Access to allocator interface</li> </ol>"},{"location":"security/#potential-threats","title":"Potential Threats","text":"Threat Impact Mitigation Unauthorized admin access Full system control Strong passwords, HTTPS, IP restrictions AWS credential exposure Unauthorized infrastructure changes OIDC (no stored credentials), IAM policies SSH key leakage Direct server access Ephemeral keys, proper permissions (600) Database access Data exposure, manipulation Firewall rules, strong passwords Man-in-the-middle Credential theft, data interception HTTPS, VPC isolation Resource exhaustion Denial of service, high costs Billing alerts, resource limits"},{"location":"security/#authentication-authorization","title":"Authentication &amp; Authorization","text":""},{"location":"security/#change-default-passwords","title":"Change Default Passwords","text":"<p>Critical: Change default passwords before deployment!</p>"},{"location":"security/#allocator-admin-password","title":"Allocator Admin Password","text":"<p>Default: Configuration files use <code>PLACEHOLDER_ADMIN_PASSWORD</code> which must be replaced with a secure password.</p> <p>Method 1: GitHub Secrets (Recommended for CI/CD)</p> <p>For GitHub Actions deployments, add the <code>ADMIN_PASSWORD</code> secret to your repository:</p> <ol> <li>Go to repository Settings \u2192 Secrets and variables \u2192 Actions</li> <li>Click New repository secret</li> <li>Name: <code>ADMIN_PASSWORD</code></li> <li>Value: Your secure password</li> <li>Click Add secret</li> </ol> <p>The deployment workflow automatically injects this secret into configuration files before Terraform apply, preventing passwords from appearing in logs.</p> <p>Method 2: Manual configuration</p> <p>Edit <code>lablink-infrastructure/config/config.yaml</code>: <pre><code>app:\n  admin_user: \"admin\"\n  admin_password: \"YOUR_SECURE_PASSWORD_HERE\"\n</code></pre></p> <p>Method 3: Environment variable</p> <pre><code>export ADMIN_PASSWORD=\"your_secure_password\"\n\n# Docker\ndocker run -d \\\n  -e ADMIN_PASSWORD=\"your_secure_password\" \\\n  -p 5000:5000 \\\n  ghcr.io/talmolab/lablink-allocator-image:latest\n</code></pre> <p>Password requirements: - Minimum 12 characters - Mix of uppercase, lowercase, numbers, symbols - Not a dictionary word - Use a password manager</p>"},{"location":"security/#database-password","title":"Database Password","text":"<p>Default: Configuration files use <code>PLACEHOLDER_DB_PASSWORD</code> which must be replaced with a secure password.</p> <p>Method 1: GitHub Secrets (Recommended for CI/CD)</p> <p>For GitHub Actions deployments, add the <code>DB_PASSWORD</code> secret to your repository:</p> <ol> <li>Go to repository Settings \u2192 Secrets and variables \u2192 Actions</li> <li>Click New repository secret</li> <li>Name: <code>DB_PASSWORD</code></li> <li>Value: Your secure database password</li> <li>Click Add secret</li> </ol> <p>The deployment workflow automatically injects this secret into configuration files before Terraform apply, preventing passwords from appearing in logs.</p> <p>Method 2: Manual configuration</p> <p>Edit <code>lablink-infrastructure/config/config.yaml</code>: <pre><code>db:\n  user: \"lablink\"\n  password: \"YOUR_SECURE_DB_PASSWORD_HERE\"\n</code></pre></p> <p>Method 3: Environment variable</p> <pre><code>export DB_PASSWORD=\"your_secure_db_password\"\n</code></pre> <p>Method 4: AWS Secrets Manager (Advanced)</p> <pre><code># Store in Secrets Manager\naws secretsmanager create-secret \\\n  --name lablink/db-password \\\n  --secret-string \"your-secure-db-password\"\n\n# Retrieve in application\nimport boto3\nclient = boto3.client('secretsmanager', region_name='us-west-2')\nresponse = client.get_secret_value(SecretId='lablink/db-password')\ndb_password = response['SecretString']\n</code></pre>"},{"location":"security/#oidc-for-github-actions","title":"OIDC for GitHub Actions","text":"<p>OpenID Connect (OIDC) allows GitHub Actions to authenticate to AWS without storing credentials.</p>"},{"location":"security/#how-it-works","title":"How It Works","text":"<pre><code>1. GitHub Action requests token from GitHub OIDC provider\n2. GitHub issues short-lived token with repository info\n3. Action presents token to AWS STS\n4. AWS validates token against IAM role trust policy\n5. AWS issues temporary AWS credentials\n6. Action uses credentials for Terraform operations\n7. Credentials expire automatically\n</code></pre>"},{"location":"security/#benefits","title":"Benefits","text":"<ul> <li>No stored credentials: Nothing to leak or rotate</li> <li>Short-lived: Credentials expire quickly</li> <li>Scoped: Permissions limited to specific role</li> <li>Auditable: CloudTrail logs all API calls</li> </ul>"},{"location":"security/#trust-policy","title":"Trust Policy","text":"<p>The IAM role trust policy restricts which repositories can assume the role:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Federated\": \"arn:aws:iam::ACCOUNT_ID:oidc-provider/token.actions.githubusercontent.com\"\n      },\n      \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"token.actions.githubusercontent.com:aud\": \"sts.amazonaws.com\"\n        },\n        \"StringLike\": {\n          \"token.actions.githubusercontent.com:sub\": \"repo:talmolab/lablink:*\"\n        }\n      }\n    }\n  ]\n}\n</code></pre> <p>Key: <code>token.actions.githubusercontent.com:sub</code> restricts to specific repository.</p>"},{"location":"security/#setup","title":"Setup","text":"<p>See AWS Setup \u2192 OIDC Configuration.</p>"},{"location":"security/#iam-role-permissions","title":"IAM Role Permissions","text":"<p>Follow principle of least privilege.</p> <p>Minimal permissions for LabLink deployment:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:RunInstances\",\n        \"ec2:TerminateInstances\",\n        \"ec2:DescribeInstances\",\n        \"ec2:CreateSecurityGroup\",\n        \"ec2:DeleteSecurityGroup\",\n        \"ec2:AuthorizeSecurityGroupIngress\",\n        \"ec2:RevokeSecurityGroupIngress\",\n        \"ec2:CreateKeyPair\",\n        \"ec2:DeleteKeyPair\",\n        \"ec2:DescribeKeyPairs\",\n        \"ec2:AllocateAddress\",\n        \"ec2:AssociateAddress\",\n        \"ec2:DescribeAddresses\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n        \"s3:DeleteObject\",\n        \"s3:ListBucket\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::tf-state-lablink-*\",\n        \"arn:aws:s3:::tf-state-lablink-*/*\"\n      ]\n    }\n  ]\n}\n</code></pre> <p>Restrict by tags (advanced):</p> <pre><code>{\n  \"Effect\": \"Allow\",\n  \"Action\": \"ec2:*\",\n  \"Resource\": \"*\",\n  \"Condition\": {\n    \"StringEquals\": {\n      \"ec2:ResourceTag/Project\": \"lablink\"\n    }\n  }\n}\n</code></pre>"},{"location":"security/#network-security","title":"Network Security","text":""},{"location":"security/#security-groups","title":"Security Groups","text":"<p>LabLink creates security groups for allocator and client VMs.</p>"},{"location":"security/#allocator-security-group","title":"Allocator Security Group","text":"<p>Inbound Rules:</p> Port Protocol Source Purpose 80 TCP 0.0.0.0/0 HTTP web interface 22 TCP 0.0.0.0/0 SSH access 5432 TCP VPC CIDR PostgreSQL (internal) <p>Recommendations:</p> <ol> <li> <p>Restrict SSH: Change source from <code>0.0.0.0/0</code> to your IP:    <pre><code>YOUR_IP=$(curl -s ifconfig.me)\naws ec2 authorize-security-group-ingress \\\n  --group-id sg-xxxxx \\\n  --protocol tcp \\\n  --port 22 \\\n  --cidr $YOUR_IP/32\n</code></pre></p> </li> <li> <p>Enable HTTPS: Use port 443 instead of 80 with SSL certificate:    <pre><code># Install certbot on allocator\nsudo certbot --nginx -d lablink.yourdomain.com\n</code></pre></p> </li> <li> <p>Restrict HTTP: Limit to known client IPs if possible</p> </li> </ol>"},{"location":"security/#client-vm-security-group","title":"Client VM Security Group","text":"<p>Inbound Rules:</p> Port Protocol Source Purpose 22 TCP Your IP SSH access <p>Outbound Rules:</p> Port Protocol Destination Purpose All All 0.0.0.0/0 Internet access (packages, GitHub) <p>Recommendations:</p> <ol> <li>Restrict outbound: If possible, limit to specific destinations:</li> <li>Package repos (apt, pip)</li> <li>GitHub</li> <li> <p>Allocator IP</p> </li> <li> <p>VPC Endpoints: Use VPC endpoints for AWS services (S3, EC2) to avoid internet routing</p> </li> </ol>"},{"location":"security/#vpc-configuration","title":"VPC Configuration","text":"<p>For production, use a dedicated VPC:</p> <pre><code>resource \"aws_vpc\" \"lablink\" {\n  cidr_block           = \"10.0.0.0/16\"\n  enable_dns_hostnames = true\n  enable_dns_support   = true\n\n  tags = {\n    Name = \"lablink-vpc\"\n  }\n}\n\nresource \"aws_subnet\" \"public\" {\n  vpc_id                  = aws_vpc.lablink.id\n  cidr_block              = \"10.0.1.0/24\"\n  availability_zone       = \"us-west-2a\"\n  map_public_ip_on_launch = true\n\n  tags = {\n    Name = \"lablink-public-subnet\"\n  }\n}\n\nresource \"aws_subnet\" \"private\" {\n  vpc_id            = aws_vpc.lablink.id\n  cidr_block        = \"10.0.2.0/24\"\n  availability_zone = \"us-west-2a\"\n\n  tags = {\n    Name = \"lablink-private-subnet\"\n  }\n}\n</code></pre> <p>Benefits: - Isolation from other workloads - Custom network ACLs - VPC Flow Logs for monitoring</p>"},{"location":"security/#staging-mode-security","title":"Staging Mode Security","text":"<p>Warning: Staging mode (<code>ssl.staging: true</code>) serves unencrypted HTTP traffic. All data transmitted between users and the allocator is sent in plaintext.</p>"},{"location":"security/#data-exposed-in-staging-mode","title":"Data Exposed in Staging Mode","text":"<p>When using staging mode, the following information is transmitted unencrypted:</p> <ul> <li>Admin usernames and passwords</li> <li>Database credentials</li> <li>VM allocation requests</li> <li>Research data filenames and metadata</li> <li>SSH keys and access tokens</li> <li>All HTTP request/response data</li> </ul>"},{"location":"security/#when-staging-mode-is-acceptable","title":"When Staging Mode is Acceptable","text":"<p>Use staging mode only when:</p> <ul> <li>Testing in isolated VPCs with no internet access</li> <li>Accessing via VPN on private networks</li> <li>Local testing on development machines</li> <li>Short-term infrastructure testing (less than 1 hour)</li> <li>Automated CI/CD testing pipelines</li> <li>No sensitive data is involved</li> </ul>"},{"location":"security/#when-production-mode-is-required","title":"When Production Mode is Required","text":"<p>Use production mode (<code>ssl.staging: false</code>) for:</p> <ul> <li>Any internet-accessible deployment</li> <li>Handling sensitive research data</li> <li>Multi-user environments</li> <li>Long-running deployments</li> <li>Production or staging environments</li> <li>Compliance requirements (HIPAA, GDPR, etc.)</li> </ul>"},{"location":"security/#mitigations-for-staging-mode","title":"Mitigations for Staging Mode","text":"<p>If you must use staging mode with potentially sensitive data:</p> <ol> <li> <p>Restrict access to your IP only:    <pre><code># In Terraform security group\ningress {\n  from_port   = 80\n  to_port     = 80\n  protocol    = \"tcp\"\n  cidr_blocks = [\"YOUR_IP/32\"]\n}\n</code></pre></p> </li> <li> <p>Use a VPN - All access through VPN tunnel</p> </li> <li> <p>Deploy in private VPC - No internet gateway</p> </li> <li> <p>Time-limited - Switch to production mode as soon as testing is complete</p> </li> <li> <p>Monitor access - Check CloudWatch logs for unexpected connections</p> </li> </ol>"},{"location":"security/#switching-to-production-mode","title":"Switching to Production Mode","text":"<p>To switch a deployment from staging to production:</p> <ol> <li> <p>Update configuration:    <pre><code>ssl:\n  staging: false\n</code></pre></p> </li> <li> <p>Redeploy:    <pre><code>terraform apply\n</code></pre></p> </li> <li> <p>Wait for Let's Encrypt certificate (30-60 seconds)</p> </li> <li> <p>Access via HTTPS:    <pre><code>https://your-domain.com\n</code></pre></p> </li> <li> <p>Clear browser HSTS cache if you previously accessed via HTTP (see Troubleshooting)</p> </li> </ol>"},{"location":"security/#secrets-management","title":"Secrets Management","text":""},{"location":"security/#environment-variables","title":"Environment Variables","text":"<p>For development:</p> <pre><code>export DB_PASSWORD=\"secure_password\"\nexport ADMIN_PASSWORD=\"secure_admin_password\"\nexport AWS_ACCESS_KEY_ID=\"AKIA...\"\nexport AWS_SECRET_ACCESS_KEY=\"...\"\n</code></pre> <p>Pros: - Simple - No external dependencies</p> <p>Cons: - Visible in process list - Can leak in logs - Not encrypted at rest</p>"},{"location":"security/#aws-secrets-manager","title":"AWS Secrets Manager","text":"<p>For production:</p> <p>Store secrets: <pre><code>aws secretsmanager create-secret \\\n  --name lablink/config \\\n  --secret-string '{\n    \"db_password\": \"secure_db_password\",\n    \"admin_password\": \"secure_admin_password\"\n  }'\n</code></pre></p> <p>Retrieve in application: <pre><code>import boto3\nimport json\n\ndef get_secrets():\n    client = boto3.client('secretsmanager', region_name='us-west-2')\n    response = client.get_secret_value(SecretId='lablink/config')\n    secrets = json.loads(response['SecretString'])\n    return secrets\n\nsecrets = get_secrets()\ndb_password = secrets['db_password']\nadmin_password = secrets['admin_password']\n</code></pre></p> <p>Pros: - Encrypted at rest and in transit - Automatic rotation - Audit logs (CloudTrail) - Versioning</p> <p>Cons: - Additional cost ($0.40/secret/month) - Requires IAM permissions</p>"},{"location":"security/#github-secrets","title":"GitHub Secrets","text":"<p>For CI/CD workflows, GitHub Secrets provide secure password storage.</p> <p>Add secrets: 1. Go to repository Settings \u2192 Secrets and variables \u2192 Actions 2. Click New repository secret 3. Add both required secrets:    - Name: <code>ADMIN_PASSWORD</code>, Value: your secure admin password    - Name: <code>DB_PASSWORD</code>, Value: your secure database password 4. Click Add secret for each</p> <p>How it works:</p> <p>The deployment workflow automatically injects secrets into configuration files before Terraform runs:</p> <pre><code>- name: Inject Password Secrets\n  env:\n    ADMIN_PASSWORD: ${{ secrets.ADMIN_PASSWORD || 'CHANGEME_admin_password' }}\n    DB_PASSWORD: ${{ secrets.DB_PASSWORD || 'CHANGEME_db_password' }}\n  run: |\n    sed -i \"s/PLACEHOLDER_ADMIN_PASSWORD/${ADMIN_PASSWORD}/g\" \"$CONFIG_FILE\"\n    sed -i \"s/PLACEHOLDER_DB_PASSWORD/${DB_PASSWORD}/g\" \"$CONFIG_FILE\"\n</code></pre> <p>This replaces <code>PLACEHOLDER_ADMIN_PASSWORD</code> and <code>PLACEHOLDER_DB_PASSWORD</code> in config files with actual values from secrets, preventing passwords from appearing in Terraform logs.</p> <p>Pros: - Integrated with GitHub Actions - Encrypted at rest and in transit - Not visible in workflow logs - Prevents password exposure in Terraform apply output</p> <p>Cons: - Only available in workflows - Can't be read after creation</p>"},{"location":"security/#ssh-key-security","title":"SSH Key Security","text":""},{"location":"security/#key-generation","title":"Key Generation","text":"<p>Terraform generates SSH keys automatically:</p> <pre><code>resource \"tls_private_key\" \"lablink_key\" {\n  algorithm = \"RSA\"\n  rsa_bits  = 4096\n}\n\nresource \"aws_key_pair\" \"lablink_key_pair\" {\n  key_name   = \"lablink-${var.resource_suffix}-key\"\n  public_key = tls_private_key.lablink_key.public_key_openssh\n}\n</code></pre> <p>Good: - Unique key per environment - 4096-bit RSA (strong)</p> <p>Bad: - Stored in Terraform state (plaintext) - Artifacts expire (GitHub Actions)</p>"},{"location":"security/#key-permissions","title":"Key Permissions","text":"<p>Always set proper permissions:</p> <pre><code>chmod 600 ~/lablink-key.pem\n</code></pre> <p>Why: Prevents SSH from rejecting key: <pre><code>Permissions 0644 for 'lablink-key.pem' are too open.\nIt is required that your private key files are NOT accessible by others.\n</code></pre></p>"},{"location":"security/#key-rotation","title":"Key Rotation","text":"<p>Rotate keys regularly:</p> <pre><code># Destroy and recreate infrastructure\nterraform destroy -var=\"resource_suffix=dev\"\nterraform apply -var=\"resource_suffix=dev\"\n\n# New keys generated automatically\n</code></pre> <p>Frequency: Every 90 days for production</p>"},{"location":"security/#key-storage","title":"Key Storage","text":"<p>Never: - Commit keys to version control - Share keys via email/Slack - Store keys in cloud storage without encryption</p> <p>Instead: - Use SSH agent: <code>ssh-add ~/lablink-key.pem</code> - Store in password manager - Use AWS Systems Manager Session Manager (no SSH needed)</p>"},{"location":"security/#session-manager-alternative-to-ssh","title":"Session Manager (Alternative to SSH)","text":"<p>Use AWS Systems Manager for SSH-less access:</p> <pre><code># Install Session Manager plugin\n# macOS\nbrew install --cask session-manager-plugin\n\n# Start session\naws ssm start-session --target i-xxxxx\n</code></pre> <p>Benefits: - No SSH keys needed - Audit logs in CloudTrail - Fine-grained IAM control</p>"},{"location":"security/#data-encryption","title":"Data Encryption","text":""},{"location":"security/#encryption-at-rest","title":"Encryption at Rest","text":""},{"location":"security/#terraform-state","title":"Terraform State","text":"<p>S3 bucket encryption (AES-256): <pre><code>aws s3api put-bucket-encryption \\\n  --bucket tf-state-lablink \\\n  --server-side-encryption-configuration '{\n    \"Rules\": [{\n      \"ApplyServerSideEncryptionByDefault\": {\n        \"SSEAlgorithm\": \"AES256\"\n      }\n    }]\n  }'\n</code></pre></p>"},{"location":"security/#ebs-volumes","title":"EBS Volumes","text":"<p>Encrypt EC2 instance volumes:</p> <pre><code>resource \"aws_instance\" \"lablink_allocator\" {\n  ami           = var.ami_id\n  instance_type = \"t2.micro\"\n\n  root_block_device {\n    volume_size           = 30\n    volume_type           = \"gp3\"\n    encrypted             = true\n    delete_on_termination = true\n  }\n}\n</code></pre>"},{"location":"security/#database","title":"Database","text":"<p>For RDS (if using external database): <pre><code>resource \"aws_db_instance\" \"lablink\" {\n  storage_encrypted = true\n  kms_key_id        = aws_kms_key.lablink.arn\n}\n</code></pre></p>"},{"location":"security/#encryption-in-transit","title":"Encryption in Transit","text":""},{"location":"security/#https-for-allocator","title":"HTTPS for Allocator","text":"<p>Use Let's Encrypt certificate:</p> <pre><code># SSH into allocator\nssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\n\n# Install certbot\nsudo apt-get update\nsudo apt-get install -y certbot python3-certbot-nginx\n\n# Get certificate\nsudo certbot --nginx -d lablink.yourdomain.com --non-interactive --agree-tos -m your@email.com\n\n# Auto-renewal\nsudo systemctl enable certbot.timer\n</code></pre> <p>Update security group to allow port 443.</p>"},{"location":"security/#postgresql-ssl","title":"PostgreSQL SSL","text":"<p>Enable SSL for database connections:</p> <p><code>postgresql.conf</code>: <pre><code>ssl = on\nssl_cert_file = '/etc/ssl/certs/server.crt'\nssl_key_file = '/etc/ssl/private/server.key'\n</code></pre></p> <p>Client connection: <pre><code>import psycopg2\n\nconn = psycopg2.connect(\n    host=\"allocator-ip\",\n    database=\"lablink_db\",\n    user=\"lablink\",\n    password=\"password\",\n    sslmode=\"require\"  # Force SSL\n)\n</code></pre></p>"},{"location":"security/#monitoring-auditing","title":"Monitoring &amp; Auditing","text":""},{"location":"security/#cloudtrail","title":"CloudTrail","text":"<p>Enable CloudTrail for AWS API auditing:</p> <pre><code>aws cloudtrail create-trail \\\n  --name lablink-trail \\\n  --s3-bucket-name lablink-cloudtrail-logs\n\naws cloudtrail start-logging --name lablink-trail\n</code></pre> <p>Logs include: - EC2 instance launches/terminations - Security group changes - IAM role assumptions - S3 access</p>"},{"location":"security/#vpc-flow-logs","title":"VPC Flow Logs","text":"<p>Monitor network traffic:</p> <pre><code>aws ec2 create-flow-logs \\\n  --resource-type VPC \\\n  --resource-ids vpc-xxxxx \\\n  --traffic-type ALL \\\n  --log-destination-type cloud-watch-logs \\\n  --log-group-name lablink-vpc-flow-logs\n</code></pre>"},{"location":"security/#application-logging","title":"Application Logging","text":"<p>Log security events in application:</p> <pre><code>import logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Log authentication attempts\n@app.route('/admin')\n@requires_auth\ndef admin():\n    logger.info(f\"Admin access by {request.remote_addr}\")\n    return render_template('admin.html')\n\n# Log VM requests\n@app.route('/request_vm', methods=['POST'])\ndef request_vm():\n    logger.info(f\"VM requested by {request.form.get('email')} from {request.remote_addr}\")\n    # ... handle request\n</code></pre>"},{"location":"security/#compliance-best-practices","title":"Compliance &amp; Best Practices","text":""},{"location":"security/#security-checklist","title":"Security Checklist","text":"<ul> <li> Changed default admin password</li> <li> Changed default database password</li> <li> Enabled HTTPS for allocator</li> <li> Restricted SSH access to known IPs</li> <li> Enabled S3 bucket encryption</li> <li> Enabled EBS volume encryption</li> <li> Set up CloudTrail logging</li> <li> Set up billing alerts</li> <li> Rotated SSH keys (if older than 90 days)</li> <li> Reviewed IAM role permissions</li> <li> Enabled MFA for AWS account</li> <li> Set up VPC Flow Logs</li> <li> Documented security procedures</li> </ul>"},{"location":"security/#regular-security-tasks","title":"Regular Security Tasks","text":"Task Frequency Review CloudTrail logs Weekly Rotate SSH keys Every 90 days Update dependencies Monthly Review security group rules Quarterly Audit IAM permissions Quarterly Penetration testing Annually"},{"location":"security/#incident-response","title":"Incident Response","text":"<p>If security incident occurs:</p> <ol> <li>Isolate: Modify security groups to block traffic</li> <li>Investigate: Review CloudTrail, VPC Flow Logs, application logs</li> <li>Contain: Terminate compromised instances</li> <li>Recover: Deploy from known-good state</li> <li>Learn: Document incident, improve security</li> </ol>"},{"location":"security/#security-resources","title":"Security Resources","text":"<ul> <li>AWS Security Best Practices</li> <li>OWASP Top 10</li> <li>CIS AWS Foundations Benchmark</li> </ul>"},{"location":"security/#next-steps","title":"Next Steps","text":"<ul> <li>AWS Setup: Secure AWS resource configuration</li> <li>Deployment: Secure deployment practices</li> <li>Troubleshooting: Security-related issues</li> </ul>"},{"location":"ssh-access/","title":"SSH Access","text":"<p>This guide covers SSH access to LabLink allocator and client EC2 instances.</p>"},{"location":"ssh-access/#overview","title":"Overview","text":"<p>SSH (Secure Shell) provides secure remote access to EC2 instances for:</p> <ul> <li>Debugging and troubleshooting</li> <li>Log inspection</li> <li>Configuration changes</li> <li>Database access</li> <li>Manual operations</li> </ul>"},{"location":"ssh-access/#ssh-key-management","title":"SSH Key Management","text":""},{"location":"ssh-access/#key-generation","title":"Key Generation","text":"<p>Terraform automatically generates SSH key pairs during deployment:</p> <pre><code>resource \"tls_private_key\" \"lablink_key\" {\n  algorithm = \"RSA\"\n  rsa_bits  = 4096\n}\n\nresource \"aws_key_pair\" \"lablink_key_pair\" {\n  key_name   = \"lablink-${var.resource_suffix}-key\"\n  public_key = tls_private_key.lablink_key.public_key_openssh\n}\n</code></pre> <p>Key characteristics: - Algorithm: RSA - Key size: 4096 bits (strong security) - Format: OpenSSH - Naming: <code>lablink-&lt;environment&gt;-key</code> (e.g., <code>lablink-dev-key</code>, <code>lablink-prod-key</code>)</p>"},{"location":"ssh-access/#retrieving-ssh-keys","title":"Retrieving SSH Keys","text":""},{"location":"ssh-access/#from-terraform-output","title":"From Terraform Output","text":"<p>After deployment:</p> <pre><code>cd lablink-infrastructure\n\n# Display private key\nterraform output -raw private_key_pem\n\n# Save to file\nterraform output -raw private_key_pem &gt; ~/lablink-dev-key.pem\n\n# Set proper permissions\nchmod 600 ~/lablink-dev-key.pem\n</code></pre>"},{"location":"ssh-access/#from-github-actions-artifacts","title":"From GitHub Actions Artifacts","text":"<p>For deployments via GitHub Actions:</p> <ol> <li>Navigate to Actions tab in GitHub</li> <li>Click on the deployment workflow run</li> <li>Scroll to Artifacts section</li> <li>Download <code>lablink-key-&lt;env&gt;</code> artifact</li> <li>Extract <code>lablink-key.pem</code></li> <li>Set permissions: <code>chmod 600 ~/lablink-key.pem</code></li> </ol> <p>Artifact Expiration</p> <p>GitHub Actions artifacts expire after 1 day. Retrieve keys promptly or re-run deployment.</p>"},{"location":"ssh-access/#key-permissions","title":"Key Permissions","text":"<p>Required permissions: <code>600</code> (read/write for owner only)</p> <pre><code># Set correct permissions\nchmod 600 ~/lablink-key.pem\n\n# Verify\nls -l ~/lablink-key.pem\n# Should show: -rw------- (600)\n</code></pre> <p>Why: SSH refuses to use keys with overly permissive permissions: <pre><code>@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@         WARNING: UNPROTECTED PRIVATE KEY FILE!          @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nPermissions 0644 for 'lablink-key.pem' are too open.\n</code></pre></p>"},{"location":"ssh-access/#connecting-to-allocator","title":"Connecting to Allocator","text":""},{"location":"ssh-access/#get-allocator-ip-address","title":"Get Allocator IP Address","text":""},{"location":"ssh-access/#from-terraform-output_1","title":"From Terraform Output","text":"<pre><code>cd lablink-infrastructure\nterraform output ec2_public_ip\n</code></pre>"},{"location":"ssh-access/#from-aws-console","title":"From AWS Console","text":"<ol> <li>Navigate to EC2 \u2192 Instances</li> <li>Find instance tagged <code>lablink-allocator-&lt;env&gt;</code></li> <li>Copy Public IPv4 address</li> </ol>"},{"location":"ssh-access/#from-aws-cli","title":"From AWS CLI","text":"<pre><code>aws ec2 describe-instances \\\n  --filters \"Name=tag:Name,Values=lablink-allocator-dev\" \\\n  --query 'Reservations[0].Instances[0].PublicIpAddress' \\\n  --output text\n</code></pre>"},{"location":"ssh-access/#ssh-command","title":"SSH Command","text":"<pre><code>ssh -i ~/lablink-dev-key.pem ubuntu@&lt;allocator-public-ip&gt;\n</code></pre> <p>Example: <pre><code>ssh -i ~/lablink-dev-key.pem ubuntu@54.123.45.67\n</code></pre></p> <p>Default user: <code>ubuntu</code> (for Ubuntu AMI)</p>"},{"location":"ssh-access/#first-connection","title":"First Connection","text":"<p>On first SSH connection, you'll see:</p> <pre><code>The authenticity of host '54.123.45.67 (54.123.45.67)' can't be established.\nED25519 key fingerprint is SHA256:xxxxx.\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\n</code></pre> <p>Type <code>yes</code> and press Enter. This adds the host to <code>~/.ssh/known_hosts</code>.</p>"},{"location":"ssh-access/#successful-connection","title":"Successful Connection","text":"<p>You should see:</p> <pre><code>Welcome to Ubuntu 20.04.6 LTS (GNU/Linux 5.15.0-1023-aws x86_64)\n...\nubuntu@ip-xxx-xx-xx-xx:~$\n</code></pre>"},{"location":"ssh-access/#common-ssh-tasks","title":"Common SSH Tasks","text":""},{"location":"ssh-access/#inspect-docker-containers","title":"Inspect Docker Containers","text":"<pre><code># List running containers\nsudo docker ps\n\n# View allocator container logs\nsudo docker logs &lt;container-id&gt;\n\n# Follow logs in real-time\nsudo docker logs -f &lt;container-id&gt;\n</code></pre>"},{"location":"ssh-access/#access-allocator-container","title":"Access Allocator Container","text":"<pre><code># Get container ID\nCONTAINER_ID=$(sudo docker ps --filter \"ancestor=ghcr.io/talmolab/lablink-allocator-image\" --format \"{{.ID}}\")\n\n# Execute bash in container\nsudo docker exec -it $CONTAINER_ID bash\n</code></pre> <p>Inside container:</p> <pre><code># View configuration\ncat /app/config/config.yaml\n\n# Check Flask app\nps aux | grep flask\n\n# Access PostgreSQL\npsql -U lablink -d lablink_db\n</code></pre>"},{"location":"ssh-access/#restart-postgresql","title":"Restart PostgreSQL","text":"<p>Known issue: PostgreSQL may need restart after first boot:</p> <pre><code># SSH into allocator\nssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\n\n# Get container ID\nsudo docker ps\n\n# Access container\nsudo docker exec -it &lt;container-id&gt; bash\n\n# Inside container: restart PostgreSQL\n/etc/init.d/postgresql restart\n\n# Verify it's running\npg_isready -U lablink\n</code></pre>"},{"location":"ssh-access/#check-system-resources","title":"Check System Resources","text":"<pre><code># Disk usage\ndf -h\n\n# Memory usage\nfree -h\n\n# CPU usage\ntop\n\n# Running processes\nps aux\n\n# Network connections\nsudo netstat -tulpn\n</code></pre>"},{"location":"ssh-access/#view-logs","title":"View Logs","text":"<pre><code># System logs\nsudo journalctl -u docker\n\n# Docker container logs\nsudo docker logs &lt;container-id&gt;\n\n# PostgreSQL logs (inside container)\nsudo docker exec -it &lt;container-id&gt; tail -f /var/log/postgresql/postgresql-*.log\n\n# Application logs (if logging to file)\nsudo docker exec -it &lt;container-id&gt; tail -f /var/log/lablink/app.log\n</code></pre>"},{"location":"ssh-access/#transfer-files","title":"Transfer Files","text":""},{"location":"ssh-access/#from-local-to-ec2","title":"From Local to EC2","text":"<pre><code># Copy single file\nscp -i ~/lablink-key.pem local-file.txt ubuntu@&lt;allocator-ip&gt;:~/\n\n# Copy directory\nscp -i ~/lablink-key.pem -r local-dir/ ubuntu@&lt;allocator-ip&gt;:~/\n</code></pre>"},{"location":"ssh-access/#from-ec2-to-local","title":"From EC2 to Local","text":"<pre><code># Copy single file\nscp -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;:~/remote-file.txt ./\n\n# Copy directory\nscp -i ~/lablink-key.pem -r ubuntu@&lt;allocator-ip&gt;:~/remote-dir ./\n</code></pre>"},{"location":"ssh-access/#connecting-to-client-vms","title":"Connecting to Client VMs","text":""},{"location":"ssh-access/#get-client-vm-ip","title":"Get Client VM IP","text":"<pre><code># Via allocator database\nssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\nsudo docker exec -it &lt;container-id&gt; psql -U lablink -d lablink_db -c \"SELECT hostname, status FROM vms;\"\n\n# Via AWS CLI\naws ec2 describe-instances \\\n  --filters \"Name=tag:CreatedBy,Values=LabLink\" \\\n  --query 'Reservations[*].Instances[*].[InstanceId,PublicIpAddress,State.Name]' \\\n  --output table\n</code></pre>"},{"location":"ssh-access/#ssh-to-client-vm","title":"SSH to Client VM","text":"<p>Use the same key as allocator:</p> <pre><code>ssh -i ~/lablink-key.pem ubuntu@&lt;client-vm-ip&gt;\n</code></pre> <p>Note: If Terraform created clients, key might be different. Check Terraform outputs or client VM security settings.</p>"},{"location":"ssh-access/#common-client-vm-tasks","title":"Common Client VM Tasks","text":"<pre><code># Check Docker containers\nsudo docker ps\n\n# View client service logs\nsudo docker logs &lt;client-container-id&gt;\n\n# Check GPU availability\nnvidia-smi\n\n# Verify research repository cloned\nls -la /home/ubuntu/research-repo/\n\n# Check client service status\nsudo systemctl status lablink-client  # If using systemd\n</code></pre>"},{"location":"ssh-access/#ssh-configuration-file","title":"SSH Configuration File","text":"<p>For easier SSH access, create <code>~/.ssh/config</code>:</p> <pre><code># Allocator Dev\nHost lablink-dev\n    HostName 54.123.45.67\n    User ubuntu\n    IdentityFile ~/.ssh/lablink-dev-key.pem\n    StrictHostKeyChecking no\n    UserKnownHostsFile /dev/null\n\n# Allocator Test\nHost lablink-test\n    HostName 54.234.56.78\n    User ubuntu\n    IdentityFile ~/.ssh/lablink-test-key.pem\n\n# Allocator Prod\nHost lablink-prod\n    HostName 54.98.76.54\n    User ubuntu\n    IdentityFile ~/.ssh/lablink-prod-key.pem\n</code></pre> <p>Usage: <pre><code># Instead of:\nssh -i ~/lablink-dev-key.pem ubuntu@54.123.45.67\n\n# Simply:\nssh lablink-dev\n</code></pre></p>"},{"location":"ssh-access/#troubleshooting-ssh-issues","title":"Troubleshooting SSH Issues","text":""},{"location":"ssh-access/#permission-denied-publickey","title":"Permission Denied (publickey)","text":"<p>Error: <pre><code>Permission denied (publickey).\n</code></pre></p> <p>Causes &amp; Solutions:</p> <ol> <li> <p>Wrong key:    <pre><code># Verify key matches instance\nssh-keygen -lf ~/lablink-key.pem\n</code></pre></p> </li> <li> <p>Wrong permissions:    <pre><code>chmod 600 ~/lablink-key.pem\n</code></pre></p> </li> <li> <p>Wrong user:    <pre><code># Try 'ubuntu' or 'ec2-user'\nssh -i ~/lablink-key.pem ubuntu@&lt;ip&gt;\nssh -i ~/lablink-key.pem ec2-user@&lt;ip&gt;\n</code></pre></p> </li> <li> <p>Key not in authorized_keys:</p> </li> <li>Redeploy instance with correct key</li> </ol>"},{"location":"ssh-access/#connection-timeout","title":"Connection Timeout","text":"<p>Error: <pre><code>ssh: connect to host 54.123.45.67 port 22: Connection timed out\n</code></pre></p> <p>Causes &amp; Solutions:</p> <ol> <li>Security group doesn't allow SSH:    <pre><code>aws ec2 describe-security-groups \\\n  --group-ids sg-xxxxx \\\n  --query 'SecurityGroups[0].IpPermissions'\n</code></pre></li> </ol> <p>Add SSH rule:    <pre><code>aws ec2 authorize-security-group-ingress \\\n  --group-id sg-xxxxx \\\n  --protocol tcp \\\n  --port 22 \\\n  --cidr $(curl -s ifconfig.me)/32\n</code></pre></p> <ol> <li> <p>Instance not running:    <pre><code>aws ec2 describe-instances --instance-ids i-xxxxx \\\n  --query 'Reservations[0].Instances[0].State.Name'\n</code></pre></p> </li> <li> <p>Wrong IP address:</p> </li> <li>Verify IP from AWS console or Terraform output</li> </ol>"},{"location":"ssh-access/#host-key-verification-failed","title":"Host Key Verification Failed","text":"<p>Error: <pre><code>@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n</code></pre></p> <p>Cause: Instance was recreated with same IP but different host key.</p> <p>Solution: <pre><code># Remove old host key\nssh-keygen -R &lt;ip-address&gt;\n\n# Or remove entire known_hosts file (if safe)\nrm ~/.ssh/known_hosts\n</code></pre></p>"},{"location":"ssh-access/#too-many-authentication-failures","title":"Too Many Authentication Failures","text":"<p>Error: <pre><code>Received disconnect from 54.123.45.67: Too many authentication failures\n</code></pre></p> <p>Cause: SSH tried multiple keys before correct one.</p> <p>Solution: <pre><code># Specify only this key\nssh -o IdentitiesOnly=yes -i ~/lablink-key.pem ubuntu@&lt;ip&gt;\n</code></pre></p>"},{"location":"ssh-access/#alternative-access-methods","title":"Alternative Access Methods","text":""},{"location":"ssh-access/#aws-systems-manager-session-manager","title":"AWS Systems Manager Session Manager","text":"<p>No SSH keys needed:</p> <pre><code># Install Session Manager plugin\n# macOS\nbrew install --cask session-manager-plugin\n\n# Start session\naws ssm start-session --target i-xxxxx\n</code></pre> <p>Benefits: - No SSH keys to manage - Works even if security group blocks port 22 - Audit logs in CloudTrail - IAM-based access control</p> <p>Requirements: - SSM agent installed on instance (default for recent AMIs) - IAM role attached to instance with SSM permissions</p>"},{"location":"ssh-access/#ec2-instance-connect","title":"EC2 Instance Connect","text":"<p>Browser-based SSH (AWS Console):</p> <ol> <li>Navigate to EC2 \u2192 Instances</li> <li>Select instance</li> <li>Click Connect</li> <li>Choose EC2 Instance Connect</li> <li>Click Connect</li> </ol> <p>Limitations: - Only works for 60 seconds - Requires security group to allow port 22 from AWS IP ranges</p>"},{"location":"ssh-access/#serial-console","title":"Serial Console","text":"<p>For debugging boot issues:</p> <ol> <li>Navigate to EC2 \u2192 Instances</li> <li>Select instance</li> <li>Actions \u2192 Monitor and troubleshoot \u2192 EC2 Serial Console</li> </ol> <p>Note: Must be enabled in account settings.</p>"},{"location":"ssh-access/#security-best-practices","title":"Security Best Practices","text":"<ol> <li> <p>Restrict SSH access: Limit security group to your IP    <pre><code>YOUR_IP=$(curl -s ifconfig.me)\naws ec2 authorize-security-group-ingress \\\n  --group-id sg-xxxxx \\\n  --protocol tcp \\\n  --port 22 \\\n  --cidr $YOUR_IP/32\n</code></pre></p> </li> <li> <p>Use SSH agent: Avoid typing key path    <pre><code>ssh-add ~/lablink-key.pem\nssh ubuntu@&lt;ip&gt;  # No -i flag needed\n</code></pre></p> </li> <li> <p>Disable password authentication: Enforce key-based auth    <pre><code># In /etc/ssh/sshd_config\nPasswordAuthentication no\n</code></pre></p> </li> <li> <p>Use bastion host: For production, access via jump box    <pre><code>ssh -J bastion-user@bastion-ip ubuntu@private-ip\n</code></pre></p> </li> <li> <p>Rotate keys regularly: Every 90 days    <pre><code>terraform destroy &amp;&amp; terraform apply  # Generates new keys\n</code></pre></p> </li> <li> <p>Use Session Manager: Avoid SSH when possible</p> </li> </ol>"},{"location":"ssh-access/#next-steps","title":"Next Steps","text":"<ul> <li>Troubleshooting: Fix SSH and connectivity issues</li> <li>Security: Secure your SSH access</li> <li>Database Management: Access database via SSH</li> </ul>"},{"location":"ssh-access/#quick-reference","title":"Quick Reference","text":"<pre><code># Connect to allocator\nssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\n\n# Set key permissions\nchmod 600 ~/lablink-key.pem\n\n# Copy file to instance\nscp -i ~/lablink-key.pem file.txt ubuntu@&lt;ip&gt;:~/\n\n# Access container\nsudo docker exec -it &lt;container-id&gt; bash\n\n# View logs\nsudo docker logs -f &lt;container-id&gt;\n\n# Restart PostgreSQL\nsudo docker exec -it &lt;container-id&gt; /etc/init.d/postgresql restart\n</code></pre>"},{"location":"testing/","title":"Testing","text":"<p>This guide covers testing LabLink components, including unit tests, integration tests, and end-to-end testing.</p>"},{"location":"testing/#testing-overview","title":"Testing Overview","text":"<p>LabLink uses pytest for testing Python code. The testing strategy includes:</p> <ul> <li>Unit tests: Test individual functions and classes (mocked dependencies)</li> <li>Integration tests: Test component interactions</li> <li>End-to-end tests: Test full workflows from API to VM creation</li> <li>Infrastructure tests: Validate Terraform configurations</li> </ul>"},{"location":"testing/#continuous-integration-ci","title":"Continuous Integration (CI)","text":""},{"location":"testing/#ci-pipeline","title":"CI Pipeline","text":"<p>Workflow: <code>.github/workflows/ci.yml</code></p> <p>Triggers: - Pull requests to <code>main</code> - Pushes to <code>main</code> or development branches</p> <p>Steps: 1. Setup Python environment 2. Install dependencies 3. Run linting (ruff) 4. Run unit tests with pytest 5. Generate coverage reports</p> <p>CI runs: - Allocator service tests - Client service tests - Mock tests only (no AWS resources)</p>"},{"location":"testing/#view-ci-results","title":"View CI Results","text":"<ol> <li>Navigate to Actions tab in GitHub</li> <li>Click on CI workflow run</li> <li>View test results and coverage</li> </ol>"},{"location":"testing/#local-testing","title":"Local Testing","text":""},{"location":"testing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9+</li> <li><code>uv</code> package manager (recommended) or <code>pip</code></li> <li>Allocator and client service code</li> </ul>"},{"location":"testing/#setup-test-environment","title":"Setup Test Environment","text":""},{"location":"testing/#using-uv-recommended","title":"Using uv (Recommended)","text":"<pre><code># Install uv\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Navigate to service directory\ncd packages/allocator\n\n# Install dependencies including test deps\nuv sync --extra dev\n\n# Or for client\ncd ../client\nuv sync --extra dev\n</code></pre>"},{"location":"testing/#using-pip","title":"Using pip","text":"<pre><code>cd packages/allocator\n\n# Create virtual environment\npython3 -m venv venv\nsource venv/bin/activate  # Linux/macOS\n# venv\\Scripts\\activate   # Windows\n\n# Install dependencies\npip install -e \".[dev]\"\n</code></pre>"},{"location":"testing/#run-unit-tests","title":"Run Unit Tests","text":""},{"location":"testing/#allocator-service","title":"Allocator Service","text":"<pre><code>cd packages/allocator\n\n# Run all tests\nPYTHONPATH=. pytest\n\n# Run with verbose output\nPYTHONPATH=. pytest -v\n\n# Run specific test file\nPYTHONPATH=. pytest tests/test_api_calls.py\n\n# Run specific test\nPYTHONPATH=. pytest tests/test_api_calls.py::test_request_vm\n</code></pre>"},{"location":"testing/#client-service","title":"Client Service","text":"<pre><code>cd packages/client\n\n# Run all tests\nPYTHONPATH=. pytest\n\n# Run specific tests\nPYTHONPATH=. pytest tests/test_check_gpu.py\nPYTHONPATH=. pytest tests/test_subscribe.py\n</code></pre>"},{"location":"testing/#run-with-coverage","title":"Run with Coverage","text":"<pre><code># Generate coverage report\nPYTHONPATH=. pytest --cov=lablink_allocator --cov-report=html\n\n# View coverage report\nopen htmlcov/index.html  # macOS\nxdg-open htmlcov/index.html  # Linux\nstart htmlcov/index.html  # Windows\n</code></pre>"},{"location":"testing/#run-linting","title":"Run Linting","text":"<pre><code># Check code quality with ruff\nruff check .\n\n# Auto-fix issues\nruff check --fix .\n\n# Format code\nruff format .\n</code></pre>"},{"location":"testing/#test-structure","title":"Test Structure","text":""},{"location":"testing/#allocator-service-tests","title":"Allocator Service Tests","text":"<p>Located in <code>packages/allocator/tests/</code>:</p> Test File Purpose <code>test_api_calls.py</code> Test Flask API endpoints <code>test_admin_auth.py</code> Test admin authentication <code>test_pages.py</code> Test web page rendering <code>test_terraform_api.py</code> Test Terraform integration <code>utils/test_aws_utils.py</code> Test AWS utility functions <code>utils/test_terraform_utils.py</code> Test Terraform utilities <code>utils/test_scp.py</code> Test SCP file transfer"},{"location":"testing/#client-service-tests","title":"Client Service Tests","text":"<p>Located in <code>packages/client/tests/</code>:</p> Test File Purpose <code>test_check_gpu.py</code> Test GPU health check <code>test_subscribe.py</code> Test allocator subscription <code>test_update_inuse.py</code> Test status updates <code>test_connect_crd.py</code> Test CRD command execution <code>test_imports.py</code> Test module imports"},{"location":"testing/#terraform-tests","title":"Terraform Tests","text":"<p>Located in <code>packages/allocator/tests/terraform/tests/</code>:</p> Test File Purpose <code>test_plan.py</code> Test Terraform plan validation <p>The Terraform tests use a local backend for testing purposes, which is configured by passing <code>-backend=false</code> to the <code>terraform init</code> command. This avoids the need for a configured S3 backend during testing.</p>"},{"location":"testing/#feature-testing","title":"Feature Testing","text":"<p>LabLink has two key features that require integration testing:</p>"},{"location":"testing/#feature-1-update-in-use-status","title":"Feature 1: Update In-Use Status","text":"<p>Purpose: Client VMs report their status to the allocator.</p> <p>Test Setup:</p> <ol> <li>Deploy LabLink allocator</li> <li>Create client VMs via allocator</li> <li>Verify VMs register with allocator</li> <li>Check status updates in allocator database</li> </ol> <p>Manual Test:</p> <pre><code># Deploy allocator\ncd lablink-infrastructure\nterraform apply -var=\"resource_suffix=test\"\n\n# Get allocator IP\nALLOCATOR_IP=$(terraform output -raw ec2_public_ip)\n\n# Create client VMs via web interface\nopen http://$ALLOCATOR_IP:80\n\n# Check VM status\nssh -i ~/lablink-key.pem ubuntu@$ALLOCATOR_IP\nsudo docker exec &lt;container-id&gt; psql -U lablink -d lablink_db -c \"SELECT hostname, status, updated_at FROM vms;\"\n\n# Verify status changes over time\nwatch -n 5 'sudo docker exec &lt;container-id&gt; psql -U lablink -d lablink_db -c \"SELECT hostname, status, updated_at FROM vms;\"'\n</code></pre> <p>Automated Test:</p> <p>See <code>.github/workflows/client-vm-infrastructure-test.yml</code></p>"},{"location":"testing/#feature-2-gpu-health-check","title":"Feature 2: GPU Health Check","text":"<p>Purpose: Client VMs automatically check GPU health every 20 seconds and report to allocator.</p> <p>Test Setup:</p> <ol> <li>Create client VM with GPU instance type (e.g., <code>g4dn.xlarge</code>)</li> <li>Monitor GPU health checks in client logs</li> <li>Verify status updates in allocator</li> </ol> <p>Manual Test:</p> <pre><code># SSH into client VM\nssh -i ~/lablink-key.pem ubuntu@&lt;client-vm-ip&gt;\n\n# Check GPU availability\nnvidia-smi\n\n# View client service logs\nsudo docker logs -f &lt;client-container-id&gt;\n\n# Look for GPU health check messages:\n# \"GPU health: OK\" or \"GPU health: FAILED\"\n\n# Check allocator database\nssh -i ~/lablink-key.pem ubuntu@$ALLOCATOR_IP\nsudo docker exec &lt;container-id&gt; psql -U lablink -d lablink_db -c \"SELECT hostname, status FROM vms WHERE hostname='&lt;client-hostname&gt;';\"\n</code></pre> <p>Expected Behavior:</p> <ul> <li>Client checks GPU every 20 seconds</li> <li>If GPU fails, status changes to \"failed\"</li> <li>Allocator UI shows failed status</li> </ul>"},{"location":"testing/#end-to-end-testing","title":"End-to-End Testing","text":""},{"location":"testing/#full-workflow-test","title":"Full Workflow Test","text":"<p>Test complete VM allocation workflow:</p> <pre><code># 1. Deploy allocator\ncd lablink-infrastructure\nterraform apply -var=\"resource_suffix=e2e-test\"\nALLOCATOR_IP=$(terraform output -raw ec2_public_ip)\n\n# 2. Request VM via API\ncurl -X POST http://$ALLOCATOR_IP:80/request_vm \\\n  -d \"email=test@example.com\" \\\n  -d \"crd_command=echo 'test'\"\n\n# 3. Verify VM assigned\nssh -i ~/lablink-key.pem ubuntu@$ALLOCATOR_IP \\\n  'sudo docker exec $(sudo docker ps -q) psql -U lablink -d lablink_db -c \"SELECT * FROM vms WHERE email=\\\"test@example.com\\\";\"'\n\n# 4. Create client VMs\ncurl -X POST http://$ALLOCATOR_IP:80/admin/create \\\n  -u admin:IwanttoSLEAP \\\n  -d \"instance_count=2\"\n\n# 5. Wait for VMs to be created\nsleep 300\n\n# 6. Verify VMs exist\naws ec2 describe-instances --filters \"Name=tag:CreatedBy,Values=LabLink\" \\\n  --query 'Reservations[*].Instances[*].[InstanceId,State.Name,PublicIpAddress]' \\\n  --output table\n\n# 7. Check VMs registered with allocator\nssh -i ~/lablink-key.pem ubuntu@$ALLOCATOR_IP \\\n  'sudo docker exec $(sudo docker ps -q) psql -U lablink -d lablink_db -c \"SELECT hostname, status FROM vms;\"'\n\n# 8. Cleanup\nterraform destroy -var=\"resource_suffix=e2e-test\" -auto-approve\n</code></pre>"},{"location":"testing/#infrastructure-test-ci","title":"Infrastructure Test (CI)","text":"<p>Workflow: <code>.github/workflows/client-vm-infrastructure-test.yml</code></p> <p>This workflow performs end-to-end testing of client VM creation:</p> <ol> <li>Deploys allocator to test environment</li> <li>Triggers client VM creation</li> <li>Waits for VM to become ready</li> <li>Verifies VM registration</li> <li>Checks health status</li> <li>Cleans up resources</li> </ol> <p>Run Manually:</p> <ol> <li>Navigate to Actions tab</li> <li>Select \"Client VM Infrastructure Test\"</li> <li>Click \"Run workflow\"</li> <li>Monitor progress</li> </ol>"},{"location":"testing/#mocking-for-tests","title":"Mocking for Tests","text":""},{"location":"testing/#mock-aws-services","title":"Mock AWS Services","text":"<p>Use <code>moto</code> for mocking AWS:</p> <pre><code>import boto3\nfrom moto import mock_ec2, mock_s3\n\n@mock_ec2\ndef test_create_instance():\n    ec2 = boto3.client('ec2', region_name='us-west-2')\n\n    # This creates a mock instance\n    response = ec2.run_instances(ImageId='ami-12345', MinCount=1, MaxCount=1)\n\n    assert len(response['Instances']) == 1\n</code></pre>"},{"location":"testing/#mock-database","title":"Mock Database","text":"<p>Use <code>pytest</code> fixtures for database mocking:</p> <pre><code>import pytest\nfrom unittest.mock import MagicMock\n\n@pytest.fixture\ndef mock_db():\n    \"\"\"Mock database connection.\"\"\"\n    db = MagicMock()\n    db.execute.return_value = [{'hostname': 'i-12345', 'status': 'available'}]\n    return db\n\ndef test_get_available_vm(mock_db):\n    result = get_available_vm(mock_db)\n    assert result['hostname'] == 'i-12345'\n</code></pre>"},{"location":"testing/#mock-external-apis","title":"Mock External APIs","text":"<p>Mock HTTP requests with <code>responses</code>:</p> <pre><code>import responses\nimport requests\n\n@responses.activate\ndef test_allocator_api():\n    responses.add(\n        responses.POST,\n        'http://allocator:80/request_vm',\n        json={'hostname': 'i-12345', 'status': 'assigned'},\n        status=200\n    )\n\n    resp = requests.post('http://allocator:80/request_vm', data={'email': 'test@example.com'})\n    assert resp.json()['hostname'] == 'i-12345'\n</code></pre>"},{"location":"testing/#performance-testing","title":"Performance Testing","text":""},{"location":"testing/#load-testing","title":"Load Testing","text":"<p>Test allocator under load with <code>locust</code>:</p> <p><code>locustfile.py</code>: <pre><code>from locust import HttpUser, task, between\n\nclass LablinkUser(HttpUser):\n    wait_time = between(1, 3)\n\n    @task\n    def request_vm(self):\n        self.client.post(\"/request_vm\", data={\n            \"email\": \"load-test@example.com\",\n            \"crd_command\": \"echo test\"\n        })\n\n    @task(2)\n    def view_instances(self):\n        self.client.get(\"/admin/instances\", auth=('admin', 'IwanttoSLEAP'))\n</code></pre></p> <p>Run load test: <pre><code>pip install locust\nlocust -f locustfile.py --host http://&lt;allocator-ip&gt;:80\n</code></pre></p> <p>Open http://localhost:8089 to configure and start load test.</p>"},{"location":"testing/#test-best-practices","title":"Test Best Practices","text":"<ol> <li> <p>Run tests before committing:    <pre><code>pytest &amp;&amp; git commit\n</code></pre></p> </li> <li> <p>Write tests for new features:</p> </li> <li>Add test file in <code>tests/</code></li> <li>Test happy path and error cases</li> <li> <p>Use mocks to avoid external dependencies</p> </li> <li> <p>Keep tests fast:</p> </li> <li>Use mocks for external services</li> <li>Avoid time.sleep() when possible</li> <li> <p>Run integration tests separately</p> </li> <li> <p>Use descriptive test names:    <pre><code>def test_request_vm_returns_available_vm():\n    ...\n\ndef test_request_vm_returns_error_when_no_vms_available():\n    ...\n</code></pre></p> </li> <li> <p>Test edge cases:</p> </li> <li>Empty inputs</li> <li>Invalid credentials</li> <li>Network failures</li> <li>Resource exhaustion</li> </ol>"},{"location":"testing/#troubleshooting-tests","title":"Troubleshooting Tests","text":""},{"location":"testing/#tests-fail-locally-but-pass-in-ci","title":"Tests Fail Locally But Pass in CI","text":"<p>Possible causes: - Different Python versions - Missing dependencies - Environment variables not set</p> <p>Solution: <pre><code># Match CI Python version\nuv python install 3.11\n\n# Use same dependencies as CI\nuv sync\n\n# Set required environment variables\nexport DB_PASSWORD=test\nexport ADMIN_PASSWORD=test\n</code></pre></p>"},{"location":"testing/#import-errors","title":"Import Errors","text":"<p>Error: <code>ModuleNotFoundError: No module named 'lablink_allocator'</code></p> <p>Solution: <pre><code># Set PYTHONPATH\nexport PYTHONPATH=.\npytest\n\n# Or install package in editable mode\npip install -e .\npytest\n</code></pre></p>"},{"location":"testing/#database-connection-errors","title":"Database Connection Errors","text":"<p>Error: <code>psycopg2.OperationalError: could not connect to server</code></p> <p>Solution: - Tests should use mocked database - Check for hardcoded connection strings - Use fixtures for database access</p>"},{"location":"testing/#next-steps","title":"Next Steps","text":"<ul> <li>CI/CD Workflows: Understand automated testing</li> <li>Troubleshooting: Debug test failures</li> <li>Contributing: Add new tests</li> </ul>"},{"location":"testing/#quick-reference","title":"Quick Reference","text":"<pre><code># Run all tests\nPYTHONPATH=. pytest\n\n# Run with coverage\nPYTHONPATH=. pytest --cov\n\n# Run specific test\nPYTHONPATH=. pytest tests/test_api_calls.py::test_request_vm\n\n# Run linting\nruff check .\n\n# Format code\nruff format .\n\n# Watch for changes and re-run tests\nPYTHONPATH=. pytest-watch\n</code></pre>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>This guide covers common issues and their solutions when deploying and operating LabLink.</p>"},{"location":"troubleshooting/#general-troubleshooting-workflow","title":"General Troubleshooting Workflow","text":"<p>When encountering issues, follow this systematic approach:</p> <ol> <li>Initialize Terraform state locally (if not already done)</li> <li>Plan terraform resources using AWS CLI</li> <li>Apply changes</li> <li>Retrieve PEM key for SSH access</li> <li>Check admin website or SSH into EC2 instance</li> <li>Review logs for errors</li> <li>Destroy resources when done troubleshooting</li> </ol>"},{"location":"troubleshooting/#common-issues","title":"Common Issues","text":""},{"location":"troubleshooting/#docker-and-installation","title":"Docker and Installation","text":""},{"location":"troubleshooting/#docker-permission-error","title":"Docker Permission Error","text":"<p>Error: <pre><code>Got permission denied while trying to connect to the Docker daemon socket\n</code></pre></p> <p>Solution: <pre><code># Add user to docker group\nsudo usermod -aG docker $USER\n\n# Apply group changes\nnewgrp docker\n\n# Verify\ndocker ps\n</code></pre></p>"},{"location":"troubleshooting/#docker-daemon-not-running","title":"Docker Daemon Not Running","text":"<p>Error: <pre><code>Cannot connect to the Docker daemon at unix:///var/run/docker.sock\n</code></pre></p> <p>Solution: <pre><code># Start Docker (Linux)\nsudo systemctl start docker\nsudo systemctl enable docker\n\n# macOS/Windows\n# Start Docker Desktop application\n</code></pre></p>"},{"location":"troubleshooting/#ssh-access-issues","title":"SSH Access Issues","text":""},{"location":"troubleshooting/#permission-denied-publickey","title":"Permission Denied (publickey)","text":"<p>Error: <pre><code>Permission denied (publickey).\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Check key permissions:    <pre><code>chmod 600 ~/lablink-key.pem\nls -l ~/lablink-key.pem\n# Should show: -rw-------\n</code></pre></p> </li> <li> <p>Verify correct key:    <pre><code># Extract key from Terraform\ncd lablink-infrastructure\nterraform output -raw private_key_pem &gt; ~/lablink-key.pem\nchmod 600 ~/lablink-key.pem\n</code></pre></p> </li> <li> <p>Check correct user:    <pre><code># Try ubuntu user\nssh -i ~/lablink-key.pem ubuntu@&lt;ip&gt;\n\n# Or ec2-user\nssh -i ~/lablink-key.pem ec2-user@&lt;ip&gt;\n</code></pre></p> </li> <li> <p>Verify security group allows SSH:    <pre><code>aws ec2 describe-security-groups --group-ids &lt;sg-id&gt; \\\n  --query 'SecurityGroups[0].IpPermissions[?FromPort==`22`]'\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#ssh-connection-timeout","title":"SSH Connection Timeout","text":"<p>Error: <pre><code>ssh: connect to host X.X.X.X port 22: Connection timed out\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Verify security group allows port 22:    <pre><code>aws ec2 authorize-security-group-ingress \\\n  --group-id &lt;sg-id&gt; \\\n  --protocol tcp \\\n  --port 22 \\\n  --cidr 0.0.0.0/0\n</code></pre></p> </li> <li> <p>Check instance is running:    <pre><code>aws ec2 describe-instances --instance-ids &lt;instance-id&gt; \\\n  --query 'Reservations[0].Instances[0].State.Name'\n</code></pre></p> </li> <li> <p>Verify correct public IP:    <pre><code>terraform output ec2_public_ip\n</code></pre></p> </li> <li> <p>Check network ACLs:</p> </li> <li>Verify VPC network ACLs allow inbound/outbound on port 22</li> </ol>"},{"location":"troubleshooting/#flask-server-problems","title":"Flask Server Problems","text":""},{"location":"troubleshooting/#cannot-access-web-interface","title":"Cannot Access Web Interface","text":"<p>Error: Browser shows \"Connection refused\" or \"Cannot connect\"</p> <p>Solutions:</p> <ol> <li> <p>Check if container is running:    <pre><code>ssh -i ~/lablink-key.pem ubuntu@&lt;ip&gt;\nsudo docker ps\n</code></pre></p> </li> <li> <p>View container logs:    <pre><code>sudo docker logs &lt;container-id&gt;\n\n# Follow logs in real-time\nsudo docker logs -f &lt;container-id&gt;\n</code></pre></p> </li> <li> <p>Test locally from instance:    <pre><code># From within the EC2 instance\ncurl localhost:80\n</code></pre></p> </li> <li> <p>Test connectivity from outside:    <pre><code># From your local machine\nnc -vz &lt;ec2-public-ip&gt; 80\n\n# Or\ncurl http://&lt;ec2-public-ip&gt;:80\n</code></pre></p> </li> <li> <p>Check security group allows port 80:    <pre><code>aws ec2 authorize-security-group-ingress \\\n  --group-id &lt;sg-id&gt; \\\n  --protocol tcp \\\n  --port 80 \\\n  --cidr 0.0.0.0/0\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#browser-cannot-access-http-staging-mode","title":"Browser Cannot Access HTTP (Staging Mode)","text":"<p>Symptoms: - Browser cannot connect to <code>http://your-domain.com</code> when using staging mode - \"This site can't be reached\" - \"Connection refused\" - \"ERR_CONNECTION_REFUSED\"</p> <p>Cause: Your browser previously accessed the site via HTTPS and cached the HSTS (HTTP Strict Transport Security) policy. This forces all future requests to automatically upgrade to HTTPS. Since staging mode only serves HTTP (port 443 is closed), the browser cannot connect.</p> <p>Solution - Clear HSTS Cache:</p> <p>Chrome / Edge:</p> <ol> <li> <p>Open a new tab and navigate to:    <pre><code>chrome://net-internals/#hsts\n</code></pre>    (For Edge use: <code>edge://net-internals/#hsts</code>)</p> </li> <li> <p>Scroll down to \"Delete domain security policies\"</p> </li> <li> <p>Enter your full domain name:    <pre><code>test.lablink.sleap.ai\n</code></pre></p> </li> <li> <p>Click \"Delete\"</p> </li> <li> <p>Access the site again, explicitly typing <code>http://</code>:    <pre><code>http://test.lablink.sleap.ai\n</code></pre></p> </li> </ol> <p>Firefox:</p> <ol> <li> <p>Close all Firefox windows</p> </li> <li> <p>Navigate to your Firefox profile directory:</p> </li> <li>Windows: <code>%APPDATA%\\Mozilla\\Firefox\\Profiles\\</code></li> <li>macOS: <code>~/Library/Application Support/Firefox/Profiles/</code></li> <li> <p>Linux: <code>~/.mozilla/firefox/</code></p> </li> <li> <p>Find your profile folder (e.g., <code>abc123.default-release</code>)</p> </li> <li> <p>Delete the file: <code>SiteSecurityServiceState.txt</code></p> </li> <li> <p>Restart Firefox and access with <code>http://</code>:    <pre><code>http://test.lablink.sleap.ai\n</code></pre></p> </li> </ol> <p>Safari:</p> <ol> <li> <p>Close Safari completely</p> </li> <li> <p>Open Terminal and run:    <pre><code>rm ~/Library/Cookies/HSTS.plist\n</code></pre></p> </li> <li> <p>Restart Safari and access with <code>http://</code>:    <pre><code>http://test.lablink.sleap.ai\n</code></pre></p> </li> </ol> <p>Quick Workarounds:</p> <p>If you don't want to clear HSTS cache:</p> <ol> <li>Use Incognito/Private Browsing</li> <li>HSTS cache doesn't apply in incognito mode</li> <li> <p>Access <code>http://test.lablink.sleap.ai</code></p> </li> <li> <p>Access via IP Address <pre><code>http://54.214.215.124\n</code></pre>    (Find IP in Terraform outputs: <code>terraform output allocator_public_ip</code>)</p> </li> <li> <p>Use curl for testing <pre><code>curl http://test.lablink.sleap.ai\n</code></pre></p> </li> </ol> <p>Verify Staging Mode is Working:</p> <pre><code># HTTP should return 200 OK\ncurl -I http://test.lablink.sleap.ai\n\n# HTTPS should fail (connection refused)\ncurl -I https://test.lablink.sleap.ai\n</code></pre> <p>Expected behavior: When using staging mode (<code>ssl.staging: true</code>), your browser will show \"Not Secure\" in the address bar. This is normal and expected - staging mode uses unencrypted HTTP for testing.</p> <p>To get a secure HTTPS connection, set <code>ssl.staging: false</code> in your configuration. See Configuration - SSL Options.</p>"},{"location":"troubleshooting/#flask-app-not-starting","title":"Flask App Not Starting","text":"<p>Symptoms: Container runs but Flask doesn't start</p> <p>Check:</p> <pre><code># View full logs\nsudo docker logs &lt;container-id&gt;\n\n# Look for errors like:\n# - Port already in use\n# - Module import errors\n# - Configuration errors\n</code></pre> <p>Solutions:</p> <ol> <li> <p>Restart container:    <pre><code>sudo docker restart &lt;container-id&gt;\n</code></pre></p> </li> <li> <p>Check for port conflicts:    <pre><code>sudo netstat -tulpn | grep 5000\n</code></pre></p> </li> <li> <p>Verify configuration:    <pre><code>sudo docker exec &lt;container-id&gt; cat /app/config/config.yaml\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#postgresql-issues","title":"PostgreSQL Issues","text":""},{"location":"troubleshooting/#postgresql-not-accessible","title":"PostgreSQL Not Accessible","text":"<p>Known Issue: PostgreSQL server may need manual restart after first deployment.</p> <p>Solution: <pre><code># SSH into allocator\nssh -i ~/lablink-key.pem ubuntu@&lt;ec2-public-ip&gt;\n\n# Get container name\nsudo docker ps\n\n# Enter container\nsudo docker exec -it &lt;container-name&gt; bash\n\n# Inside container, restart PostgreSQL\n/etc/init.d/postgresql restart\n\n# Verify it's running\npg_isready -U lablink\n</code></pre></p> <p>Verify manually: <pre><code># Test connection\nsudo docker exec &lt;container-id&gt; psql -U lablink -d lablink_db -c \"SELECT 1;\"\n</code></pre></p>"},{"location":"troubleshooting/#database-connection-refused","title":"Database Connection Refused","text":"<p>Error: <code>psycopg2.OperationalError: could not connect to server</code></p> <p>Solutions:</p> <ol> <li> <p>Check PostgreSQL is running:    <pre><code>sudo docker exec &lt;container-id&gt; pg_isready -U lablink\n</code></pre></p> </li> <li> <p>Check PostgreSQL logs:    <pre><code>sudo docker exec &lt;container-id&gt; tail -f /var/log/postgresql/postgresql-*-main.log\n</code></pre></p> </li> <li> <p>Verify configuration:    <pre><code># Check pg_hba.conf\nsudo docker exec &lt;container-id&gt; cat /etc/postgresql/*/main/pg_hba.conf\n\n# Should contain:\n# host    all             all             0.0.0.0/0            md5\n</code></pre></p> </li> <li> <p>Restart PostgreSQL (see above)</p> </li> </ol>"},{"location":"troubleshooting/#vm-spawning-issues","title":"VM Spawning Issues","text":""},{"location":"troubleshooting/#vms-not-being-created","title":"VMs Not Being Created","text":"<p>Symptoms: Click \"Create VMs\" but nothing happens</p> <p>Check:</p> <ol> <li> <p>Allocator container logs:    <pre><code>sudo docker logs -f &lt;allocator-container-id&gt;\n</code></pre></p> </li> <li> <p>AWS credentials configured:    <pre><code># Inside container\nsudo docker exec &lt;container-id&gt; aws sts get-caller-identity\n</code></pre></p> </li> <li> <p>Terraform installed in container:    <pre><code>sudo docker exec &lt;container-id&gt; terraform version\n</code></pre></p> </li> </ol> <p>Solutions:</p> <ol> <li>Set AWS credentials (if not using IAM role):</li> <li>Navigate to <code>/admin/set-aws-credentials</code> in web interface</li> <li>Enter AWS access key and secret key</li> <li> <p>Submit</p> </li> <li> <p>Check Terraform errors:    <pre><code># Inside container\nsudo docker exec &lt;container-id&gt; cat /tmp/terraform-*.log\n</code></pre></p> </li> <li> <p>Verify IAM permissions:</p> </li> <li>EC2 permissions (RunInstances, DescribeInstances)</li> <li>VPC permissions (CreateSecurityGroup, etc.)</li> </ol>"},{"location":"troubleshooting/#terraform-apply-failed-inside-container","title":"Terraform Apply Failed Inside Container","text":"<p>Error: Terraform operations fail when triggered from allocator</p> <p>Solutions:</p> <ol> <li> <p>Check AWS credentials:    <pre><code>sudo docker exec &lt;container-id&gt; env | grep AWS\n</code></pre></p> </li> <li> <p>Test Terraform manually:    <pre><code>sudo docker exec -it &lt;container-id&gt; bash\ncd /app/.venv/lib/python*/site-packages/lablink_allocator/terraform\nterraform init\nterraform plan\n</code></pre></p> </li> <li> <p>Verify Docker socket access (if using Docker-in-Docker):    <pre><code>sudo docker exec &lt;container-id&gt; docker ps\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#terraform-issues","title":"Terraform Issues","text":""},{"location":"troubleshooting/#terraform-init-fails","title":"Terraform Init Fails","text":"<p>Error: <code>Error configuring the backend \"s3\": ... bucket does not exist</code></p> <p>Solution: <pre><code># Create S3 bucket first\naws s3 mb s3://tf-state-lablink-allocator-bucket --region us-west-2\n\n# Enable versioning\naws s3api put-bucket-versioning \\\n  --bucket tf-state-lablink-allocator-bucket \\\n  --versioning-configuration Status=Enabled\n\n# Re-init\nterraform init\n</code></pre></p>"},{"location":"troubleshooting/#terraform-state-locked","title":"Terraform State Locked","text":"<p>Error: <code>Error acquiring the state lock</code></p> <p>Cause: A previous Terraform operation didn't complete cleanly, leaving the state locked in DynamoDB.</p> <p>Diagnosis:</p> <ol> <li> <p>Identify the lock:    <pre><code># Check for locks in DynamoDB\naws dynamodb scan --table-name lock-table --region us-west-2\n</code></pre></p> </li> <li> <p>Check if a process is actually running:    <pre><code># Look for terraform processes\nps aux | grep terraform\n\n# In allocator container\nsudo docker exec &lt;container-id&gt; ps aux | grep terraform\n</code></pre></p> </li> </ol> <p>Solutions:</p> <p>Option 1: Unlock via AWS CLI (Recommended - works from anywhere)</p> <p>Step 1: First, scan the lock table to find the exact LockID: <pre><code># List all locks\naws dynamodb scan --profile &lt;your-profile&gt; --table-name lock-table --region us-west-2\n</code></pre></p> <p>Step 2: Look for entries with an <code>Info</code> field (these are actual locks, not just digests). Copy the exact <code>LockID</code> value.</p> <p>Step 3: Delete the lock:</p> <p>Linux/macOS: <pre><code>aws dynamodb delete-item \\\n    --profile &lt;your-profile&gt; \\\n    --table-name lock-table \\\n    --key '{\"LockID\":{\"S\":\"&lt;exact-lock-id-from-scan&gt;\"}}' \\\n    --region us-west-2\n</code></pre></p> <p>Windows PowerShell: <pre><code># Create key.json file with:\n# {\n#   \"LockID\": {\n#     \"S\": \"&lt;exact-lock-id-from-scan&gt;\"\n#   }\n# }\n\naws dynamodb delete-item --profile &lt;your-profile&gt; --table-name lock-table --key file://key.json --region us-west-2\n</code></pre></p> <p>Common lock paths: - Infrastructure: <code>tf-state-lablink-allocator-bucket/test/terraform.tfstate</code> - Client VMs: <code>tf-state-lablink-allocator-bucket/test/client/terraform.tfstate</code></p> <p>Note: Lock IDs do NOT always have <code>-md5</code> suffix - use the exact value from the scan!</p> <p>Option 2: Unlock from allocator (Requires DynamoDB IAM permissions) <pre><code># SSH into allocator\nssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\n\n# Enter container\nsudo docker exec -it &lt;container-id&gt; bash\n\n# Navigate to terraform directory\ncd /app/lablink_allocator/terraform\n\n# Force unlock (using lock ID from error message)\nterraform force-unlock &lt;lock-id&gt;\n</code></pre></p> <p>Common Error: <code>AccessDeniedException: User is not authorized to perform: dynamodb:GetItem</code></p> <p>Cause: The allocator IAM role lacks DynamoDB permissions.</p> <p>Solution: Ensure the allocator IAM role includes these permissions: <pre><code>{\n  \"Effect\": \"Allow\",\n  \"Action\": [\n    \"dynamodb:GetItem\",\n    \"dynamodb:PutItem\",\n    \"dynamodb:DeleteItem\"\n  ],\n  \"Resource\": \"arn:aws:dynamodb:us-west-2:&lt;account-id&gt;:table/lock-table\"\n}\n</code></pre></p> <p>After updating IAM permissions, redeploy infrastructure for changes to take effect.</p> <p>Prevention: - Don't manually terminate EC2 instances while Terraform is running - Always let Terraform operations complete fully - Use destroy workflows instead of manual AWS console deletions - Monitor allocator logs during VM creation/destruction</p>"},{"location":"troubleshooting/#resource-already-exists","title":"Resource Already Exists","text":"<p>Error: <code>Error creating ... already exists</code></p> <p>Solutions:</p> <ol> <li> <p>Import existing resource:    <pre><code>terraform import aws_security_group.lablink sg-xxxxx\nterraform import aws_instance.lablink_allocator i-xxxxx\n</code></pre></p> </li> <li> <p>Delete resource manually (if safe):    <pre><code>aws ec2 terminate-instances --instance-ids i-xxxxx\naws ec2 delete-security-group --group-id sg-xxxxx\n</code></pre></p> </li> <li> <p>Use different resource names:</p> </li> <li>Change <code>resource_suffix</code> variable: <code>-dev</code>, <code>-test</code>, <code>-prod</code></li> </ol>"},{"location":"troubleshooting/#cannot-destroy-resources","title":"Cannot Destroy Resources","text":"<p>Error: Resources won't destroy cleanly</p> <p>Solution:</p> <ol> <li> <p>Destroy in order:    <pre><code># Terminate instances first\naws ec2 terminate-instances --instance-ids i-xxxxx\n\n# Wait for termination\naws ec2 wait instance-terminated --instance-ids i-xxxxx\n\n# Delete security group\naws ec2 delete-security-group --group-id sg-xxxxx\n</code></pre></p> </li> <li> <p>Check dependencies:</p> </li> <li>Network interfaces attached</li> <li>Elastic IPs associated</li> <li> <p>Security group rules referencing each other</p> </li> <li> <p>Force destroy:    <pre><code>terraform destroy -auto-approve\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#github-actions-workflow-issues","title":"GitHub Actions Workflow Issues","text":""},{"location":"troubleshooting/#workflow-wont-trigger","title":"Workflow Won't Trigger","text":"<p>Check:</p> <ol> <li> <p>Workflow file syntax:    <pre><code># Use YAML validator\nyamllint .github/workflows/*.yml\n</code></pre></p> </li> <li> <p>Branch protection rules:</p> </li> <li>Check repository settings</li> <li> <p>Ensure workflows are enabled</p> </li> <li> <p>Trigger conditions:</p> </li> <li>Verify branch name matches trigger</li> <li>Check file path filters</li> </ol>"},{"location":"troubleshooting/#aws-authentication-fails-in-workflow","title":"AWS Authentication Fails in Workflow","text":"<p>Error: <code>Error: Could not assume role with OIDC</code></p> <p>Solutions:</p> <ol> <li> <p>Verify OIDC provider exists:    <pre><code>aws iam list-open-id-connect-providers\n</code></pre></p> </li> <li> <p>Check IAM role trust policy:    <pre><code>aws iam get-role --role-name github-lablink-deploy \\\n  --query 'Role.AssumeRolePolicyDocument'\n</code></pre></p> </li> <li> <p>Verify repository in trust policy:</p> </li> <li> <p>Trust policy must include: <code>repo:talmolab/lablink:*</code></p> </li> <li> <p>Check role ARN in workflow:</p> </li> <li>Ensure ARN matches your account ID</li> </ol>"},{"location":"troubleshooting/#terraform-apply-fails-in-workflow","title":"Terraform Apply Fails in Workflow","text":"<p>Check workflow logs for specific error:</p> <ol> <li>Permission errors: Update IAM role permissions</li> <li>Resource limits: Check AWS service quotas</li> <li>State lock: Clear lock if safe</li> </ol>"},{"location":"troubleshooting/#client-vm-issues","title":"Client VM Issues","text":""},{"location":"troubleshooting/#client-vm-not-registering","title":"Client VM Not Registering","text":"<p>Symptoms: VM created but doesn't appear in allocator database</p> <p>Root Cause: VMs are not being inserted into the database after Terraform creates them.</p> <p>Step-by-Step Diagnosis:</p> <ol> <li> <p>Verify VMs were created by Terraform:    <pre><code># Check terraform outputs from allocator\nssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\nsudo docker exec &lt;container-id&gt; terraform -chdir=/app/.venv/lib/python*/site-packages/lablink_allocator/terraform output vm_instance_names\n</code></pre></p> </li> <li> <p>Check if VMs exist in database:    <pre><code># SSH into allocator\nssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\n\n# Query database\nsudo docker exec &lt;container-id&gt; psql -U lablink -d lablink_db -c \"SELECT hostname, inuse, status FROM vms;\"\n</code></pre></p> </li> <li> <p>Check client VM container logs:    <pre><code>ssh -i ~/lablink-key.pem ubuntu@&lt;client-vm-ip&gt;\nsudo docker logs &lt;client-container-id&gt;\n\n# Look for errors like:\n# \"POST request failed with status code: 404\"\n# \"VM not found\"\n</code></pre></p> </li> <li> <p>Check allocator logs for /vm_startup requests:    <pre><code>ssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\nsudo docker logs &lt;allocator-container-id&gt; | grep vm_startup\n\n# Look for:\n# POST /vm_startup - 404 errors\n</code></pre></p> </li> <li> <p>Test network connectivity:    <pre><code># From client VM\ncurl http://&lt;allocator-ip&gt;/vm_startup \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"hostname\": \"lablink-vm-test-1\"}'\n\n# Expected if VM not in DB: {\"error\":\"VM not found.\"}\n# Expected if VM in DB: Success response\n</code></pre></p> </li> </ol> <p>Solutions:</p> <p>Option A: Manual Database Insertion (Temporary Fix) <pre><code># SSH into allocator\nssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\n\n# Get container ID\nCONTAINER_ID=$(sudo docker ps -q)\n\n# Insert VMs manually\nsudo docker exec $CONTAINER_ID psql -U lablink -d lablink_db -c \\\n  \"INSERT INTO vms (hostname, inuse) VALUES ('lablink-vm-test-1', FALSE);\"\n\nsudo docker exec $CONTAINER_ID psql -U lablink -d lablink_db -c \\\n  \"INSERT INTO vms (hostname, inuse) VALUES ('lablink-vm-test-2', FALSE);\"\n\n# Verify\nsudo docker exec $CONTAINER_ID psql -U lablink -d lablink_db -c \\\n  \"SELECT hostname, inuse FROM vms;\"\n</code></pre></p> <p>Option B: Code Fix (Permanent Solution)</p> <p>The <code>/api/launch</code> endpoint needs to be updated to insert VMs after Terraform succeeds. See VM_REGISTRATION_ISSUE.md for details.</p> <p>After Fix - Verify Registration: <pre><code># Watch client VM logs\nssh -i ~/lablink-key.pem ubuntu@&lt;client-vm-ip&gt;\nsudo docker logs -f &lt;client-container-id&gt;\n\n# Should see:\n# \"POST request was successful.\"\n# \"Received success response from server.\"\n</code></pre></p> <p>Preventive Measures: - Always verify VMs appear in database after creation - Check allocator logs during VM creation - Monitor client VM registration within 5 minutes of creation</p>"},{"location":"troubleshooting/#gpu-not-available","title":"GPU Not Available","text":"<p>Error: <code>RuntimeError: CUDA not available</code></p> <p>Check:</p> <ol> <li> <p>GPU instance type:    <pre><code># Verify instance has GPU\naws ec2 describe-instances --instance-ids &lt;id&gt; \\\n  --query 'Reservations[0].Instances[0].InstanceType'\n</code></pre></p> </li> <li> <p>NVIDIA drivers installed:    <pre><code>ssh -i ~/lablink-key.pem ubuntu@&lt;client-vm-ip&gt;\nnvidia-smi\n</code></pre></p> </li> <li> <p>Docker GPU support:    <pre><code>docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi\n</code></pre></p> </li> </ol> <p>Solution:</p> <ol> <li> <p>Use GPU-enabled AMI or install drivers:    <pre><code># Ubuntu Deep Learning AMI\nami_id = \"ami-0c2b0d3fb02824d92\"  # us-west-2\n</code></pre></p> </li> <li> <p>Verify Docker GPU runtime:    <pre><code>cat /etc/docker/daemon.json\n# Should have: \"default-runtime\": \"nvidia\"\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#dns-and-domain-issues","title":"DNS and Domain Issues","text":"<p>For detailed DNS configuration, see DNS Configuration Guide.</p>"},{"location":"troubleshooting/#dns-record-not-resolving","title":"DNS Record Not Resolving","text":"<p>Symptoms: Domain doesn't resolve to allocator IP</p> <p>Step-by-Step Diagnosis:</p> <ol> <li> <p>Verify DNS is enabled in config:    <pre><code>cat lablink-infrastructure/config/config.yaml | grep -A 10 \"^dns:\"\n</code></pre></p> </li> <li> <p>Check Route53 record exists:    <pre><code>aws route53 list-resource-record-sets \\\n  --hosted-zone-id Z010760118DSWF5IYKMOM \\\n  --query \"ResourceRecordSets[?Name=='test.lablink.sleap.ai.']\"\n</code></pre></p> </li> <li> <p>Query authoritative nameservers directly:    <pre><code># Should return the allocator IP\nnslookup test.lablink.sleap.ai ns-158.awsdns-19.com\n</code></pre></p> </li> <li> <p>Check public DNS propagation:    <pre><code># Google DNS\nnslookup test.lablink.sleap.ai 8.8.8.8\n\n# Cloudflare DNS\nnslookup test.lablink.sleap.ai 1.1.1.1\n\n# Local DNS\nnslookup test.lablink.sleap.ai\n</code></pre></p> </li> <li> <p>Verify IP matches:    <pre><code># Get terraform output\ncd lablink-infrastructure\nterraform output allocator_public_ip\n\n# Compare with DNS resolution\ndig test.lablink.sleap.ai +short\n</code></pre></p> </li> </ol> <p>Solutions:</p> <p>If record doesn't exist: <pre><code># Re-run terraform to create DNS record\ncd lablink-infrastructure\nterraform apply\n</code></pre></p> <p>If DNS not propagating: - Wait 5-15 minutes for global propagation - Check NS delegation is correct (see below) - Try flushing local DNS cache:   <pre><code># macOS\nsudo dscacheutil -flushcache; sudo killall -HUP mDNSResponder\n\n# Linux\nsudo systemd-resolve --flush-caches\n\n# Windows\nipconfig /flushdns\n</code></pre></p>"},{"location":"troubleshooting/#dns-record-in-wrong-zone","title":"DNS Record in Wrong Zone","text":"<p>Symptoms: DNS record created in <code>sleap.ai</code> zone instead of <code>lablink.sleap.ai</code> zone</p> <p>Root Cause: Terraform data source matched parent zone</p> <p>Diagnosis: <pre><code># Check both zones\naws route53 list-resource-record-sets --hosted-zone-id &lt;sleap.ai-zone-id&gt; \\\n  --query \"ResourceRecordSets[?contains(Name, 'lablink')]\"\n\naws route53 list-resource-record-sets --hosted-zone-id Z010760118DSWF5IYKMOM \\\n  --query \"ResourceRecordSets[?contains(Name, 'test')]\"\n</code></pre></p> <p>Solution:</p> <ol> <li> <p>Add zone_id to config.yaml:    <pre><code>dns:\n  enabled: true\n  domain: \"lablink.sleap.ai\"\n  zone_id: \"Z010760118DSWF5IYKMOM\"  # Force correct zone\n  pattern: \"custom\"\n  custom_subdomain: \"test\"\n</code></pre></p> </li> <li> <p>Delete record from wrong zone (if exists):    <pre><code># Use AWS console or CLI to delete A record from sleap.ai zone\n</code></pre></p> </li> <li> <p>Re-run terraform:    <pre><code>cd lablink-infrastructure\nterraform apply\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#ns-delegation-not-working","title":"NS Delegation Not Working","text":"<p>Symptoms: - nslookup returns Cloudflare nameservers instead of AWS - DNS queries fail even though record exists in Route53</p> <p>Diagnosis: <pre><code># Check NS delegation\ndig NS lablink.sleap.ai\n\n# Should show AWS nameservers:\n# ns-158.awsdns-19.com\n# ns-697.awsdns-23.net\n# ns-1839.awsdns-37.co.uk\n# ns-1029.awsdns-00.org\n</code></pre></p> <p>Solution:</p> <ol> <li> <p>Get Route53 nameservers:    <pre><code>aws route53 get-hosted-zone --id Z010760118DSWF5IYKMOM \\\n  --query 'DelegationSet.NameServers'\n</code></pre></p> </li> <li> <p>Add NS records in Cloudflare:</p> </li> <li>Log into Cloudflare</li> <li>Navigate to DNS for <code>sleap.ai</code></li> <li> <p>Add 4 NS records:</p> <ul> <li>Type: NS</li> <li>Name: <code>lablink</code></li> <li>Content: Each of the 4 AWS nameservers</li> <li>TTL: 300 (or Auto)</li> </ul> </li> <li> <p>Verify delegation:    <pre><code># Wait 5-15 minutes, then check\ndig NS lablink.sleap.ai\n</code></pre></p> </li> <li> <p>Test resolution:    <pre><code># Should now resolve via AWS\nnslookup test.lablink.sleap.ai 8.8.8.8\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#httpsssl-certificate-not-working","title":"HTTPS/SSL Certificate Not Working","text":"<p>Symptoms: - HTTP works but HTTPS fails - Browser shows \"Connection refused\" or \"SSL error\"</p> <p>Step-by-Step Diagnosis:</p> <ol> <li> <p>Check DNS is resolving:    <pre><code>nslookup test.lablink.sleap.ai 8.8.8.8\n# Must resolve before Let's Encrypt can issue certificate\n</code></pre></p> </li> <li> <p>Check Caddy is running:    <pre><code>ssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\nsudo systemctl status caddy\n</code></pre></p> </li> <li> <p>Check Caddy logs:    <pre><code>sudo journalctl -u caddy -f\n\n# Look for:\n# - \"certificate obtained successfully\" (success)\n# - \"challenge failed\" (DNS not ready)\n# - \"timeout\" (network issue)\n</code></pre></p> </li> <li> <p>Test HTTPS manually:    <pre><code>curl -v https://test.lablink.sleap.ai\n\n# Look for SSL handshake or certificate errors\n</code></pre></p> </li> <li> <p>Check ports are open:    <pre><code># Port 80 (HTTP-01 challenge)\nnc -vz test.lablink.sleap.ai 80\n\n# Port 443 (HTTPS)\nnc -vz test.lablink.sleap.ai 443\n</code></pre></p> </li> </ol> <p>Solutions:</p> <p>If DNS not propagated: - Wait 5-10 more minutes - Verify DNS resolves to correct IP - Caddy will automatically retry every 2 minutes</p> <p>If ports blocked: <pre><code># Check security group\naws ec2 describe-security-groups \\\n  --filters \"Name=tag:Name,Values=lablink-allocator-*\" \\\n  --query 'SecurityGroups[0].IpPermissions'\n\n# Add rules if missing\naws ec2 authorize-security-group-ingress \\\n  --group-id &lt;sg-id&gt; \\\n  --protocol tcp \\\n  --port 80 \\\n  --cidr 0.0.0.0/0\n\naws ec2 authorize-security-group-ingress \\\n  --group-id &lt;sg-id&gt; \\\n  --protocol tcp \\\n  --port 443 \\\n  --cidr 0.0.0.0/0\n</code></pre></p> <p>If Caddy configuration error: <pre><code># Check Caddyfile\nssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\ncat /etc/caddy/Caddyfile\n\n# Should contain:\n# test.lablink.sleap.ai {\n#     reverse_proxy localhost:5000\n# }\n\n# Restart Caddy\nsudo systemctl restart caddy\n</code></pre></p> <p>Manual certificate check: <pre><code># View certificate details\necho | openssl s_client -servername test.lablink.sleap.ai \\\n  -connect test.lablink.sleap.ai:443 2&gt;/dev/null | \\\n  openssl x509 -noout -text\n\n# Check issuer and expiration\necho | openssl s_client -servername test.lablink.sleap.ai \\\n  -connect test.lablink.sleap.ai:443 2&gt;/dev/null | \\\n  openssl x509 -noout -issuer -dates\n</code></pre></p>"},{"location":"troubleshooting/#multiple-hosted-zones-causing-conflicts","title":"Multiple Hosted Zones Causing Conflicts","text":"<p>Symptoms: Unpredictable DNS behavior, records in multiple zones</p> <p>Diagnosis: <pre><code># List all zones\naws route53 list-hosted-zones --query 'HostedZones[*].[Name,Id]' --output table\n\n# Check for duplicates or parent/child conflicts\n</code></pre></p> <p>Solution:</p> <ol> <li>Identify which zone to keep:</li> <li>Keep <code>lablink.sleap.ai</code> in Route53 (managed by LabLink)</li> <li> <p>Delete <code>sleap.ai</code> from Route53 (managed in Cloudflare)</p> </li> <li> <p>Delete conflicting zone:    <pre><code># ONLY if sleap.ai is managed in Cloudflare\naws route53 delete-hosted-zone --id &lt;sleap-ai-zone-id&gt;\n</code></pre></p> </li> <li> <p>Verify NS delegation in Cloudflare (see above)</p> </li> </ol>"},{"location":"troubleshooting/#deployment-verification-failing","title":"Deployment Verification Failing","text":"<p>Symptoms: <code>verify-deployment.sh</code> reports failures</p> <p>Run verification: <pre><code>cd lablink-infrastructure\n./verify-deployment.sh test.lablink.sleap.ai 52.40.142.146\n</code></pre></p> <p>Common failures:</p> <ol> <li>DNS timeout - Wait longer and retry</li> <li>HTTP not responding - Check allocator container logs</li> <li>SSL not ready - Check Caddy logs, may need more time</li> </ol> <p>Interpret results: - \u2713 Green checkmarks = Success - \u26a0 Yellow warnings = May need more time - \u2717 Red errors = Actual problem requiring action</p>"},{"location":"troubleshooting/#diagnostic-commands","title":"Diagnostic Commands","text":""},{"location":"troubleshooting/#check-overall-system-health","title":"Check Overall System Health","text":"<pre><code># SSH into allocator\nssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\n\n# System resources\ndf -h              # Disk usage\nfree -h            # Memory usage\ntop                # CPU usage\n\n# Docker\nsudo docker ps -a  # All containers\nsudo docker stats  # Resource usage\n\n# Network\nsudo netstat -tulpn    # Listening ports\nip addr show           # Network interfaces\n\n# Logs\nsudo journalctl -u docker -n 100  # Docker service logs\ndmesg | tail                      # Kernel messages\n</code></pre>"},{"location":"troubleshooting/#check-flask-app-status","title":"Check Flask App Status","text":"<pre><code># From allocator instance\ncurl localhost:80\n\n# Expected: HTML response with LabLink interface\n</code></pre>"},{"location":"troubleshooting/#check-database-status","title":"Check Database Status","text":"<pre><code># PostgreSQL running?\nsudo docker exec &lt;container-id&gt; pg_isready -U lablink\n\n# Can connect?\nsudo docker exec &lt;container-id&gt; psql -U lablink -d lablink_db -c \"SELECT version();\"\n\n# View VMs\nsudo docker exec &lt;container-id&gt; psql -U lablink -d lablink_db -c \"SELECT * FROM vms;\"\n</code></pre>"},{"location":"troubleshooting/#check-aws-connectivity","title":"Check AWS Connectivity","text":"<pre><code># From allocator container\nsudo docker exec &lt;container-id&gt; aws sts get-caller-identity\n\n# List EC2 instances\nsudo docker exec &lt;container-id&gt; aws ec2 describe-instances --region us-west-2\n</code></pre>"},{"location":"troubleshooting/#getting-help","title":"Getting Help","text":"<p>If issues persist:</p> <ol> <li>Check logs thoroughly: Most issues have error messages in logs</li> <li>Search existing issues: GitHub Issues</li> <li>Open new issue: Provide:</li> <li>Error messages</li> <li>Logs</li> <li>Steps to reproduce</li> <li>Environment (OS, versions)</li> <li>What you've tried</li> </ol>"},{"location":"troubleshooting/#related-documentation","title":"Related Documentation","text":"<ul> <li>Installation: Setup instructions</li> <li>Deployment: Deployment guides</li> <li>SSH Access: Connection help</li> <li>Database: Database issues</li> <li>Security: Security problems</li> </ul>"},{"location":"troubleshooting/#quick-troubleshooting-checklist","title":"Quick Troubleshooting Checklist","text":"<ul> <li> Docker is running</li> <li> Container is running (<code>docker ps</code>)</li> <li> Security groups allow required ports (22, 80, 5432)</li> <li> SSH key has correct permissions (600)</li> <li> AWS credentials are configured</li> <li> PostgreSQL has been restarted (if needed)</li> <li> Terraform state is not locked</li> <li> S3 bucket exists for Terraform state</li> <li> IAM role/user has necessary permissions</li> <li> Network connectivity between components</li> <li> Logs have been checked for errors</li> </ul>"},{"location":"workflows/","title":"Workflows","text":"<p>This guide explains LabLink's CI/CD workflows, how they work, and how to customize them.</p>"},{"location":"workflows/#overview","title":"Overview","text":"<p>LabLink uses GitHub Actions for continuous integration and deployment. The workflows automate:</p> <ul> <li>Python package publishing to PyPI</li> <li>Docker image building and publishing to GHCR</li> <li>Testing and validation (linting, unit tests, Docker builds)</li> <li>Documentation deployment to GitHub Pages</li> </ul> <p>Note: Infrastructure deployment workflows (Terraform) have been moved to the LabLink Template Repository.</p>"},{"location":"workflows/#cicd-pipeline-overview","title":"CI/CD Pipeline Overview","text":"flowchart TB     subgraph Triggers[\"Workflow Triggers\"]         PR[Pull Request]         Push[Push to Branch]         Tag[Git Tag Push]         Manual[Manual Dispatch]     end      subgraph CI[\"ci.yml - Continuous Integration\"]         Lint[Lint Code&lt;br/&gt;ruff check]         Test[Run Tests&lt;br/&gt;pytest]         DockerTest[Docker Build Test&lt;br/&gt;Allocator only]         Lint --&gt; Test --&gt; DockerTest     end      subgraph Publish[\"publish-pip.yml - Package Publishing\"]         Validate[Validate Version&lt;br/&gt;&amp; Branch]         RunTests[Run Tests&lt;br/&gt;&amp; Linting]         PyPI[Publish to PyPI]         ShowCommand[Display Docker&lt;br/&gt;Build Command]         Validate --&gt; RunTests --&gt; PyPI --&gt; ShowCommand     end      subgraph Images[\"lablink-images.yml - Docker Images\"]         SelectDockerfile{Select&lt;br/&gt;Dockerfile}         BuildDev[Build Dev Image&lt;br/&gt;Dockerfile.dev]         BuildProd[Build Prod Image&lt;br/&gt;Dockerfile]         VerifyDev[Verify Dev Image]         VerifyProd[Verify Prod Image]         PushDev[Push with -test tags]         PushProd[Push with version tags]          SelectDockerfile --&gt;|PR/test/main| BuildDev         SelectDockerfile --&gt;|prod + versions| BuildProd         BuildDev --&gt; VerifyDev --&gt; PushDev         BuildProd --&gt; VerifyProd --&gt; PushProd     end      subgraph Docs[\"docs.yml - Documentation\"]         BuildDocs[Build MkDocs]         DeployPages[Deploy to&lt;br/&gt;GitHub Pages]         BuildDocs --&gt; DeployPages     end      PR --&gt; CI     Push --&gt; CI     Push --&gt; Images     Tag --&gt; Publish     Manual --&gt; Images     Publish -.-&gt;|Manual trigger needed| Images      style CI fill:#e3f2fd     style Publish fill:#fff3e0     style Images fill:#e8f5e9     style Docs fill:#f3e5f5"},{"location":"workflows/#workflow-files","title":"Workflow Files","text":"<p>All workflows are located in <code>.github/workflows/</code>:</p> Workflow File Purpose Trigger <code>ci.yml</code> Unit tests, linting, Docker build tests PRs, pushes <code>publish-pip.yml</code> Publish Python packages to PyPI Git tags, manual dispatch <code>lablink-images.yml</code> Build and push Docker images to GHCR Push to branches, PRs, package publish <code>docs.yml</code> Build and deploy documentation Pushes to main, docs changes"},{"location":"workflows/#continuous-integration-workflow","title":"Continuous Integration Workflow","text":"<p>File: <code>.github/workflows/ci.yml</code></p>"},{"location":"workflows/#purpose","title":"Purpose","text":"<p>Runs tests, linting, and Docker build verification on every pull request affecting service code.</p>"},{"location":"workflows/#triggers","title":"Triggers","text":"<ul> <li>Pull requests with changes to:</li> <li><code>packages/client/**</code></li> <li><code>packages/allocator/**</code></li> <li><code>.github/workflows/ci.yml</code></li> </ul>"},{"location":"workflows/#jobs","title":"Jobs","text":"<ol> <li> <p>Lint - Checks code quality with <code>ruff</code></p> </li> <li> <p>Allocator service: <code>uv run ruff check src tests</code></p> </li> <li> <p>Client service: <code>uv run ruff check src tests</code></p> </li> <li> <p>Test - Runs unit tests with <code>pytest</code></p> </li> <li> <p>Allocator: <code>uv run pytest tests --cov=. --cov-report=xml</code></p> </li> <li> <p>Client: <code>uv run pytest tests --cov=src/lablink_client_service --cov-report=xml</code></p> </li> <li> <p>Docker Build Test (Allocator Only)</p> </li> <li>Builds <code>packages/allocator/Dockerfile.dev</code> using <code>uv sync --extra dev</code></li> <li>Verifies virtual environment activation</li> <li>Verifies console script entry points are importable and callable</li> <li>Verifies console scripts exist (<code>lablink-allocator</code>, <code>generate-init-sql</code>)</li> <li>Verifies dev dependencies installed (pytest, ruff, coverage with versions)</li> <li>Verifies package imports (main, database, get_config)</li> <li>Verifies <code>uv sync</code> installation</li> <li>Note: Client Docker build test skipped due to large image size (~6GB with CUDA)</li> </ol>"},{"location":"workflows/#example-workflow-run","title":"Example Workflow Run","text":"<pre><code>PR opened \u2192 ci.yml triggered\n  \u251c\u2500 Lint allocator-service \u2713\n  \u251c\u2500 Lint client-service \u2713\n  \u251c\u2500 Test allocator-service \u2713\n  \u251c\u2500 Test client-service \u2713\n  \u2514\u2500 Docker Build Test - Allocator \u2713\n     \u251c\u2500 Venv activated: /app/lablink-allocator-service/.venv\n     \u251c\u2500 Entry points importable: main(), generate_init_sql.main() \u2713\n     \u251c\u2500 Console scripts: lablink-allocator, generate-init-sql \u2713\n     \u251c\u2500 Dev dependencies: pytest 8.4.2, ruff, coverage 7.10.7 \u2713\n     \u251c\u2500 Package imports: main.main, database.PostgresqlDatabase, get_config \u2713\n     \u2514\u2500 Installation: Package installed via uv sync \u2713\n</code></pre>"},{"location":"workflows/#package-publishing-workflow","title":"Package Publishing Workflow","text":"<p>File: <code>.github/workflows/publish-pip.yml</code></p>"},{"location":"workflows/#purpose_1","title":"Purpose","text":"<p>Publishes Python packages to PyPI with safety guardrails.</p>"},{"location":"workflows/#triggers_1","title":"Triggers","text":"<ul> <li>Git tags matching package name pattern (e.g., <code>lablink-allocator-service_v0.0.2a0</code>)</li> <li>Manual dispatch with dry-run option</li> </ul>"},{"location":"workflows/#features","title":"Features","text":"<ul> <li>Version verification (prevents republishing same version)</li> <li>Metadata validation</li> <li>Linting and tests before publishing</li> <li>Dry-run mode for testing</li> <li>Per-package control (publish allocator/client independently)</li> </ul>"},{"location":"workflows/#input-parameters-manual-dispatch","title":"Input Parameters (Manual Dispatch)","text":"Parameter Description Options Default <code>package</code> Which package to publish <code>lablink-allocator-service</code>, <code>lablink-client-service</code> Required <code>dry_run</code> Test without publishing <code>true</code>, <code>false</code> <code>true</code> <code>skip_tests</code> Skip test suite <code>true</code>, <code>false</code> <code>false</code>"},{"location":"workflows/#workflow-steps","title":"Workflow Steps","text":"<ol> <li>Determine which packages to publish (from tag or input)</li> <li>Run guardrails:</li> <li>Verify release from main branch (for releases)</li> <li>Validate version matches tag</li> <li>Validate package metadata</li> <li>Run linting with <code>ruff</code></li> <li>Run unit tests (unless skipped)</li> <li>Build package with <code>uv build</code></li> <li>Publish to PyPI (unless dry-run)</li> <li>Display manual Docker build instructions</li> </ol>"},{"location":"workflows/#package-versioning","title":"Package Versioning","text":"<ul> <li>Format: <code>{package-name}_v{version}</code></li> <li>Examples:</li> <li><code>lablink-allocator-service_v0.0.2a0</code></li> <li><code>lablink-client-service_v0.0.7a0</code></li> </ul>"},{"location":"workflows/#example-publishing-a-release","title":"Example: Publishing a Release","text":"<pre><code># 1. Create and push tags\ngit tag lablink-allocator-service_v0.0.2a0\ngit tag lablink-client-service_v0.0.7a0\ngit push origin lablink-allocator-service_v0.0.2a0 lablink-client-service_v0.0.7a0\n\n# 2. Workflow automatically:\n#    - Detects tags\n#    - Runs tests for each package\n#    - Publishes to PyPI\n#    - Displays Docker build instructions\n\n# 3. Manually trigger Docker image build (see below)\ngh workflow run lablink-images.yml \\\n  -f environment=prod \\\n  -f allocator_version=0.0.2a0 \\\n  -f client_version=0.0.7a0\n</code></pre>"},{"location":"workflows/#building-docker-images-after-publishing","title":"Building Docker Images After Publishing","text":"<p>CRITICAL: After successfully publishing to PyPI, you MUST manually trigger Docker image builds to create production images with the new package version. This is NOT automatic.</p> <p>Why manual? Production images should only be built with explicit version numbers to ensure traceability. Automatic builds use local code and <code>-test</code> suffix - they are NOT production images.</p> <p>Step-by-Step Process:</p> <ol> <li>Publish packages to PyPI (either via git tags or manual dispatch of <code>publish-pip.yml</code>)</li> <li>Wait for publish workflow to complete - Verify packages are on PyPI</li> <li>Manually trigger Docker image build using one of the methods below</li> </ol> <p>Option 1: Using GitHub CLI (recommended):</p> <pre><code># Build both images with their respective versions\ngh workflow run lablink-images.yml \\\n  -f environment=prod \\\n  -f allocator_version=0.0.2a0 \\\n  -f client_version=0.0.7a0\n</code></pre> <p>Option 2: Using GitHub UI:</p> <ol> <li>Go to Actions \u2192 Build and Push Docker Images</li> <li>Click \"Run workflow\"</li> <li>Select branch: <code>main</code></li> <li>Set environment: <code>prod</code> (required!)</li> <li>Enter allocator version: <code>0.0.2a0</code> (required!)</li> <li>Enter client version: <code>0.0.7a0</code> (required!)</li> <li>Click \"Run workflow\"</li> </ol> <p>What happens:</p> <ul> <li>Pulls packages from PyPI with specified versions</li> <li>Builds Docker images using production <code>Dockerfile</code></li> <li>Tags images with version numbers (e.g., <code>:0.0.2a0</code>, <code>:linux-amd64-0.0.2a0</code>)</li> <li>Tags images with <code>:latest</code> for convenience</li> <li>Verifies images work correctly</li> <li>No <code>-test</code> suffix on production images</li> </ul> <p>Common mistake: Forgetting this step means your packages exist on PyPI but there are no corresponding Docker images, causing deployment failures.</p> <p>Important Note: Pushing to <code>main</code> branch will NOT create production images. It will create development images with <code>-test</code> suffix using local code, not published packages.</p>"},{"location":"workflows/#image-building-workflow","title":"Image Building Workflow","text":"<p>File: <code>.github/workflows/lablink-images.yml</code></p>"},{"location":"workflows/#purpose_2","title":"Purpose","text":"<p>Builds and publishes Docker images to GitHub Container Registry (ghcr.io) using either local code (dev) or published packages (prod), then verifies the images work correctly.</p>"},{"location":"workflows/#triggers_2","title":"Triggers","text":"<ul> <li>Pull requests: Build dev images with <code>-test</code> tag</li> <li>Push to <code>test</code> branch: Build dev images with <code>-test</code> tag</li> <li>Push to <code>main</code>: Build dev images with <code>-test</code> tag</li> <li>Manual dispatch with <code>environment=test</code> or <code>environment=ci-test</code>: Build dev images with <code>-test</code> tag</li> <li>Manual dispatch with <code>environment=prod</code>: Build production images from PyPI (REQUIRES version parameters)</li> </ul>"},{"location":"workflows/#workflow-decision-logic","title":"Workflow Decision Logic","text":"<p>The workflow automatically selects between development (<code>Dockerfile.dev</code>) and production (<code>Dockerfile</code>) builds based on the trigger type and inputs.</p> <p>Key Principle: Production images from PyPI are ONLY created via manual dispatch with <code>environment=prod</code> and explicit version numbers. All automatic builds (PR, push to test/main) use local code.</p>"},{"location":"workflows/#decision-flow-diagram","title":"Decision Flow Diagram","text":"flowchart TD     Start[How was the workflow triggered?]     Start --&gt; PR[Pull Request]     Start --&gt; Push{Push to branch}     Start --&gt; Manual{Manual Dispatch&lt;br/&gt;workflow_dispatch}      PR --&gt; DevBuild     Push --&gt;|main branch| DevBuild     Push --&gt;|test branch| DevBuild      Manual --&gt;|environment=test| DevBuild     Manual --&gt;|environment=ci-test| DevBuild     Manual --&gt;|environment=prod| ProdBuild      DevBuild[Use Dockerfile.dev&lt;br/&gt;Local Code]     ProdBuild[Use Dockerfile&lt;br/&gt;PyPI Package]      DevBuild --&gt; DevDetails[\"\u2713 dev dependencies&lt;br/&gt;\u2713 -test suffix&lt;br/&gt;\u2713 runs tests\"]     ProdBuild --&gt; ProdDetails[\"\u2713 REQUIRES version params&lt;br/&gt;\u2713 version tag&lt;br/&gt;\u2713 no tests (already tested)\"]      DevDetails --&gt; DevEnd[Push to ghcr.io with -test tags]     ProdDetails --&gt; ProdEnd[Push to ghcr.io with version tags]      style DevBuild fill:#e1f5ff     style ProdBuild fill:#fff4e1     style DevDetails fill:#e1f5ff     style ProdDetails fill:#fff4e1"},{"location":"workflows/#complete-decision-table","title":"Complete Decision Table","text":"Trigger Type Branch/Ref Environment Input Dockerfile Used Package Source Version Required? Tag Suffix Dev Tests Run? Use Case Pull Request any N/A <code>Dockerfile.dev</code> Local code No <code>-test</code> Yes CI validation Push <code>test</code> N/A <code>Dockerfile.dev</code> Local code No <code>-test</code> Yes Staging/testing Push <code>main</code> N/A <code>Dockerfile.dev</code> Local code No <code>-test</code> Yes Latest development Manual Dispatch any <code>test</code> <code>Dockerfile.dev</code> Local code No <code>-test</code> Yes Test specific changes Manual Dispatch any <code>ci-test</code> <code>Dockerfile.dev</code> Local code No <code>-test</code> Yes CI testing with S3 backend Manual Dispatch any <code>prod</code> <code>Dockerfile</code> PyPI (explicit version) YES none No Production releases"},{"location":"workflows/#key-points","title":"Key Points","text":"<ul> <li> <p>Development builds (<code>Dockerfile.dev</code>):</p> </li> <li> <p>Copy local source code into image</p> </li> <li>Install with <code>uv sync --extra dev</code> from lockfile</li> <li>Include dev dependencies (pytest, ruff)</li> <li>Run verification tests in CI</li> <li>Always have <code>-test</code> suffix</li> <li> <p>Fast iteration, reproducible via lockfile</p> </li> <li> <p>Production builds (<code>Dockerfile</code>):</p> </li> <li> <p>Install packages from PyPI using <code>uv pip install</code></p> </li> <li>ONLY created via manual dispatch with <code>environment=prod</code></li> <li>REQUIRES explicit <code>allocator_version</code> and <code>client_version</code></li> <li>No dev dependencies (smaller image)</li> <li>No tests run (package already tested before publishing)</li> <li>Tagged with version number for traceability</li> <li>No suffix - clean version tags</li> <li> <p>Directly traceable to specific package release</p> </li> <li> <p>Version Validation:</p> </li> <li>Manual dispatch with <code>environment=prod</code> requires both <code>allocator_version</code> and <code>client_version</code></li> <li>Workflow fails with clear error if versions are missing</li> <li>Prevents untrackable production images</li> </ul>"},{"location":"workflows/#production-release-workflow","title":"Production Release Workflow","text":"<p>IMPORTANT: Production Docker images must be built AFTER publishing packages to PyPI. This is a manual two-step process:</p> sequenceDiagram     actor Developer     participant Git as Git Repository     participant GHA as GitHub Actions&lt;br/&gt;publish-pip.yml     participant PyPI     participant Manual as Manual Trigger     participant Build as GitHub Actions&lt;br/&gt;lablink-images.yml     participant Registry as ghcr.io      Developer-&gt;&gt;Git: Create and push tags&lt;br/&gt;lablink-allocator-service_v0.0.2a0&lt;br/&gt;lablink-client-service_v0.0.7a0     Git-&gt;&gt;GHA: Trigger publish-pip.yml      Note over GHA: Step 1: Publish to PyPI     GHA-&gt;&gt;GHA: Run tests     GHA-&gt;&gt;GHA: Validate versions     GHA-&gt;&gt;PyPI: Publish packages     PyPI--&gt;&gt;GHA: Confirm published     GHA-&gt;&gt;Developer: Display manual Docker&lt;br/&gt;build command      Note over Developer,Manual: CRITICAL: Do NOT skip Step 2      Developer-&gt;&gt;Manual: gh workflow run lablink-images.yml&lt;br/&gt;-f environment=prod&lt;br/&gt;-f allocator_version=0.0.2a0&lt;br/&gt;-f client_version=0.0.7a0      Note over Build: Step 2: Build Production Images     Manual-&gt;&gt;Build: Trigger with versions     Build-&gt;&gt;PyPI: Pull packages&lt;br/&gt;lablink-allocator==0.0.2a0&lt;br/&gt;lablink-client==0.0.7a0     Build-&gt;&gt;Build: Build from Dockerfile&lt;br/&gt;(PyPI packages)     Build-&gt;&gt;Registry: Push images with&lt;br/&gt;version tags     Registry--&gt;&gt;Developer: Images ready for&lt;br/&gt;deployment <p>Step 1: Publish packages to PyPI</p> <pre><code># Create and push git tags\ngit tag lablink-allocator-service_v0.0.2a0\ngit tag lablink-client-service_v0.0.7a0\ngit push origin lablink-allocator-service_v0.0.2a0 lablink-client-service_v0.0.7a0\n\n# publish-pip.yml workflow automatically:\n#   - Runs tests\n#   - Publishes to PyPI\n#   - Displays manual Docker build command\n</code></pre> <p>Step 2: Manually trigger Docker image build (required)</p> <pre><code># After packages are published, build production images\ngh workflow run lablink-images.yml \\\n  -f environment=prod \\\n  -f allocator_version=0.0.2a0 \\\n  -f client_version=0.0.7a0\n</code></pre> <p>Critical: Do NOT skip Step 2. Without it, your published packages won't have corresponding Docker images, and deployments will fail.</p>"},{"location":"workflows/#developmenttesting-workflows","title":"Development/Testing Workflows","text":"<p>Automatic (no action needed):</p> <pre><code># Push to test branch \u2192 automatically builds dev images with -test suffix\ngit push origin test\n\n# Push to main \u2192 automatically builds dev images with -test suffix\ngit push origin main\n</code></pre> <p>Manual testing:</p> <pre><code># Test specific changes without pushing\ngh workflow run lablink-images.yml -f environment=test\n\n# For CI testing with S3 backend (e.g., testing Terraform configurations)\ngh workflow run lablink-images.yml -f environment=ci-test\n</code></pre>"},{"location":"workflows/#common-mistakes","title":"Common Mistakes","text":"<p>Forgetting to build Docker images after publishing packages</p> <pre><code># Published to PyPI but forgot Step 2\ngit push origin lablink-allocator-service_v0.0.2a0\n# Result: Package exists but no Docker image with version tag\n</code></pre> <p>Trying to build production images without versions</p> <pre><code>gh workflow run lablink-images.yml -f environment=prod\n# Error: Production builds require both allocator_version and client_version\n</code></pre> <p>Correct production release</p> <pre><code># 1. Publish packages\ngit push origin lablink-allocator-service_v0.0.2a0 lablink-client-service_v0.0.7a0\n\n# 2. Wait for publish-pip.yml to complete successfully\n\n# 3. Build Docker images with explicit versions\ngh workflow run lablink-images.yml \\\n  -f environment=prod \\\n  -f allocator_version=0.0.2a0 \\\n  -f client_version=0.0.7a0\n</code></pre>"},{"location":"workflows/#smart-dockerfile-selection","title":"Smart Dockerfile Selection","text":"<p>The workflow uses different Dockerfiles depending on whether you're building for development/testing or production:</p> Trigger Dockerfile Used Package Source Installation Method Tests Run? Suffix Version Tagged? PR <code>Dockerfile.dev</code> Local code (copied) <code>uv sync --extra dev</code> Yes <code>-test</code> No Push to <code>test</code> <code>Dockerfile.dev</code> Local code (copied) <code>uv sync --extra dev</code> Yes <code>-test</code> No Push to <code>main</code> <code>Dockerfile.dev</code> Local code (copied) <code>uv sync --extra dev</code> Yes <code>-test</code> No Manual <code>environment=test</code> <code>Dockerfile.dev</code> Local code (copied) <code>uv sync --extra dev</code> Yes <code>-test</code> No Manual <code>environment=ci-test</code> <code>Dockerfile.dev</code> Local code (copied) <code>uv sync --extra dev</code> Yes <code>-test</code> No Manual <code>environment=prod</code> <code>Dockerfile</code> PyPI (explicit version) <code>uv pip install</code> No none Yes <p>Key Distinction:</p> <ul> <li>All automatic builds = Development images with <code>-test</code> suffix</li> <li>Manual production builds = Production images without suffix, with version tags</li> </ul>"},{"location":"workflows/#image-tagging-strategy","title":"Image Tagging Strategy","text":"<p>Docker images are tagged differently based on how they are triggered. This allows you to reference specific versions, latest development builds, or stable releases.</p>"},{"location":"workflows/#allocator-image-tags","title":"Allocator Image Tags","text":"<p>Manual trigger with package version (recommended for production):</p> <pre><code>gh workflow run lablink-images.yml \\\n  -f environment=prod \\\n  -f allocator_version=0.0.2a0 \\\n  -f client_version=0.0.7a0\n</code></pre> <p>Creates images tagged with:</p> <ul> <li><code>ghcr.io/talmolab/lablink-allocator-image:0.0.2a0</code> - Version-specific tag</li> <li><code>ghcr.io/talmolab/lablink-allocator-image:linux-amd64-0.0.2a0</code> - Platform + version</li> <li><code>ghcr.io/talmolab/lablink-allocator-image:linux-amd64-latest</code> - Latest for platform</li> <li><code>ghcr.io/talmolab/lablink-allocator-image:linux-amd64</code> - Platform tag</li> <li><code>ghcr.io/talmolab/lablink-allocator-image:linux-amd64-terraform-1.4.6</code> - Metadata tag</li> <li><code>ghcr.io/talmolab/lablink-allocator-image:linux-amd64-postgres-15</code> - Metadata tag</li> <li><code>ghcr.io/talmolab/lablink-allocator-image:&lt;sha&gt;</code> - Git commit SHA</li> <li><code>ghcr.io/talmolab/lablink-allocator-image:latest</code> - Latest stable</li> </ul> <p>Push to main branch (automatic):</p> <pre><code>git push origin main\n</code></pre> <p>Creates images tagged with (no version-specific tags):</p> <ul> <li><code>ghcr.io/talmolab/lablink-allocator-image:linux-amd64-latest</code></li> <li><code>ghcr.io/talmolab/lablink-allocator-image:linux-amd64</code></li> <li><code>ghcr.io/talmolab/lablink-allocator-image:&lt;sha&gt;</code></li> <li><code>ghcr.io/talmolab/lablink-allocator-image:latest</code></li> <li>Plus metadata tags</li> </ul> <p>Pull requests / test branch (automatic):</p> <pre><code>git push origin test\n</code></pre> <p>Creates images tagged with <code>-test</code> suffix:</p> <ul> <li><code>ghcr.io/talmolab/lablink-allocator-image:linux-amd64-test</code></li> <li><code>ghcr.io/talmolab/lablink-allocator-image:&lt;sha&gt;-test</code></li> <li>Plus metadata tags with <code>-test</code> suffix</li> </ul>"},{"location":"workflows/#client-image-tags","title":"Client Image Tags","text":"<p>Manual trigger with package version (recommended for production):</p> <pre><code>gh workflow run lablink-images.yml \\\n  -f environment=prod \\\n  -f allocator_version=0.0.2a0 \\\n  -f client_version=0.0.7a0\n</code></pre> <p>Creates images tagged with:</p> <ul> <li><code>ghcr.io/talmolab/lablink-client-base-image:0.0.7a0</code> - Version-specific tag</li> <li><code>ghcr.io/talmolab/lablink-client-base-image:linux-amd64-0.0.7a0</code> - Platform + version</li> <li><code>ghcr.io/talmolab/lablink-client-base-image:linux-amd64-latest</code> - Latest for platform</li> <li><code>ghcr.io/talmolab/lablink-client-base-image:linux-amd64-nvidia-cuda-11.6.1-cudnn8-runtime-ubuntu20.04</code></li> <li><code>ghcr.io/talmolab/lablink-client-base-image:linux-amd64-ubuntu20.04-nvm-0.40.2-uv-0.6.8-miniforge3-24.11.3</code></li> <li><code>ghcr.io/talmolab/lablink-client-base-image:&lt;sha&gt;</code> - Git commit SHA</li> <li><code>ghcr.io/talmolab/lablink-client-base-image:latest</code> - Latest stable</li> </ul> <p>Push to main branch (automatic):</p> <p>Creates same tags as manual trigger except without version-specific tags (<code>0.0.7a0</code>, <code>linux-amd64-0.0.7a0</code>)</p> <p>Pull requests / test branch (automatic):</p> <p>Creates same tags as main but with <code>-test</code> suffix</p>"},{"location":"workflows/#tag-usage-in-terraform","title":"Tag Usage in Terraform","text":"<p>For production deployments, always use version-specific tags in your Terraform configuration:</p> <pre><code># terraform.tfvars or -var flags\nallocator_image_tag = \"0.0.2a0\"  # Pin to specific version\nclient_image_tag    = \"0.0.7a0\"  # Pin to specific version\n</code></pre> <p>For development/testing, you can use environment-specific tags:</p> <pre><code># Development\nallocator_image_tag = \"linux-amd64-test\"\n\n# Latest main branch\nallocator_image_tag = \"latest\"\n</code></pre>"},{"location":"workflows/#summary-table","title":"Summary Table","text":"Trigger Type Environment Version Tag? Suffix Use Case Manual w/ version <code>prod</code> \u2705 Yes None Production releases Push to main N/A \u274c No <code>-test</code> Latest development Push to test N/A \u274c No <code>-test</code> Staging/testing Pull request N/A \u274c No <code>-test</code> CI/CD validation Manual dispatch <code>test</code> \u274c No <code>-test</code> Test specific changes Manual dispatch <code>ci-test</code> \u274c No <code>-test</code> CI testing with S3 backend"},{"location":"workflows/#workflow-jobs","title":"Workflow Jobs","text":""},{"location":"workflows/#1-build-job","title":"1. Build Job","text":"<ol> <li> <p>Free Disk Space</p> </li> <li> <p>Removes large pre-installed packages from GitHub Actions runner (~20-30GB freed)</p> </li> <li>Why needed: Client image with CUDA is ~6GB and requires 12-18GB during build</li> <li>Problem: GitHub runners start with only ~14GB free space, causing \"No space left on device\" errors</li> <li>What's removed:<ul> <li>Android SDK (~11GB) - Not needed for Python/Docker builds</li> <li>.NET SDK (~2GB) - Not needed</li> <li>Haskell GHC (~5GB) - Not needed</li> <li>Swap storage (~4GB) - Not needed for Docker builds</li> <li>Other cloud SDKs - Not needed</li> </ul> </li> <li>What's kept: Tool cache (Python, Node.js) for potential use in later steps</li> <li>Impact: Adds 2-4 minutes to build time, eliminates disk space failures</li> <li> <p>Safety: Runs on separate runners from verification jobs, no impact on image quality</p> </li> <li> <p>Select Dockerfile</p> </li> <li> <p>Dev: Uses <code>Dockerfile.dev</code> (copies local source, uses <code>uv sync</code>)</p> </li> <li> <p>Prod: Uses <code>Dockerfile</code> (installs from PyPI with <code>uv pip install</code>)</p> </li> <li> <p>Build Allocator Image</p> </li> <li> <p>Context: Repository root</p> </li> <li>Dockerfile: <code>packages/allocator/Dockerfile[.dev]</code></li> <li> <p>Tags: <code>ghcr.io/talmolab/lablink-allocator-image:&lt;tags&gt;</code></p> </li> <li> <p>Build Client Image</p> </li> <li> <p>Context: Repository root</p> </li> <li>Dockerfile: <code>packages/client/Dockerfile[.dev]</code></li> <li> <p>Tags: <code>ghcr.io/talmolab/lablink-client-base-image:&lt;tags&gt;</code></p> </li> <li> <p>Push to Registry</p> </li> <li>Authenticates to ghcr.io</li> <li>Pushes images with all applicable tags</li> </ol>"},{"location":"workflows/#2-verify-allocator-job","title":"2. Verify Allocator Job","text":"<p>Runs after successful build, pulls and tests the allocator image:</p> <ul> <li>Image Selection: Pulls using SHA-based tag (e.g., <code>:linux-amd64-&lt;sha&gt;-test</code>) to ensure exact image match and prevent race conditions from concurrent builds</li> <li>Virtual Environment: Activates venv at <code>/app/.venv</code></li> <li>Entry Points: Verifies <code>main()</code> and <code>generate_init_sql.main()</code> are importable and callable</li> <li>Console Scripts: Verifies <code>lablink-allocator</code> and <code>generate-init-sql</code> exist and execute</li> <li>Package Imports: Tests importing <code>main</code>, <code>database.PostgresqlDatabase</code>, <code>get_config</code></li> <li>Dev Dependencies (dev images only): Verifies pytest, ruff with versions</li> </ul>"},{"location":"workflows/#3-verify-client-job","title":"3. Verify Client Job","text":"<p>Runs after successful build, pulls and tests the client image:</p> <ul> <li>Image Selection: Pulls using SHA-based tag (e.g., <code>:linux-amd64-&lt;sha&gt;-test</code>) to ensure exact image match and prevent race conditions from concurrent builds</li> <li>Virtual Environment: Activates venv at <code>/home/client/.venv</code></li> <li>Entry Points: Verifies <code>check_gpu.main()</code>, <code>subscribe.main()</code>, <code>update_inuse_status.main()</code> are importable and callable</li> <li>Console Scripts: Verifies <code>check_gpu</code>, <code>subscribe</code>, <code>update_inuse_status</code> exist and execute</li> <li>Package Imports: Tests importing subscribe, check_gpu, update_inuse_status modules</li> <li>UV Availability: Verifies <code>uv</code> command and version</li> <li>Dev Dependencies (dev images only): Verifies pytest, ruff with versions</li> </ul>"},{"location":"workflows/#example-workflow-run_1","title":"Example Workflow Run","text":"<pre><code>PR opened \u2192 lablink-images.yml triggered\n  \u2514\u2500 Build Job\n     \u251c\u2500 Build allocator dev image \u2713\n     \u251c\u2500 Build client dev image \u2713\n     \u2514\u2500 Push to ghcr.io \u2713\n  \u2514\u2500 Verify Allocator Job\n     \u251c\u2500 Pull ghcr.io/.../lablink-allocator-image:linux-amd64-abc1234-test\n     \u251c\u2500 Venv activated: /app/.venv \u2713\n     \u251c\u2500 Entry points callable: main(), generate_init_sql.main() \u2713\n     \u251c\u2500 Console scripts: lablink-allocator, generate-init-sql \u2713\n     \u251c\u2500 Imports: main.main, database.PostgresqlDatabase, get_config \u2713\n     \u2514\u2500 Dev deps: pytest 8.4.2, ruff \u2713\n  \u2514\u2500 Verify Client Job\n     \u251c\u2500 Pull ghcr.io/.../lablink-client-base-image:linux-amd64-abc1234-test\n     \u251c\u2500 Venv activated: /home/client/.venv \u2713\n     \u251c\u2500 Entry points callable: check_gpu.main(), subscribe.main(), update_inuse_status.main() \u2713\n     \u251c\u2500 Console scripts: check_gpu, subscribe, update_inuse_status \u2713\n     \u251c\u2500 Imports: subscribe.main, check_gpu.main, update_inuse_status.main \u2713\n     \u251c\u2500 UV: uv 0.6.8 \u2713\n     \u2514\u2500 Dev deps: pytest 8.4.2, ruff \u2713\n</code></pre>"},{"location":"workflows/#customization","title":"Customization","text":"<p>To modify image building:</p> <pre><code># .github/workflows/lablink-images.yml\n\n# Build for different platforms\n- name: Build and push\n  uses: docker/build-push-action@v5\n  with:\n    platforms: linux/amd64,linux/arm64 # Add ARM support\n    push: ${{ github.event_name != 'pull_request' }}\n    tags: ${{ steps.meta.outputs.tags }}\n</code></pre>"},{"location":"workflows/#terraform-deployment-workflow","title":"Terraform Deployment Workflow","text":"<p>File: <code>.github/workflows/lablink-allocator-terraform.yml</code></p>"},{"location":"workflows/#purpose_3","title":"Purpose","text":"<p>Deploys LabLink infrastructure to AWS using Terraform.</p>"},{"location":"workflows/#triggers_3","title":"Triggers","text":"<ul> <li>Push to <code>test</code> branch: Automatic test deployment</li> <li>Workflow dispatch: Manual deployment (dev/test/prod)</li> <li>Repository dispatch: Programmatic deployment (for prod)</li> </ul>"},{"location":"workflows/#input-parameters-manual-dispatch_1","title":"Input Parameters (Manual Dispatch)","text":"Parameter Description Required Default <code>environment</code> Environment to deploy (<code>dev</code>, <code>test</code>, <code>prod</code>) Yes <code>dev</code> <code>image_tag</code> Docker image tag (required for prod) For prod only N/A <p>All deployments use the <code>lablink-infrastructure/</code> directory structure with configuration at <code>lablink-infrastructure/config/config.yaml</code>.</p>"},{"location":"workflows/#workflow-steps_1","title":"Workflow Steps","text":""},{"location":"workflows/#1-environment-determination","title":"1. Environment Determination","text":"<pre><code>Push to 'test' branch \u2192 env=test\nManual dispatch \u2192 env=&lt;user input&gt;\nRepository dispatch \u2192 env=&lt;payload&gt;\n</code></pre>"},{"location":"workflows/#2-aws-authentication","title":"2. AWS Authentication","text":"<p>Uses OpenID Connect (OIDC) to assume IAM role:</p> <pre><code>- name: Configure AWS credentials via OIDC\n  uses: aws-actions/configure-aws-credentials@v3\n  with:\n    role-to-assume: arn:aws:iam::711387140753:role/github_lablink_repository-AE68499B37C7\n    aws-region: us-west-2\n</code></pre> <p>No AWS credentials stored in GitHub!</p>"},{"location":"workflows/#3-terraform-initialization","title":"3. Terraform Initialization","text":"<pre><code># Dev (local state)\nterraform init\n\n# Test/Prod (remote state)\nterraform init -backend-config=backend-&lt;env&gt;.hcl\n</code></pre>"},{"location":"workflows/#4-validation","title":"4. Validation","text":"<pre><code>terraform fmt -check  # Check formatting\nterraform validate    # Validate syntax\n</code></pre>"},{"location":"workflows/#5-planning","title":"5. Planning","text":"<pre><code>terraform plan \\\n  -var=\"resource_suffix=&lt;env&gt;\" \\\n  -var=\"allocator_image_tag=&lt;tag&gt;\"\n</code></pre>"},{"location":"workflows/#6-application","title":"6. Application","text":"<pre><code>terraform apply -auto-approve \\\n  -var=\"resource_suffix=&lt;env&gt;\" \\\n  -var=\"allocator_image_tag=&lt;tag&gt;\"\n</code></pre>"},{"location":"workflows/#7-artifact-handling","title":"7. Artifact Handling","text":"<ul> <li>Extracts SSH private key from Terraform output</li> <li>Saves as artifact (expires in 1 day)</li> <li>Provides download link in workflow summary</li> </ul>"},{"location":"workflows/#8-failure-handling","title":"8. Failure Handling","text":"<p>If <code>terraform apply</code> fails:</p> <pre><code>terraform destroy -auto-approve\n</code></pre> <p>Automatically cleans up partial deployments.</p>"},{"location":"workflows/#example-workflow-run_2","title":"Example Workflow Run","text":"<p>Scenario: Deploy to production</p> <pre><code>1. Navigate to Actions \u2192 Terraform Deploy \u2192 Run workflow\n2. Select:\n   - Environment: prod\n   - Image tag: v1.0.0\n3. Workflow starts:\n   - Authenticates to AWS via OIDC\n   - Initializes Terraform with backend-prod.hcl\n   - Plans infrastructure\n   - Applies changes\n   - Saves SSH key to artifacts\n4. Deployment complete\n5. Outputs displayed in workflow summary:\n   - Allocator FQDN: lablink-prod.example.com\n   - EC2 Public IP: 54.xxx.xxx.xxx\n   - EC2 Key Name: lablink-prod-key\n</code></pre>"},{"location":"workflows/#customization_1","title":"Customization","text":"<p>To add deployment notifications:</p> <pre><code># Add at end of workflow\n- name: Notify Slack\n  if: success()\n  uses: slackapi/slack-github-action@v1\n  with:\n    webhook-url: ${{ secrets.SLACK_WEBHOOK }}\n    payload: |\n      {\n        \"text\": \"LabLink deployed to ${{ steps.setenv.outputs.env }}!\"\n      }\n</code></pre>"},{"location":"workflows/#destroy-workflow","title":"Destroy Workflow","text":"<p>File: <code>.github/workflows/lablink-allocator-destroy.yml</code></p>"},{"location":"workflows/#purpose_4","title":"Purpose","text":"<p>Safely destroy LabLink infrastructure for an environment.</p>"},{"location":"workflows/#triggers_4","title":"Triggers","text":"<ul> <li>Manual dispatch only: Requires explicit user action</li> </ul>"},{"location":"workflows/#input-parameters","title":"Input Parameters","text":"Parameter Description Required <code>environment</code> Environment to destroy (<code>dev</code>, <code>test</code>, <code>prod</code>) Yes"},{"location":"workflows/#safety-features","title":"Safety Features","text":"<ul> <li>Manual trigger only (no automatic destruction)</li> <li>Requires environment selection</li> <li>Shows plan before destroying</li> <li>Logs all destroyed resources</li> </ul>"},{"location":"workflows/#workflow-steps_2","title":"Workflow Steps","text":"<ol> <li>Authenticate to AWS via OIDC</li> <li>Initialize Terraform with correct backend</li> <li>Plan destruction</li> <li>Execute <code>terraform destroy -auto-approve</code></li> <li>Output destroyed resources</li> </ol>"},{"location":"workflows/#example-usage","title":"Example Usage","text":"<pre><code>1. Navigate to Actions \u2192 Allocator Master Destroy\n2. Click \"Run workflow\"\n3. Select environment: dev\n4. Confirm\n5. Workflow destroys:\n   - EC2 instance\n   - Security group\n   - SSH key pair\n6. Terraform state updated\n</code></pre> <p>Destructive Operation</p> <p>This action is irreversible. Ensure you have backups of any data before destroying.</p>"},{"location":"workflows/#infrastructure-testing-workflow","title":"Infrastructure Testing Workflow","text":"<p>File: <code>.github/workflows/client-vm-infrastructure-test.yml</code></p>"},{"location":"workflows/#purpose_5","title":"Purpose","text":"<p>End-to-end test of client VM creation and management.</p>"},{"location":"workflows/#triggers_5","title":"Triggers","text":"<ul> <li>Manual dispatch: On-demand testing</li> <li>Scheduled: Nightly/weekly regression tests (optional)</li> </ul>"},{"location":"workflows/#what-it-tests","title":"What It Tests","text":"<ol> <li>Allocator deployment</li> <li>Client VM spawning</li> <li>VM registration with allocator</li> <li>Health check reporting</li> <li>VM destruction</li> </ol>"},{"location":"workflows/#test-workflow","title":"Test Workflow","text":"<pre><code>1. Deploy test allocator\n2. Request client VM via API\n3. Wait for VM to be created and register\n4. Verify VM appears in allocator database\n5. Check VM health status\n6. Destroy client VM\n7. Destroy allocator\n8. Verify all resources cleaned up\n</code></pre>"},{"location":"workflows/#documentation-workflow","title":"Documentation Workflow","text":"<p>File: <code>.github/workflows/docs.yml</code></p>"},{"location":"workflows/#purpose_6","title":"Purpose","text":"<p>Builds and deploys MkDocs documentation to GitHub Pages.</p>"},{"location":"workflows/#triggers_6","title":"Triggers","text":"<ul> <li>Pushes to <code>main</code> branch</li> <li>Pull requests affecting <code>docs/**</code> or <code>mkdocs.yml</code></li> </ul>"},{"location":"workflows/#what-it-does","title":"What It Does","text":"<ol> <li>Installs Python and dependencies (including docs extras from <code>pyproject.toml</code>)</li> <li>Builds documentation with <code>mkdocs build</code></li> <li>Deploys to GitHub Pages branch (<code>gh-pages</code>)</li> </ol>"},{"location":"workflows/#deployment","title":"Deployment","text":"<p>Documentation is available at: <code>https://talmolab.github.io/lablink/</code></p>"},{"location":"workflows/#workflow-environment-variables","title":"Workflow Environment Variables","text":"<p>Common environment variables used across workflows:</p> Variable Description Source <code>GITHUB_TOKEN</code> GitHub API token Automatic <code>AWS_REGION</code> AWS region Hardcoded (us-west-2) <code>GITHUB_REPOSITORY</code> Repo name Automatic <code>GITHUB_REF_NAME</code> Branch/tag name Automatic"},{"location":"workflows/#secrets-management","title":"Secrets Management","text":""},{"location":"workflows/#required-secrets","title":"Required Secrets","text":"Secret Purpose Where Used None! OIDC handles AWS auth All AWS workflows"},{"location":"workflows/#optional-secrets","title":"Optional Secrets","text":"Secret Purpose How to Set <code>ADMIN_PASSWORD</code> Override admin password Settings \u2192 Secrets <code>DB_PASSWORD</code> Override DB password Settings \u2192 Secrets <code>SLACK_WEBHOOK</code> Notifications Settings \u2192 Secrets"},{"location":"workflows/#adding-secrets","title":"Adding Secrets","text":"<pre><code>1. Go to repository Settings\n2. Navigate to Secrets and variables \u2192 Actions\n3. Click \"New repository secret\"\n4. Name: ADMIN_PASSWORD\n5. Value: your-secure-password\n6. Click \"Add secret\"\n</code></pre> <p>Access in workflows:</p> <pre><code>- name: Use secret\n  env:\n    ADMIN_PASSWORD: ${{ secrets.ADMIN_PASSWORD }}\n  run: |\n    echo \"Password is set\"\n</code></pre>"},{"location":"workflows/#workflow-monitoring","title":"Workflow Monitoring","text":""},{"location":"workflows/#view-workflow-runs","title":"View Workflow Runs","text":"<ol> <li>Navigate to Actions tab in GitHub</li> <li>Select workflow from left sidebar</li> <li>View recent runs</li> </ol>"},{"location":"workflows/#workflow-status","title":"Workflow Status","text":"<ul> <li>\u2705 Green checkmark: Success</li> <li>\u274c Red X: Failure</li> <li>\ud83d\udfe1 Yellow dot: In progress</li> <li>\u26aa Gray circle: Queued</li> </ul>"},{"location":"workflows/#debugging-failed-workflows","title":"Debugging Failed Workflows","text":"<ol> <li>Click on failed workflow run</li> <li>Click on failed job</li> <li>Expand failed step</li> <li>Read error logs</li> <li>Fix issue and re-run</li> </ol>"},{"location":"workflows/#re-running-workflows","title":"Re-running Workflows","text":"<p>From workflow run page:</p> <ul> <li>Re-run all jobs: Retry entire workflow</li> <li>Re-run failed jobs: Only retry failures</li> </ul>"},{"location":"workflows/#creating-custom-workflows","title":"Creating Custom Workflows","text":""},{"location":"workflows/#example-backup-workflow","title":"Example: Backup Workflow","text":"<p>Create <code>.github/workflows/backup.yml</code>:</p> <pre><code>name: Backup Database\n\non:\n  schedule:\n    - cron: \"0 2 * * *\" # Daily at 2 AM\n  workflow_dispatch:\n\njobs:\n  backup:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Configure AWS\n        uses: aws-actions/configure-aws-credentials@v3\n        with:\n          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}\n          aws-region: us-west-2\n\n      - name: Backup Database\n        run: |\n          # SSH into allocator\n          # Run pg_dump\n          # Upload to S3\n          echo \"Backup complete\"\n</code></pre>"},{"location":"workflows/#example-notification-workflow","title":"Example: Notification Workflow","text":"<pre><code>name: Deployment Notifications\n\non:\n  workflow_run:\n    workflows: [\"Terraform Deploy\"]\n    types: [completed]\n\njobs:\n  notify:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Send Email\n        uses: dawidd6/action-send-mail@v3\n        with:\n          server_address: smtp.gmail.com\n          server_port: 465\n          username: ${{ secrets.EMAIL_USERNAME }}\n          password: ${{ secrets.EMAIL_PASSWORD }}\n          subject: \"LabLink Deployment: ${{ github.event.workflow_run.conclusion }}\"\n          body: \"Deployment finished with status: ${{ github.event.workflow_run.conclusion }}\"\n          to: admin@example.com\n</code></pre>"},{"location":"workflows/#best-practices","title":"Best Practices","text":"<ol> <li>Pin action versions: Use <code>@v3</code> not <code>@latest</code></li> <li>Minimize secrets: Use OIDC when possible</li> <li>Cache dependencies: Speed up workflows</li> <li>Fail fast: Stop on first error</li> <li>Use matrix builds: Test multiple versions</li> <li>Set timeouts: Prevent runaway workflows</li> <li>Add status badges: Show workflow status in README</li> </ol>"},{"location":"workflows/#status-badge-example","title":"Status Badge Example","text":"<p>Add to <code>README.md</code>:</p> <pre><code>![CI](https://github.com/talmolab/lablink/actions/workflows/ci.yml/badge.svg)\n![Deploy](https://github.com/talmolab/lablink/actions/workflows/lablink-allocator-terraform.yml/badge.svg)\n</code></pre>"},{"location":"workflows/#troubleshooting-workflows","title":"Troubleshooting Workflows","text":""},{"location":"workflows/#workflow-wont-trigger","title":"Workflow Won't Trigger","text":"<p>Check:</p> <ul> <li>Workflow file syntax (use YAML validator)</li> <li>Trigger conditions match your action</li> <li>Workflows enabled in repository settings</li> </ul>"},{"location":"workflows/#aws-authentication-fails","title":"AWS Authentication Fails","text":"<p>Check:</p> <ul> <li>IAM role ARN is correct</li> <li>Trust policy includes GitHub OIDC provider</li> <li>Role has necessary permissions</li> </ul>"},{"location":"workflows/#terraform-failures","title":"Terraform Failures","text":"<p>Check:</p> <ul> <li>Terraform syntax (<code>terraform validate</code>)</li> <li>AWS resource limits</li> <li>Terraform state lock status</li> </ul>"},{"location":"workflows/#image-push-fails","title":"Image Push Fails","text":"<p>Check:</p> <ul> <li>GHCR authentication (should be automatic)</li> <li>Image size limits</li> <li>Registry permissions</li> </ul>"},{"location":"workflows/#next-steps","title":"Next Steps","text":"<ul> <li>Deployment: Deploy using these workflows</li> <li>Security: Understand OIDC and secrets</li> <li>AWS Setup: Configure AWS for workflows</li> <li>Troubleshooting: Fix workflow issues</li> </ul>"},{"location":"reference/","title":"API Reference","text":"<p>Auto-generated API documentation from Python docstrings.</p>"},{"location":"reference/#allocator-service","title":"Allocator Service","text":"<p>API documentation for the LabLink allocator service.</p> <p>Browse Allocator API \u2192</p>"},{"location":"reference/#client-service","title":"Client Service","text":"<p>API documentation for the LabLink client service.</p> <p>Browse Client API \u2192</p>"},{"location":"reference/allocator/SUMMARY/","title":"SUMMARY","text":"<ul> <li>lablink_allocator_service<ul> <li>conf<ul> <li>structured_config</li> </ul> </li> <li>database</li> <li>generate_init_sql</li> <li>get_config</li> <li>main</li> <li>scheduler</li> <li>utils<ul> <li>aws_utils</li> <li>config_helpers</li> <li>scp</li> <li>terraform_utils</li> </ul> </li> <li>validate_config</li> </ul> </li> </ul>"},{"location":"reference/allocator/lablink_allocator_service/","title":"lablink_allocator_service","text":""},{"location":"reference/allocator/lablink_allocator_service/#lablink_allocator_service","title":"<code>lablink_allocator_service</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/database/","title":"lablink_allocator_service.database","text":""},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database","title":"<code>lablink_allocator_service.database</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database-attributes","title":"Attributes","text":""},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database-classes","title":"Classes","text":""},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase","title":"<code>PostgresqlDatabase(dbname, user, password, host, port, table_name, message_channel)</code>","text":"<p>Class to interact with a PostgreSQL database. This class provides methods to connect to the database, insert data, retrieve data, and listen for notifications.</p> <p>Initialize the database connection. Args:     dbname (str): The name of the database.     user (str): The username to connect to the database.     password (str): The password for the user.     host (str): The host where the database is located.     port (int): The port number for the database connection.     table_name (str): The name of the table to interact with.     message_channel (str): The name of the message channel to listen to.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/database.py</code> <pre><code>def __init__(\n    self,\n    dbname: str,\n    user: str,\n    password: str,\n    host: str,\n    port: int,\n    table_name: str,\n    message_channel: str,\n):\n    \"\"\"Initialize the database connection.\n    Args:\n        dbname (str): The name of the database.\n        user (str): The username to connect to the database.\n        password (str): The password for the user.\n        host (str): The host where the database is located.\n        port (int): The port number for the database connection.\n        table_name (str): The name of the table to interact with.\n        message_channel (str): The name of the message channel to listen to.\n    \"\"\"\n    self.dbname = dbname\n    self.user = user\n    self.password = password\n    self.host = host\n    self.port = port\n    self.table_name = table_name\n    self.message_channel = message_channel\n\n    # Connect to the PostgreSQL database\n    self.conn = psycopg2.connect(\n        dbname=dbname,\n        user=user,\n        password=password,\n        host=host,\n        port=port,\n    )\n\n    # Set the isolation level to autocommit\n    self.conn.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)\n    self.cursor = self.conn.cursor()\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase-attributes","title":"Attributes","text":""},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.conn","title":"<code>conn = psycopg2.connect(dbname=dbname, user=user, password=password, host=host, port=port)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.cursor","title":"<code>cursor = self.conn.cursor()</code>  <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.dbname","title":"<code>dbname = dbname</code>  <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.host","title":"<code>host = host</code>  <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.message_channel","title":"<code>message_channel = message_channel</code>  <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.password","title":"<code>password = password</code>  <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.port","title":"<code>port = port</code>  <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.table_name","title":"<code>table_name = table_name</code>  <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.user","title":"<code>user = user</code>  <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase-functions","title":"Functions","text":""},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.assign_vm","title":"<code>assign_vm(email, crd_command, pin)</code>","text":"<p>Assign a VM to a user.</p> <p>Parameters:</p> Name Type Description Default <code>email</code> <code>str</code> <p>The email of the user.</p> required <code>crd_command</code> <code>str</code> <p>The CRD command to assign.</p> required <code>pin</code> <code>str</code> <p>The PIN for the VM.</p> required Source code in <code>packages/allocator/src/lablink_allocator_service/database.py</code> <pre><code>def assign_vm(self, email, crd_command, pin) -&gt; None:\n    \"\"\"Assign a VM to a user.\n\n    Args:\n        email (str): The email of the user.\n        crd_command (str): The CRD command to assign.\n        pin (str): The PIN for the VM.\n    \"\"\"\n    # Gets the first available VM that is not in use\n    hostname = self.get_first_available_vm()\n\n    # Check if a VM is available\n    if not hostname:\n        logger.error(\"No available VMs found to assign.\")\n        raise ValueError(\"No available VMs to assign.\")\n\n    # SQL query to update the VM record with the user's email, CRD command, and pin\n    query = f\"\"\"\n    UPDATE {self.table_name}\n    SET useremail = %s, crdcommand = %s, pin = %s, inuse = FALSE, healthy = NULL\n    WHERE hostname = %s;\n    \"\"\"\n    try:\n        self.cursor.execute(query, (email, crd_command, pin, hostname))\n        self.conn.commit()\n        logger.debug(f\"Assigned VM '{hostname}' to user '{email}'.\")\n    except Exception as e:\n        logger.error(f\"Error assigning VM: {e}\")\n        self.conn.rollback()\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.cancel_scheduled_destruction","title":"<code>cancel_scheduled_destruction(schedule_id)</code>","text":"<p>Cancel a scheduled destruction.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/database.py</code> <pre><code>def cancel_scheduled_destruction(self, schedule_id: int) -&gt; None:\n    \"\"\"Cancel a scheduled destruction.\"\"\"\n    query = \"UPDATE scheduled_destructions SET status = 'cancelled' WHERE id = %s;\"\n    self.cursor.execute(query, (schedule_id,))\n    self.conn.commit()\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.clear_database","title":"<code>clear_database()</code>","text":"<p>Delete all VMs from the table.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/database.py</code> <pre><code>def clear_database(self) -&gt; None:\n    \"\"\"Delete all VMs from the table.\"\"\"\n    query = f\"DELETE FROM {self.table_name};\"\n    try:\n        self.cursor.execute(query)\n        self.conn.commit()\n        logger.debug(\"All VMs deleted from the table.\")\n    except Exception as e:\n        logger.error(f\"Error deleting VMs: {e}\")\n        self.conn.rollback()\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.create_scheduled_destruction","title":"<code>create_scheduled_destruction(schedule_name, destruction_time, recurrence_rule=None, created_by=None, notification_enabled=True, notification_hours_before=1)</code>","text":"<p>Create a scheduled destruction entry and return its ID.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a schedule with the same name already exists</p> <code>RuntimeError</code> <p>If database operation fails</p> Source code in <code>packages/allocator/src/lablink_allocator_service/database.py</code> <pre><code>def create_scheduled_destruction(\n    self,\n    schedule_name: str,\n    destruction_time: datetime,\n    recurrence_rule: str = None,\n    created_by: str = None,\n    notification_enabled: bool = True,\n    notification_hours_before: int = 1,\n) -&gt; int:\n    \"\"\"Create a scheduled destruction entry and return its ID.\n\n    Raises:\n        ValueError: If a schedule with the same name already exists\n        RuntimeError: If database operation fails\n    \"\"\"\n    query = \"\"\"\n        INSERT INTO scheduled_destructions\n        (schedule_name, destruction_time, recurrence_rule, created_by,\n        notification_enabled, notification_hours_before, status)\n        VALUES (%s, %s, %s, %s, %s, %s, 'scheduled')\n        RETURNING id;\n    \"\"\"\n    try:\n        self.cursor.execute(\n            query,\n            (\n                schedule_name,\n                self._naive_utc(destruction_time),\n                recurrence_rule,\n                created_by,\n                notification_enabled,\n                notification_hours_before,\n            ),\n        )\n        destruction_id = self.cursor.fetchone()[0]\n        self.conn.commit()\n        logger.debug(\n            f\"Created scheduled destruction '{schedule_name}' \"\n            f\"with ID {destruction_id}.\"\n        )\n        return destruction_id\n\n    except psycopg2.IntegrityError as e:\n        self.conn.rollback()\n        if 'schedule_name' in str(e) or 'unique constraint' in str(e).lower():\n            error_msg = f\"A schedule with the name '{schedule_name}' already exists\"\n            logger.warning(error_msg)\n            raise ValueError(error_msg) from e\n        else:\n            logger.error(f\"Database integrity error: {e}\")\n            raise RuntimeError(f\"Database integrity error: {e}\") from e\n\n    except Exception as e:\n        self.conn.rollback()\n        logger.error(f\"Error creating scheduled destruction: {e}\")\n        raise RuntimeError(f\"Failed to create scheduled destruction: {e}\") from e\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.get_all_scheduled_destructions","title":"<code>get_all_scheduled_destructions(status=None)</code>","text":"<p>Get all scheduled destructions, optionally filtered by status.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/database.py</code> <pre><code>def get_all_scheduled_destructions(\n    self, status: Optional[str] = None\n) -&gt; List[dict]:\n    \"\"\"Get all scheduled destructions, optionally filtered by status.\"\"\"\n    columns = [\n        \"id\",\n        \"schedule_name\",\n        \"destruction_time\",\n        \"recurrence_rule\",\n        \"created_by\",\n        \"status\",\n        \"execution_count\",\n        \"last_execution_time\",\n        \"last_execution_result\",\n        \"notification_enabled\",\n        \"notification_hours_before\",\n        \"created_at\",\n        \"updated_at\",\n    ]\n\n    if status:\n        query = (\n            \"SELECT * FROM scheduled_destructions \"\n            \"WHERE status = %s ORDER BY destruction_time;\"\n        )\n        self.cursor.execute(query, (status,))\n    else:\n        query = \"SELECT * FROM scheduled_destructions ORDER BY destruction_time;\"\n        self.cursor.execute(query)\n\n    return [dict(zip(columns, row)) for row in self.cursor.fetchall()]\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.get_all_vm_status","title":"<code>get_all_vm_status()</code>","text":"<p>Get the status of all VMs in the table.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the hostname and status of each VM.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/database.py</code> <pre><code>def get_all_vm_status(self) -&gt; dict:\n    \"\"\"Get the status of all VMs in the table.\n\n    Returns:\n        dict: A dictionary containing the hostname and status of each VM.\n    \"\"\"\n    query = f\"SELECT hostname, status FROM {self.table_name};\"\n    try:\n        self.cursor.execute(query)\n        rows = self.cursor.fetchall()\n        return {row[0]: row[1] for row in rows}\n    except Exception as e:\n        logger.error(f\"Error retrieving all VM status: {e}\")\n        return None\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.get_all_vms","title":"<code>get_all_vms()</code>","text":"<p>Get all VMs from the table, excluding logs.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of all VMs in the table in the form of dictionaries.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/database.py</code> <pre><code>def get_all_vms(self) -&gt; list:\n    \"\"\"Get all VMs from the table, excluding logs.\n\n    Returns:\n        list: A list of all VMs in the table in the form of dictionaries.\n    \"\"\"\n    column_names = [col for col in self.get_column_names() if col != \"logs\"]\n    query_columns = \", \".join(column_names)\n    self.cursor.execute(f\"SELECT {query_columns} FROM {self.table_name};\")\n    rows = self.cursor.fetchall()\n    return [dict(zip(column_names, row)) for row in rows]\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.get_assigned_vms","title":"<code>get_assigned_vms()</code>","text":"<p>Get the VMs that are assigned to a command.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of VMs that are assigned to a command.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/database.py</code> <pre><code>def get_assigned_vms(self) -&gt; list:\n    \"\"\"Get the VMs that are assigned to a command.\n\n    Returns:\n        list: A list of VMs that are assigned to a command.\n    \"\"\"\n    query = f\"SELECT hostname FROM {self.table_name} WHERE crdcommand IS NOT NULL\"\n    try:\n        self.cursor.execute(query)\n        return [row[0] for row in self.cursor.fetchall()]\n    except Exception as e:\n        logger.error(f\"Error retrieving assigned VMs: {e}\")\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.get_column_names","title":"<code>get_column_names(table_name=None)</code>","text":"<p>Get the column names of a table.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>The name of the table.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of column names.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/database.py</code> <pre><code>def get_column_names(self, table_name=None) -&gt; list:\n    \"\"\"Get the column names of a table.\n\n    Args:\n        table_name (str): The name of the table.\n\n    Returns:\n        list: A list of column names.\n    \"\"\"\n    if table_name is None:\n        table_name = self.table_name\n\n    # Query to get the column names from the information schema\n    self.cursor.execute(\n        \"SELECT column_name FROM information_schema.columns WHERE table_name = %s\",\n        (table_name,),\n    )\n    return [row[0] for row in self.cursor.fetchall()]\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.get_crd_command","title":"<code>get_crd_command(hostname)</code>","text":"<p>Get the command assigned to a VM.</p> <p>Parameters:</p> Name Type Description Default <code>hostname</code> <code>str</code> <p>The hostname of the VM.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The command assigned to the VM.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/database.py</code> <pre><code>def get_crd_command(self, hostname) -&gt; str:\n    \"\"\"Get the command assigned to a VM.\n\n    Args:\n        hostname (str): The hostname of the VM.\n\n    Returns:\n        str: The command assigned to the VM.\n    \"\"\"\n    if not self.vm_exists(hostname):\n        logger.error(f\"VM with hostname '{hostname}' does not exist.\")\n        return None\n\n    query = f\"SELECT crdcommand FROM {self.table_name} WHERE hostname = %s\"\n    self.cursor.execute(query, (hostname,))\n    return self.cursor.fetchone()[0]\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.get_first_available_vm","title":"<code>get_first_available_vm()</code>","text":"<p>Get the first available VM that is not assigned.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The hostname of the first available VM.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/database.py</code> <pre><code>def get_first_available_vm(self) -&gt; str:\n    \"\"\"Get the first available VM that is not assigned.\n\n    Returns:\n        str: The hostname of the first available VM.\n    \"\"\"\n    query = (\n        f\"SELECT hostname FROM {self.table_name} WHERE useremail IS NULL AND \"\n        f\"status = 'running' LIMIT 1\"\n    )\n    self.cursor.execute(query)\n    row = self.cursor.fetchone()\n    return row[0] if row else None\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.get_gpu_health","title":"<code>get_gpu_health(hostname)</code>","text":"<p>Get the GPU health status of a VM.</p> <p>Parameters:</p> Name Type Description Default <code>hostname</code> <code>str</code> <p>The hostname of the VM.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The health status of the GPU for the specified VM or None if not found.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/database.py</code> <pre><code>def get_gpu_health(self, hostname: str) -&gt; str:\n    \"\"\"Get the GPU health status of a VM.\n\n    Args:\n        hostname (str): The hostname of the VM.\n\n    Returns:\n        str: The health status of the GPU for the specified VM or None if not found.\n    \"\"\"\n    query = f\"SELECT healthy FROM {self.table_name} WHERE hostname = %s;\"\n    try:\n        self.cursor.execute(query, (hostname,))\n        result = self.cursor.fetchone()\n        if result:\n            return result[0]\n        else:\n            logger.error(f\"No VM found with hostname '{hostname}'.\")\n            return None\n    except Exception as e:\n        logger.error(f\"Error retrieving GPU health: {e}\")\n        return None\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.get_row_count","title":"<code>get_row_count()</code>","text":"<p>Get the number of rows in the table. Returns:     int: The number of rows in the table.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/database.py</code> <pre><code>def get_row_count(self) -&gt; int:\n    \"\"\"Get the number of rows in the table.\n    Returns:\n        int: The number of rows in the table.\n    \"\"\"\n    self.cursor.execute(f\"SELECT COUNT(*) FROM {self.table_name};\")\n    return self.cursor.fetchone()[0]\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.get_scheduled_destruction","title":"<code>get_scheduled_destruction(schedule_id)</code>","text":"<p>Get scheduled destruction by ID.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/database.py</code> <pre><code>def get_scheduled_destruction(self, schedule_id: int) -&gt; Optional[dict]:\n    \"\"\"Get scheduled destruction by ID.\"\"\"\n    query = \"SELECT * FROM scheduled_destructions WHERE id = %s;\"\n    self.cursor.execute(query, (schedule_id,))\n    row = self.cursor.fetchone()\n    if row:\n        columns = [\n            \"id\",\n            \"schedule_name\",\n            \"destruction_time\",\n            \"recurrence_rule\",\n            \"created_by\",\n            \"status\",\n            \"execution_count\",\n            \"last_execution_time\",\n            \"last_execution_result\",\n            \"notification_enabled\",\n            \"notification_hours_before\",\n            \"created_at\",\n            \"updated_at\",\n        ]\n        return dict(zip(columns, row))\n    return None\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.get_status_by_hostname","title":"<code>get_status_by_hostname(hostname)</code>","text":"<p>Get the status of a VM by its hostname.</p> <p>Parameters:</p> Name Type Description Default <code>hostname</code> <code>str</code> <p>The hostname of the VM.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The status of the VM, or None if not found.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/database.py</code> <pre><code>def get_status_by_hostname(self, hostname: str) -&gt; str:\n    \"\"\"Get the status of a VM by its hostname.\n\n    Args:\n        hostname (str): The hostname of the VM.\n\n    Returns:\n        str: The status of the VM, or None if not found.\n    \"\"\"\n    query = f\"SELECT status FROM {self.table_name} WHERE hostname = %s;\"\n    try:\n        self.cursor.execute(query, (hostname,))\n        result = self.cursor.fetchone()\n        if result:\n            return result[0]\n        else:\n            logger.error(f\"No VM found with hostname '{hostname}'.\")\n            return None\n    except Exception as e:\n        logger.error(f\"Error retrieving status: {e}\")\n        return None\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.get_unassigned_vms","title":"<code>get_unassigned_vms()</code>","text":"<p>Get the VMs that are not assigned to any command.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of VMs that are not assigned to any command.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/database.py</code> <pre><code>def get_unassigned_vms(self) -&gt; list:\n    \"\"\"Get the VMs that are not assigned to any command.\n\n    Returns:\n        list: A list of VMs that are not assigned to any command.\n    \"\"\"\n    query = (\n        f\"SELECT hostname FROM {self.table_name} WHERE \"\n        f\"crdcommand IS NULL AND status = 'running'\"\n    )\n    try:\n        self.cursor.execute(query)\n        return [row[0] for row in self.cursor.fetchall()]\n    except Exception as e:\n        logger.error(f\"Error retrieving unassigned VMs: {e}\")\n        return []\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.get_vm_by_hostname","title":"<code>get_vm_by_hostname(hostname)</code>","text":"<p>Get a VM by its hostname.</p> <p>Parameters:</p> Name Type Description Default <code>hostname</code> <code>str</code> <p>The hostname of the VM.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the VM details without logs.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/database.py</code> <pre><code>def get_vm_by_hostname(self, hostname: str) -&gt; dict:\n    \"\"\"Get a VM by its hostname.\n\n    Args:\n        hostname (str): The hostname of the VM.\n\n    Returns:\n        dict: A dictionary containing the VM details without logs.\n    \"\"\"\n    query = f\"SELECT * FROM {self.table_name} WHERE hostname = %s;\"\n    self.cursor.execute(query, (hostname,))\n    row = self.cursor.fetchone()\n    if row:\n        return {\n            \"hostname\": row[0],\n            \"pin\": row[1],\n            \"crdcommand\": row[2],\n            \"useremail\": row[3],\n            \"inuse\": row[4],\n            \"healthy\": row[5],\n            \"status\": row[6],\n            \"terraform_apply_start_time\": row[8],\n            \"terraform_apply_end_time\": row[9],\n            \"terraform_apply_duration_seconds\": row[10],\n            \"cloud_init_start_time\": row[11],\n            \"cloud_init_end_time\": row[12],\n            \"cloud_init_duration_seconds\": row[13],\n            \"container_start_time\": row[14],\n            \"container_end_time\": row[15],\n            \"container_startup_duration_seconds\": row[16],\n            \"total_startup_duration_seconds\": row[17],\n            \"created_at\": row[18],\n        }\n    else:\n        logger.error(f\"No VM found with hostname '{hostname}'.\")\n        return None\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.get_vm_details","title":"<code>get_vm_details(email)</code>","text":"<p>Get VM details based on the email provided.</p> <p>Parameters:</p> Name Type Description Default <code>email</code> <code>str</code> <p>The email of the user.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list containing the hostname, pin, and CRD command of the VM</p> <code>list</code> <p>assigned to the given user.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/database.py</code> <pre><code>def get_vm_details(self, email: str) -&gt; list:\n    \"\"\"Get VM details based on the email provided.\n\n    Args:\n        email (str): The email of the user.\n\n    Returns:\n        list: A list containing the hostname, pin, and CRD command of the VM\n        assigned to the given user.\n    \"\"\"\n    query = (\n        f\"SELECT hostname, pin, crdcommand FROM {self.table_name}\"\n        \" WHERE useremail = %s\"\n    )\n    self.cursor.execute(query, (email,))\n    row = self.cursor.fetchone()\n    if row:\n        hostname, pin, crdcommand = row\n        return [\n            hostname,\n            pin,\n            crdcommand,\n        ]\n    else:\n        raise ValueError(f\"No VM found for email in the database: {email}\")\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.get_vm_logs","title":"<code>get_vm_logs(hostname)</code>","text":"<p>Get the logs of a VM by its hostname.</p> <p>Parameters:</p> Name Type Description Default <code>hostname</code> <code>str</code> <p>The hostname of the VM.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The logs of the VM, or None if not found.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/database.py</code> <pre><code>def get_vm_logs(self, hostname: str) -&gt; str:\n    \"\"\"Get the logs of a VM by its hostname.\n\n    Args:\n        hostname (str): The hostname of the VM.\n\n    Returns:\n        str: The logs of the VM, or None if not found.\n    \"\"\"\n    query = f\"SELECT logs FROM {self.table_name} WHERE hostname = %s;\"\n    try:\n        self.cursor.execute(query, (hostname,))\n        result = self.cursor.fetchone()\n        if result:\n            return result[0]\n        else:\n            logger.error(f\"No VM found with hostname '{hostname}'.\")\n            return None\n    except Exception as e:\n        logger.error(f\"Error retrieving logs: {e}\")\n        return None\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.insert_vm","title":"<code>insert_vm(hostname)</code>","text":"<p>Insert a new row into the table.</p> <p>Parameters:</p> Name Type Description Default <code>hostname</code> <code>str</code> <p>The hostname of the VM.</p> required Source code in <code>packages/allocator/src/lablink_allocator_service/database.py</code> <pre><code>def insert_vm(self, hostname) -&gt; None:\n    \"\"\"Insert a new row into the table.\n\n    Args:\n        hostname (str): The hostname of the VM.\n    \"\"\"\n    column_names = self.get_column_names()\n\n    values = []\n\n    for col in column_names:\n        # Find the column that corresponds to the hostname and set its value\n        if col == \"hostname\":\n            values.append(hostname)\n        elif col == \"inuse\":\n            values.append(False)\n        else:\n            values.append(None)\n\n    # Construct the SQL query\n    columns = \", \".join(column_names)\n    placeholders = \", \".join([\"%s\" for _ in column_names])\n\n    try:\n        sql = f\"INSERT INTO {self.table_name} ({columns}) VALUES ({placeholders});\"\n        self.cursor.execute(sql, values)\n        self.conn.commit()\n        logger.debug(f\"Inserted data: {values}\")\n    except Exception as e:\n        logger.error(f\"Error inserting data: {e}\")\n        self.conn.rollback()\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.listen_for_notifications","title":"<code>listen_for_notifications(channel, target_hostname)</code>","text":"<p>Listen for notifications on a specific channel.</p> <p>Parameters:</p> Name Type Description Default <code>channel</code> <code>str</code> <p>The name of the notification channel.</p> required <code>target_hostname</code> <code>str</code> <p>The hostname of the VM to connect to.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the status, pin, and command.</p> <p>Raises:</p> Type Description <code>Error</code> <p>If there is an error in the database.</p> <code>JSONDecodeError</code> <p>If there is an error decoding the JSON payload.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/database.py</code> <pre><code>def listen_for_notifications(self, channel, target_hostname) -&gt; dict:\n    \"\"\"Listen for notifications on a specific channel.\n\n    Args:\n        channel (str): The name of the notification channel.\n        target_hostname (str): The hostname of the VM to connect to.\n\n    Returns:\n        dict: A dictionary containing the status, pin, and command.\n\n    Raises:\n        psycopg2.Error: If there is an error in the database.\n        json.JSONDecodeError: If there is an error decoding the JSON payload.\n    \"\"\"\n\n    # Create a new connection to listen for notifications\n    logger.debug(\"Creating new connection to listen for notifications...\")\n    listen_conn = psycopg2.connect(\n        dbname=self.dbname,\n        user=self.user,\n        password=self.password,\n        host=self.host,\n        port=self.port,\n    )\n    listen_conn.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)\n    listen_cursor = listen_conn.cursor()\n\n    # Infinite loop to wait for notifications\n    try:\n        listen_cursor.execute(f\"LISTEN {channel};\")\n        logger.debug(f\"Listening for notifications on '{channel}'...\")\n\n        while True:\n            # Wait for notifications\n            if select.select([listen_conn], [], [], 10) == ([], [], []):\n                continue\n            else:\n                listen_conn.poll()  # Process any pending notifications\n                while listen_conn.notifies:\n                    notify = listen_conn.notifies.pop(0)\n                    logger.debug(\n                        f\"Received notification: {notify.payload} from \"\n                        f\"channel {notify.channel}\"\n                    )\n                    # Parse the JSON payload\n                    try:\n                        payload_data = json.loads(notify.payload)\n                        logger.debug(f\"Payload data: {payload_data}\")\n                        hostname = payload_data.get(\"HostName\")\n                        pin = payload_data.get(\"Pin\")\n                        command = payload_data.get(\"CrdCommand\")\n\n                        if hostname is None or pin is None or command is None:\n                            logger.error(\n                                \"Invalid payload data. Missing required fields.\"\n                            )\n                            continue\n\n                        # Check if the hostname matches the current hostname\n                        if hostname != target_hostname:\n                            logger.debug(\n                                f\"Hostname '{hostname}' does not match the current\"\n                                f\"hostname '{target_hostname}'.\"\n                            )\n                            continue\n\n                        logger.debug(\n                            \"Chrome Remote Desktop connected successfully.\"\n                        )\n                        return {\n                            \"status\": \"success\",\n                            \"pin\": pin,\n                            \"command\": command,\n                        }\n\n                    except json.JSONDecodeError as e:\n                        logger.error(f\"Error decoding JSON payload: {e}\")\n                        continue\n                    except Exception as e:\n                        logger.error(f\"Error processing notification: {e}\")\n                        continue\n    finally:\n        # Close the listener connection\n        listen_cursor.close()\n        listen_conn.close()\n        logger.debug(\"Listener connection closed.\")\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.load_database","title":"<code>load_database(dbname, user, password, host, port, table_name, message_channel)</code>  <code>classmethod</code>","text":"<p>Loads an existing database from PostgreSQL.</p> <p>Parameters:</p> Name Type Description Default <code>dbname</code> <code>str</code> <p>The name of the database.</p> required <code>user</code> <code>str</code> <p>The username to connect to the database.</p> required <code>password</code> <code>str</code> <p>The password for the user.</p> required <code>host</code> <code>str</code> <p>The host where the database is located.</p> required <code>port</code> <code>int</code> <p>The port number for the database connection.</p> required <code>table_name</code> <code>str</code> <p>The name of the table to interact with.</p> required <code>message_channel</code> <code>str</code> <p>The name of the message channel to listen to.</p> required <p>Returns:</p> Name Type Description <code>PostgresqlDatabase</code> <code>PostgresqlDatabase</code> <p>An instance of the PostgresqlDatabase class.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/database.py</code> <pre><code>@classmethod\ndef load_database(\n    cls, dbname, user, password, host, port, table_name, message_channel\n) -&gt; \"PostgresqlDatabase\":\n    \"\"\"Loads an existing database from PostgreSQL.\n\n    Args:\n        dbname (str): The name of the database.\n        user (str): The username to connect to the database.\n        password (str): The password for the user.\n        host (str): The host where the database is located.\n        port (int): The port number for the database connection.\n        table_name (str): The name of the table to interact with.\n        message_channel (str): The name of the message channel to listen to.\n\n    Returns:\n        PostgresqlDatabase: An instance of the PostgresqlDatabase class.\n    \"\"\"\n    return cls(dbname, user, password, host, port, table_name, message_channel)\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.save_logs_by_hostname","title":"<code>save_logs_by_hostname(hostname, logs)</code>","text":"<p>Save logs for a VM by its hostname.</p> <p>Parameters:</p> Name Type Description Default <code>hostname</code> <code>str</code> <p>The hostname of the VM.</p> required <code>logs</code> <code>str</code> <p>The logs to save for the VM.</p> required Source code in <code>packages/allocator/src/lablink_allocator_service/database.py</code> <pre><code>def save_logs_by_hostname(self, hostname: str, logs: str) -&gt; None:\n    \"\"\"Save logs for a VM by its hostname.\n\n    Args:\n        hostname (str): The hostname of the VM.\n        logs (str): The logs to save for the VM.\n    \"\"\"\n    query = f\"UPDATE {self.table_name} SET logs = %s WHERE hostname = %s;\"\n    try:\n        self.cursor.execute(query, (logs, hostname))\n        self.conn.commit()\n        logger.debug(f\"Saved logs for VM '{hostname}'.\")\n    except Exception as e:\n        logger.error(f\"Error saving logs: {e}\")\n        self.conn.rollback()\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.update_health","title":"<code>update_health(hostname, healthy)</code>","text":"<p>Modify the health status of a VM.</p> <p>Parameters:</p> Name Type Description Default <code>hostname</code> <code>str</code> <p>The hostname of the VM.</p> required <code>healthy</code> <code>str</code> <p>The health status to set for the VM.</p> required Source code in <code>packages/allocator/src/lablink_allocator_service/database.py</code> <pre><code>def update_health(self, hostname: str, healthy: str) -&gt; None:\n    \"\"\"Modify the health status of a VM.\n\n    Args:\n        hostname (str): The hostname of the VM.\n        healthy (str): The health status to set for the VM.\n    \"\"\"\n    query = f\"UPDATE {self.table_name} SET healthy = %s WHERE hostname = %s;\"\n    try:\n        self.cursor.execute(query, (healthy, hostname))\n        self.conn.commit()\n        logger.debug(f\"Updated health status for VM '{hostname}' to {healthy}.\")\n    except Exception as e:\n        logger.error(f\"Error updating health status: {e}\")\n        self.conn.rollback()\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.update_scheduled_destruction_status","title":"<code>update_scheduled_destruction_status(schedule_id, status, execution_result=None)</code>","text":"<p>Update destruction execution status.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/database.py</code> <pre><code>def update_scheduled_destruction_status(\n    self,\n    schedule_id: int,\n    status: str,\n    execution_result: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Update destruction execution status.\"\"\"\n    query = \"\"\"\n        UPDATE scheduled_destructions\n        SET status = %s,\n            execution_count = execution_count + 1,\n            last_execution_time = NOW(),\n            last_execution_result = %s\n        WHERE id = %s;\n    \"\"\"\n    self.cursor.execute(query, (status, execution_result, schedule_id))\n    self.conn.commit()\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.update_terraform_timing","title":"<code>update_terraform_timing(hostname, per_instance_seconds, per_instance_start_time, per_instance_end_time)</code>","text":"<p>Update the Terraform timing metrics for a VM. Args:     hostname (str): The hostname of the VM.     per_instance_seconds (float): The total startup duration in seconds.     per_instance_start_time (datetime): The start time of the Terraform apply.     per_instance_end_time (datetime): The end time of the Terraform apply.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/database.py</code> <pre><code>def update_terraform_timing(\n    self,\n    hostname: str,\n    per_instance_seconds: float,\n    per_instance_start_time: datetime,\n    per_instance_end_time: datetime,\n) -&gt; None:\n    \"\"\"Update the Terraform timing metrics for a VM.\n    Args:\n        hostname (str): The hostname of the VM.\n        per_instance_seconds (float): The total startup duration in seconds.\n        per_instance_start_time (datetime): The start time of the Terraform apply.\n        per_instance_end_time (datetime): The end time of the Terraform apply.\n    \"\"\"\n\n    query = f\"\"\"\n        INSERT INTO {self.table_name} (\n            hostname,\n            terraformapplydurationseconds,\n            terraformapplystarttime,\n            terraformapplyendtime\n        )\n        VALUES (%s, %s, %s, %s)\n        ON CONFLICT (hostname) DO UPDATE\n        SET terraformapplydurationseconds = EXCLUDED.terraformapplydurationseconds,\n            terraformapplystarttime = EXCLUDED.terraformapplystarttime,\n            terraformapplyendtime = EXCLUDED.terraformapplyendtime\n    \"\"\"\n    with self.conn.cursor() as cursor:\n        try:\n            cursor.execute(\n                query,\n                (\n                    hostname,\n                    per_instance_seconds,\n                    self._naive_utc(per_instance_start_time),\n                    self._naive_utc(per_instance_end_time),\n                ),\n            )\n            self.conn.commit()\n            logger.debug(\n                f\"Updated Terraform timing for VM '{hostname}': \"\n                f\"{per_instance_seconds}s.\"\n            )\n        except Exception as e:\n            logger.error(f\"Error updating Terraform timing: {e}\")\n            self.conn.rollback()\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.update_vm_in_use","title":"<code>update_vm_in_use(hostname, in_use)</code>","text":"<p>Update the in-use status of a VM.</p> <p>Parameters:</p> Name Type Description Default <code>hostname</code> <code>str</code> <p>The hostname of the VM.</p> required <code>in_use</code> <code>bool</code> <p>The in-use status to set.</p> required Source code in <code>packages/allocator/src/lablink_allocator_service/database.py</code> <pre><code>def update_vm_in_use(self, hostname: str, in_use: bool) -&gt; None:\n    \"\"\"Update the in-use status of a VM.\n\n    Args:\n        hostname (str): The hostname of the VM.\n        in_use (bool): The in-use status to set.\n    \"\"\"\n    query = f\"UPDATE {self.table_name} SET inuse = %s WHERE hostname = %s\"\n    try:\n        self.cursor.execute(query, (in_use, hostname))\n        self.conn.commit()\n        logger.debug(f\"Updated VM '{hostname}' in-use status to {in_use}.\")\n    except Exception as e:\n        logger.error(f\"Error updating VM in-use status: {e}\")\n        self.conn.rollback()\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.update_vm_metrics_atomic","title":"<code>update_vm_metrics_atomic(hostname, metrics)</code>","text":"<p>Update VM metrics and calculate total startup time in a single transaction.</p> <p>Parameters:</p> Name Type Description Default <code>hostname</code> <code>str</code> <p>The hostname of the VM.</p> required <code>metrics</code> <code>dict</code> <p>A dictionary containing the timing metrics to update. Supported keys: - cloud_init_start (int): Unix timestamp - cloud_init_end (int): Unix timestamp - cloud_init_duration_seconds (float) - container_start (int): Unix timestamp - container_end (int): Unix timestamp - container_startup_duration_seconds (float)</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If the database operation fails (re-raised after rollback).</p> Source code in <code>packages/allocator/src/lablink_allocator_service/database.py</code> <pre><code>def update_vm_metrics_atomic(self, hostname: str, metrics: dict) -&gt; None:\n    \"\"\"Update VM metrics and calculate total startup time in a single transaction.\n\n    Args:\n        hostname (str): The hostname of the VM.\n        metrics (dict): A dictionary containing the timing metrics to update.\n            Supported keys:\n            - cloud_init_start (int): Unix timestamp\n            - cloud_init_end (int): Unix timestamp\n            - cloud_init_duration_seconds (float)\n            - container_start (int): Unix timestamp\n            - container_end (int): Unix timestamp\n            - container_startup_duration_seconds (float)\n\n    Raises:\n        Exception: If the database operation fails (re-raised after rollback).\n    \"\"\"\n    updates = []\n    values = []\n\n    # Build metric updates\n    if \"cloud_init_start\" in metrics:\n        updates.append(\"CloudInitStartTime = to_timestamp(%s)\")\n        values.append(metrics[\"cloud_init_start\"])\n    if \"cloud_init_end\" in metrics:\n        updates.append(\"CloudInitEndTime = to_timestamp(%s)\")\n        values.append(metrics[\"cloud_init_end\"])\n    if \"cloud_init_duration_seconds\" in metrics:\n        updates.append(\"CloudInitDurationSeconds = %s\")\n        values.append(metrics[\"cloud_init_duration_seconds\"])\n\n    if \"container_start\" in metrics:\n        updates.append(\"ContainerStartTime = to_timestamp(%s)\")\n        values.append(metrics[\"container_start\"])\n    if \"container_end\" in metrics:\n        updates.append(\"ContainerEndTime = to_timestamp(%s)\")\n        values.append(metrics[\"container_end\"])\n    if \"container_startup_duration_seconds\" in metrics:\n        updates.append(\"ContainerStartupDurationSeconds = %s\")\n        values.append(metrics[\"container_startup_duration_seconds\"])\n\n    if not updates:\n        logger.debug(\"No metrics to update.\")\n        return\n\n    # Add total startup time calculation to the same UPDATE\n    # This recalculates the total whenever any metric is updated\n    updates.append(\n        \"\"\"TotalStartupDurationSeconds =\n            COALESCE(TerraformApplyDurationSeconds, 0) +\n            COALESCE(CloudInitDurationSeconds, 0) +\n            COALESCE(ContainerStartupDurationSeconds, 0)\"\"\"\n    )\n\n    query = f\"\"\"\n        UPDATE {self.table_name}\n        SET {\", \".join(updates)}\n        WHERE hostname = %s\n        RETURNING TotalStartupDurationSeconds;\n    \"\"\"\n    values.append(hostname)\n\n    with self.conn.cursor() as cursor:\n        try:\n            cursor.execute(query, tuple(values))\n            result = cursor.fetchone()\n            self.conn.commit()\n\n            logger.debug(f\"Updated VM metrics for '{hostname}': {metrics}.\")\n\n            # Log total startup time if all components are present\n            if result and result[0] is not None and result[0] &gt; 0:\n                logger.info(\n                    f\"Total startup time for VM '{hostname}': \"\n                    f\"{result[0]:.1f} seconds.\"\n                )\n        except Exception as e:\n            logger.error(\n                f\"Error updating VM metrics atomically for '{hostname}': {e}\",\n                exc_info=True,\n            )\n            self.conn.rollback()\n            raise  # Re-raise to let caller know the operation failed\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.update_vm_status","title":"<code>update_vm_status(hostname, status)</code>","text":"<p>Update the status of a VM by its hostname.</p> <p>Parameters:</p> Name Type Description Default <code>hostname</code> <code>str</code> <p>The hostname of the VM.</p> required <code>status</code> <code>str</code> <p>The new status to set for the VM.</p> required Source code in <code>packages/allocator/src/lablink_allocator_service/database.py</code> <pre><code>def update_vm_status(self, hostname: str, status: str) -&gt; None:\n    \"\"\"Update the status of a VM by its hostname.\n\n    Args:\n        hostname (str): The hostname of the VM.\n        status (str): The new status to set for the VM.\n    \"\"\"\n    possible_statuses = [\"running\", \"initializing\", \"unknown\", \"error\"]\n    if status not in possible_statuses:\n        logger.error(\n            f\"Invalid status '{status}'. Must be one of {possible_statuses}.\"\n        )\n        return\n\n    query = f\"\"\"\n    INSERT INTO {self.table_name} (hostname, status)\n    VALUES (%s, %s)\n    ON CONFLICT (hostname) DO UPDATE\n        SET status = EXCLUDED.status;\n    \"\"\"\n    try:\n        self.cursor.execute(query, (hostname, status))\n        self.conn.commit()\n        logger.debug(f\"Updated status for VM '{hostname}' to {status}.\")\n    except Exception as e:\n        logger.error(f\"Error updating VM status: {e}\")\n        self.conn.rollback()\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/database/#lablink_allocator_service.database.PostgresqlDatabase.vm_exists","title":"<code>vm_exists(hostname)</code>","text":"<p>Check if a VM with the given hostname exists in the table.</p> <p>Parameters:</p> Name Type Description Default <code>hostname</code> <code>str</code> <p>The hostname of the VM.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the VM exists, False otherwise.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/database.py</code> <pre><code>def vm_exists(self, hostname) -&gt; bool:\n    \"\"\"Check if a VM with the given hostname exists in the table.\n\n    Args:\n        hostname (str): The hostname of the VM.\n\n    Returns:\n        bool: True if the VM exists, False otherwise.\n    \"\"\"\n    query = f\"SELECT EXISTS (SELECT 1 FROM {self.table_name} WHERE hostname = %s)\"\n    self.cursor.execute(query, (hostname,))\n    result = self.cursor.fetchone()\n    if result is None:\n        logger.warning(\n            f\"vm_exists query returned None for hostname '{hostname}'.\"\n            \" Assuming VM does not exist.\"\n        )\n        return False\n    return result[0]\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/generate_init_sql/","title":"lablink_allocator_service.generate_init_sql","text":""},{"location":"reference/allocator/lablink_allocator_service/generate_init_sql/#lablink_allocator_service.generate_init_sql","title":"<code>lablink_allocator_service.generate_init_sql</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/generate_init_sql/#lablink_allocator_service.generate_init_sql-functions","title":"Functions","text":""},{"location":"reference/allocator/lablink_allocator_service/generate_init_sql/#lablink_allocator_service.generate_init_sql.main","title":"<code>main()</code>","text":"<p>Generate PostgreSQL initialization SQL script.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/generate_init_sql.py</code> <pre><code>def main():\n    \"\"\"Generate PostgreSQL initialization SQL script.\"\"\"\n    config = get_config()\n\n    # Load database configuration from config.yaml\n    DB_NAME = config.db.dbname\n    DB_USER = config.db.user\n    DB_PASSWORD = config.db.password\n    VM_TABLE = config.db.table_name\n    MESSAGE_CHANNEL = config.db.message_channel\n\n    template = f\"\"\"\nALTER SYSTEM SET listen_addresses = '*';\n\nSET client_min_messages TO WARNING;\nDROP USER IF EXISTS {DB_USER};\nCREATE USER {DB_USER} WITH ENCRYPTED PASSWORD '{DB_PASSWORD}';\nALTER USER {DB_USER} WITH LOGIN;\nCREATE DATABASE {DB_NAME} OWNER {DB_USER};\nGRANT ALL PRIVILEGES ON DATABASE {DB_NAME} TO {DB_USER};\n\n\\\\c {DB_NAME};\n\nSET ROLE {DB_USER};\n\nCREATE TABLE IF NOT EXISTS scheduled_destructions (\n    id SERIAL PRIMARY KEY,\n    schedule_name VARCHAR(255) NOT NULL UNIQUE,\n    destruction_time TIMESTAMP NOT NULL,\n    recurrence_rule VARCHAR(255),\n    created_by VARCHAR(255),\n    status VARCHAR(50) NOT NULL DEFAULT 'scheduled',\n    execution_count INTEGER DEFAULT 0,\n    last_execution_time TIMESTAMP,\n    last_execution_result TEXT,\n    notification_enabled BOOLEAN DEFAULT TRUE,\n    notification_hours_before INTEGER DEFAULT 1,\n    created_at TIMESTAMP DEFAULT NOW(),\n    updated_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE INDEX idx_destruction_time ON scheduled_destructions(destruction_time);\nCREATE INDEX idx_status ON scheduled_destructions(status);\n\nCREATE OR REPLACE FUNCTION update_scheduled_destructions_updated_at()\nRETURNS TRIGGER AS $$\nBEGIN\n    NEW.updated_at = NOW();\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER scheduled_destructions_updated_at\n    BEFORE UPDATE ON scheduled_destructions\n    FOR EACH ROW\n    EXECUTE FUNCTION update_scheduled_destructions_updated_at();\n\nCREATE TABLE IF NOT EXISTS {VM_TABLE} (\n    HostName VARCHAR(1024) PRIMARY KEY,\n    Pin VARCHAR(1024),\n    CrdCommand VARCHAR(1024),\n    UserEmail VARCHAR(1024),\n    InUse BOOLEAN NOT NULL DEFAULT FALSE,\n    Healthy VARCHAR(1024),\n    Status   VARCHAR(1024),\n    Logs TEXT,\n    TerraformApplyStartTime TIMESTAMP,\n    TerraformApplyEndTime TIMESTAMP,\n    TerraformApplyDurationSeconds FLOAT,\n    CloudInitStartTime TIMESTAMP,\n    CloudInitEndTime TIMESTAMP,\n    CloudInitDurationSeconds FLOAT,\n    ContainerStartTime TIMESTAMP,\n    ContainerEndTime TIMESTAMP,\n    ContainerStartupDurationSeconds FLOAT,\n    TotalStartupDurationSeconds FLOAT,\n    CreatedAt TIMESTAMP DEFAULT NOW()\n);\n\nCREATE OR REPLACE FUNCTION notify_crd_command_update()\nRETURNS TRIGGER AS $$\nBEGIN\n    PERFORM pg_notify(\n        '{MESSAGE_CHANNEL}',\n        json_build_object(\n            'HostName', NEW.HostName,\n            'CrdCommand', NEW.CrdCommand,\n            'Pin', NEW.Pin\n        )::text\n    );\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER trigger_crd_command_insert_or_update\nAFTER INSERT OR UPDATE OF CrdCommand ON {VM_TABLE}\nFOR EACH ROW\nEXECUTE FUNCTION notify_crd_command_update();\n\n\"\"\"\n\n    with open(\"/app/init.sql\", \"w\") as f:\n        f.write(template)\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/get_config/","title":"lablink_allocator_service.get_config","text":""},{"location":"reference/allocator/lablink_allocator_service/get_config/#lablink_allocator_service.get_config","title":"<code>lablink_allocator_service.get_config</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/get_config/#lablink_allocator_service.get_config-attributes","title":"Attributes","text":""},{"location":"reference/allocator/lablink_allocator_service/get_config/#lablink_allocator_service.get_config.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/get_config/#lablink_allocator_service.get_config-classes","title":"Classes","text":""},{"location":"reference/allocator/lablink_allocator_service/get_config/#lablink_allocator_service.get_config-functions","title":"Functions","text":""},{"location":"reference/allocator/lablink_allocator_service/get_config/#lablink_allocator_service.get_config.get_config","title":"<code>get_config(config_path=None)</code>","text":"<p>Load the configuration file using Hydra and return it as a Config object.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>Optional[str]</code> <p>Optional explicit path to config file. If provided, loads from         this path. If None, uses default runtime or bundled config.</p> <code>None</code> <p>Returns:</p> Type Description <code>Config</code> <p>Config object validated against the schema.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/get_config.py</code> <pre><code>def get_config(config_path: Optional[str] = None) -&gt; Config:\n    \"\"\"\n    Load the configuration file using Hydra and return it as a Config object.\n\n    Args:\n        config_path: Optional explicit path to config file. If provided, loads from\n                    this path. If None, uses default runtime or bundled config.\n\n    Returns:\n        Config object validated against the schema.\n    \"\"\"\n    if config_path:\n        # Explicit path provided (for validation/testing)\n        path = Path(config_path)\n        config_dir = path.parent.as_posix()\n        config_name = path.stem  # Remove extension\n        logger.info(f\"Using explicit config from {config_path}\")\n    else:\n        # Use defaults (Docker runtime or bundled)\n        config_dir = os.getenv(\"CONFIG_DIR\", \"/config\")\n        config_name = os.getenv(\"CONFIG_NAME\", \"config.yaml\")\n        config_name = config_name.replace(\".yaml\", \"\").replace(\".yml\", \"\")\n\n    runtime_cfg_path = Path(config_dir) / f\"{config_name}.yaml\"\n\n    if runtime_cfg_path.exists() or config_path:\n        # Get relative path from this file to the config dir\n        relative_path = os.path.relpath(config_dir, Path(__file__).parent)\n        chosen_path = relative_path\n        chosen_name = config_name\n        if not config_path:\n            logger.info(f\"Using runtime config from {config_dir}\")\n    else:\n        # Fall back to bundled config\n        chosen_path = \"conf\"\n        chosen_name = \"config\"\n        logger.info(f\"Using bundled config from {chosen_path}\")\n\n    with initialize(config_path=chosen_path, version_base=None):\n        cfg = compose(config_name=chosen_name)\n        logger.debug(OmegaConf.to_yaml(cfg))\n        return cfg\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/main/","title":"lablink_allocator_service.main","text":""},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main","title":"<code>lablink_allocator_service.main</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main-attributes","title":"Attributes","text":""},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.ANSI_ESCAPE","title":"<code>ANSI_ESCAPE = re.compile('\\\\x1B(?:[@-Z\\\\\\\\-_]|\\\\[[0-?]*[ -/]*[@-~])')</code>  <code>module-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.ENVIRONMENT","title":"<code>ENVIRONMENT = os.getenv('ENVIRONMENT', 'prod').strip().lower().replace(' ', '-')</code>  <code>module-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.MESSAGE_CHANNEL","title":"<code>MESSAGE_CHANNEL = cfg.db.message_channel</code>  <code>module-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.PIN","title":"<code>PIN = '123456'</code>  <code>module-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.TERRAFORM_DIR","title":"<code>TERRAFORM_DIR = (Path(__file__).parent / 'terraform').resolve()</code>  <code>module-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.allocator_ip","title":"<code>allocator_ip = os.getenv('ALLOCATOR_PUBLIC_IP')</code>  <code>module-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.app","title":"<code>app = Flask(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.auth","title":"<code>auth = HTTPBasicAuth()</code>  <code>module-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.cfg","title":"<code>cfg = get_config()</code>  <code>module-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.cloud_init_output_log_group","title":"<code>cloud_init_output_log_group = os.getenv('CLOUD_INIT_LOG_GROUP')</code>  <code>module-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.database","title":"<code>database = None</code>  <code>module-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.db","title":"<code>db = SQLAlchemy(app)</code>  <code>module-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.db_uri","title":"<code>db_uri = f'postgresql://{cfg.db.user}:{cfg.db.password}@{cfg.db.host}:{cfg.db.port}/{cfg.db.dbname}'</code>  <code>module-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.key_name","title":"<code>key_name = os.getenv('ALLOCATOR_KEY_NAME')</code>  <code>module-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.scheduler_service","title":"<code>scheduler_service = None</code>  <code>module-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.users","title":"<code>users = {cfg.app.admin_user: generate_password_hash(cfg.app.admin_password)}</code>  <code>module-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main-classes","title":"Classes","text":""},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main-functions","title":"Functions","text":""},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.admin","title":"<code>admin()</code>","text":"Source code in <code>packages/allocator/src/lablink_allocator_service/main.py</code> <pre><code>@app.route(\"/admin\")\n@auth.login_required\ndef admin():\n    return render_template(\"admin.html\")\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.cancel_scheduled_destruction","title":"<code>cancel_scheduled_destruction(schedule_id)</code>","text":"<p>Cancel a scheduled destruction.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/main.py</code> <pre><code>@app.route(\"/api/schedule-destruction/&lt;int:schedule_id&gt;\", methods=[\"DELETE\"])\n@auth.login_required\ndef cancel_scheduled_destruction(schedule_id: int):\n    \"\"\"Cancel a scheduled destruction.\"\"\"\n\n    # Check if schedule exists\n    schedule = database.get_scheduled_destruction(schedule_id)\n    if not schedule:\n        return jsonify({\"success\": False, \"message\": \"Schedule not found\"}), 404\n\n    # Check if already cancelled or completed\n    if schedule[\"status\"] in [\"cancelled\", \"completed\"]:\n        return jsonify(\n            {\n                \"success\": False,\n                \"message\": f\"Cannot cancel schedule with status '{schedule['status']}'\",\n            }\n        ), 400\n\n    if scheduler_service is None:\n        return jsonify(\n            {\"success\": False, \"message\": \"Scheduler service not initialized\"}\n        ), 500\n\n    try:\n        scheduler_service.cancel_scheduled_destruction(schedule_id)\n\n        return jsonify(\n            {\n                \"success\": True,\n                \"message\": (\n                    f\"Scheduled destruction {schedule_id} cancelled successfully\"\n                ),\n            }\n        )\n\n    except Exception as e:\n        logger.error(f\"Failed to cancel scheduled destruction: {e}\")\n        return jsonify({\"success\": False, \"message\": str(e)}), 500\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.check_crd_input","title":"<code>check_crd_input(crd_command)</code>","text":"<p>Check if the CRD command is valid.</p> <p>Parameters:</p> Name Type Description Default <code>crd_command</code> <code>string</code> <p>The CRD command to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the command is valid, False otherwise.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/main.py</code> <pre><code>def check_crd_input(crd_command: str) -&gt; bool:\n    \"\"\"Check if the CRD command is valid.\n\n    Args:\n        crd_command (string): The CRD command to check.\n\n    Returns:\n        bool: True if the command is valid, False otherwise.\n    \"\"\"\n    if crd_command is None:\n        logger.error(\"CRD command is None.\")\n        return False\n\n    elif \"--code\" not in crd_command:\n        logger.error(\"Invalid CRD command: --code not found.\")\n        return False\n\n    return True\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.create_instances","title":"<code>create_instances()</code>","text":"Source code in <code>packages/allocator/src/lablink_allocator_service/main.py</code> <pre><code>@app.route(\"/admin/create\")\n@auth.login_required\ndef create_instances():\n    return render_template(\"create-instances.html\")\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.create_scheduled_destruction","title":"<code>create_scheduled_destruction()</code>","text":"<p>Create a new scheduled destruction.</p> <p>Request JSON: {     \"schedule_name\": \"Friday Tutorial End\",     \"destruction_time\": \"2025-12-05T17:30:00Z\",     \"recurrence_rule\": null  // or \"FREQ=WEEKLY;BYDAY=FR;BYHOUR=17;BYMINUTE=30\" }</p> <p>Returns:</p> Name Type Description <code>Response</code> <code>Response | tuple[Response, int]</code> <p>JSON with schedule_id and success status, or error with status code.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/main.py</code> <pre><code>@app.route(\"/api/schedule-destruction\", methods=[\"POST\"])\n@auth.login_required\ndef create_scheduled_destruction() -&gt; Response | tuple[Response, int]:\n    \"\"\"\n    Create a new scheduled destruction.\n\n    Request JSON:\n    {\n        \"schedule_name\": \"Friday Tutorial End\",\n        \"destruction_time\": \"2025-12-05T17:30:00Z\",\n        \"recurrence_rule\": null  // or \"FREQ=WEEKLY;BYDAY=FR;BYHOUR=17;BYMINUTE=30\"\n    }\n\n    Returns:\n        Response: JSON with schedule_id and success status, or error with status code.\n    \"\"\"\n    from datetime import datetime\n\n    data = request.get_json()\n\n    # Validation\n    if not data.get(\"schedule_name\"):\n        return jsonify({\"success\": False, \"message\": \"schedule_name is required\"}), 400\n\n    if not data.get(\"destruction_time\"):\n        return jsonify(\n            {\"success\": False, \"message\": \"destruction_time is required\"}\n        ), 400\n\n    try:\n        destruction_time = datetime.fromisoformat(\n            data[\"destruction_time\"].replace(\"Z\", \"+00:00\")\n        )\n\n        # Ensure time is in future\n        if destruction_time &lt;= datetime.now(destruction_time.tzinfo):\n            return jsonify(\n                {\"success\": False, \"message\": \"destruction_time must be in the future\"}\n            ), 400\n\n        if scheduler_service is None:\n            return jsonify(\n                {\"success\": False, \"message\": \"Scheduler service not initialized\"}\n            ), 500\n\n        try:\n            schedule_id = scheduler_service.schedule_destruction(\n                schedule_name=data[\"schedule_name\"],\n                destruction_time=destruction_time,\n                recurrence_rule=data.get(\"recurrence_rule\"),\n                created_by=auth.current_user(),\n                notification_enabled=data.get(\"notification_enabled\", False),\n                notification_hours_before=data.get(\"notification_hours_before\", 1),\n            )\n        except ValueError as e:\n            # Duplicate schedule name (from database unique constraint)\n            return jsonify({\"success\": False, \"message\": str(e)}), 409\n        except RuntimeError as e:\n            # Database or scheduler error\n            logger.error(f\"Failed to create scheduled destruction: {e}\")\n            return jsonify({\"success\": False, \"message\": str(e)}), 500\n\n        return jsonify(\n            {\n                \"success\": True,\n                \"schedule_id\": schedule_id,\n                \"message\": \"Scheduled destruction created successfully\",\n            }\n        ), 200\n\n    except ValueError as e:\n        return jsonify(\n            {\"success\": False, \"message\": f\"Invalid destruction_time format: {str(e)}\"}\n        ), 400\n    except Exception as e:\n        logger.error(f\"Failed to create scheduled destruction: {e}\")\n        return jsonify({\"success\": False, \"message\": str(e)}), 500\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.delete_instances","title":"<code>delete_instances()</code>","text":"Source code in <code>packages/allocator/src/lablink_allocator_service/main.py</code> <pre><code>@app.route(\"/admin/instances/delete\")\n@auth.login_required\ndef delete_instances():\n    return render_template(\"delete-instances.html\", extension=cfg.machine.extension)\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.destroy","title":"<code>destroy()</code>","text":"Source code in <code>packages/allocator/src/lablink_allocator_service/main.py</code> <pre><code>@app.route(\"/destroy\", methods=[\"POST\"])\n@auth.login_required\ndef destroy():\n    try:\n        # Destroy Terraform resources\n        apply_cmd = [\n            \"terraform\",\n            \"destroy\",\n            \"-auto-approve\",\n            \"-var-file=terraform.runtime.tfvars\",\n        ]\n        result = subprocess.run(\n            apply_cmd, cwd=TERRAFORM_DIR, check=True, capture_output=True, text=True\n        )\n\n        # Clear the database\n        logger.debug(\"Clearing the database...\")\n        database.clear_database()\n        logger.debug(\"Database cleared successfully.\")\n\n        # Format the output to remove ANSI escape codes\n        clean_output = ANSI_ESCAPE.sub(\"\", result.stdout)\n\n        return render_template(\"delete-dashboard.html\", output=clean_output)\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"Error during Terraform destroy: {e}\")\n        error_output = e.stderr or e.stdout\n        clean_output = ANSI_ESCAPE.sub(\"\", error_output or \"\")\n        return render_template(\"delete-dashboard.html\", error=clean_output)\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.download_all_data","title":"<code>download_all_data()</code>","text":"Source code in <code>packages/allocator/src/lablink_allocator_service/main.py</code> <pre><code>@app.route(\"/api/scp-client\", methods=[\"GET\"])\n@auth.login_required\ndef download_all_data():\n    if database.get_row_count() == 0:\n        logger.warning(\"No VMs found in the database.\")\n        return jsonify({\"error\": \"No VMs found in the database.\"}), 404\n    try:\n        instance_ips = get_instance_ips(terraform_dir=TERRAFORM_DIR)\n        key_path = get_ssh_private_key(terraform_dir=TERRAFORM_DIR)\n        empty_data = True\n\n        with tempfile.TemporaryDirectory() as temp_dir:\n            for i, ip in enumerate(instance_ips):\n                # Make temporary directory for each VM\n                logger.debug(f\"Downloading data from VM {i + 1} at {ip}...\")\n                vm_dir = Path(temp_dir) / f\"vm_{i + 1}\"\n                vm_dir.mkdir(parents=True, exist_ok=True)\n\n                logger.info(\n                    f\"Extracting {cfg.machine.extension} files \"\n                    f\"from container on {ip}...\"\n                )\n\n                # Find files from the Docker container\n                files = find_files_in_container(\n                    ip=ip, key_path=key_path, extension=cfg.machine.extension\n                )\n\n                # If no files are found, log a warning and continue to the next VM\n                if len(files) == 0:\n                    logger.warning(\n                        f\"No {cfg.machine.extension} files found in container on {ip}.\"\n                    )\n                    continue\n                else:\n                    logger.debug(\n                        f\"Found {len(files)} {cfg.machine.extension} \"\n                        f\"files in container on {ip}.\"\n                    )\n                    # Extract files from the Docker container\n                    extract_files_from_docker(\n                        ip=ip,\n                        key_path=key_path,\n                        files=files,\n                    )\n                    empty_data = False\n                logger.info(\n                    f\"Copying {cfg.machine.extension} files from {ip} to {vm_dir}...\"\n                )\n\n                # Copy the extracted files to the allocator container's local\n                rsync_files_to_allocator(\n                    ip=ip,\n                    key_path=key_path,\n                    local_dir=vm_dir.as_posix(),\n                    extension=cfg.machine.extension,\n                )\n\n            if empty_data:\n                logger.warning(f\"No {cfg.machine.extension} files found in any VMs.\")\n                return (\n                    jsonify(\n                        {\"error\": f\"No {cfg.machine.extension} files found in any VMs.\"}\n                    ),\n                    404,\n                )\n\n            logger.info(f\"All files copied to {temp_dir}.\")\n\n            # Create a zip file of the downloaded data with a timestamp\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            zip_file = Path(tempfile.gettempdir()) / f\"lablink_data{timestamp}.zip\"\n\n            with ZipFile(zip_file, \"w\") as archive:\n                for vm_dir in Path(temp_dir).iterdir():\n                    if vm_dir.is_dir():\n                        logger.debug(f\"Zipping data for VM: {vm_dir.name}\")\n                        for file in vm_dir.rglob(f\"*.{cfg.machine.extension}\"):\n                            logger.debug(f\"Adding {file.name} to zip archive.\")\n                            # Add with relative path inside zip\n                            archive.write(file, arcname=file.relative_to(temp_dir))\n            logger.debug(\"All data downloaded and zipped successfully.\")\n\n            # Send the zip file as a response and remove it after the request\n            @after_this_request\n            def remove_zip_file(response):\n                try:\n                    os.remove(zip_file)\n                    logger.debug(f\"Removed zip file: {zip_file}\")\n                except Exception as e:\n                    logger.error(f\"Error removing zip file: {e}\")\n                return response\n\n            return send_file(zip_file, as_attachment=True)\n\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"Error downloading data: {e}\")\n        return (\n            jsonify({\"error\": \"An error occurred while downloading data from VMs.\"}),\n            500,\n        )\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.get_all_vm_status","title":"<code>get_all_vm_status()</code>","text":"Source code in <code>packages/allocator/src/lablink_allocator_service/main.py</code> <pre><code>@app.route(\"/api/vm-status\", methods=[\"GET\"])\ndef get_all_vm_status():\n    try:\n        vm_status = database.get_all_vm_status()\n        if not vm_status:\n            return jsonify({\"error\": \"No VMs found.\"}), 404\n\n        return jsonify(vm_status), 200\n    except Exception as e:\n        logger.error(f\"Error getting all VM status: {e}\")\n        return jsonify({\"error\": \"Failed to get VM status.\"}), 500\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.get_scheduled_destruction","title":"<code>get_scheduled_destruction(schedule_id)</code>","text":"<p>Get details of a scheduled destruction.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/main.py</code> <pre><code>@app.route(\"/api/schedule-destruction/&lt;int:schedule_id&gt;\", methods=[\"GET\"])\n@auth.login_required\ndef get_scheduled_destruction(schedule_id: int):\n    \"\"\"Get details of a scheduled destruction.\"\"\"\n\n    schedule = database.get_scheduled_destruction(schedule_id)\n\n    if not schedule:\n        return jsonify({\"success\": False, \"message\": \"Schedule not found\"}), 404\n\n    return jsonify({\"success\": True, \"schedule\": schedule})\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.get_unassigned_instance_counts","title":"<code>get_unassigned_instance_counts()</code>","text":"<p>Get the counts of all instance types.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/main.py</code> <pre><code>@app.route(\"/api/unassigned_vms_count\", methods=[\"GET\"])\ndef get_unassigned_instance_counts():\n    \"\"\"Get the counts of all instance types.\"\"\"\n    instance_counts = len(database.get_unassigned_vms())\n    return jsonify(count=instance_counts), 200\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.get_vm_logs","title":"<code>get_vm_logs(hostname)</code>","text":"<p>Get the logs for a specific VM.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/main.py</code> <pre><code>@app.route(\"/admin/logs/&lt;hostname&gt;\", methods=[\"GET\"])\n@auth.login_required\ndef get_vm_logs(hostname):\n    \"\"\"Get the logs for a specific VM.\"\"\"\n    logger.debug(f\"Fetching logs for VM: {hostname}\")\n    if not database.vm_exists(hostname=hostname):\n        logger.error(f\"VM with hostname {hostname} not found.\")\n        return jsonify({\"error\": \"VM not found.\"}), 404\n    return render_template(\"instance-logs.html\", hostname=hostname)\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.get_vm_logs_by_hostname","title":"<code>get_vm_logs_by_hostname(hostname)</code>","text":"Source code in <code>packages/allocator/src/lablink_allocator_service/main.py</code> <pre><code>@app.route(\"/api/vm-logs/&lt;hostname&gt;\", methods=[\"GET\"])\ndef get_vm_logs_by_hostname(hostname):\n    try:\n        vm = database.get_vm_by_hostname(hostname=hostname)\n        logger.debug(f\"Fetching logs for VM: {hostname}: {vm}\")\n\n        # Check if the VM exists\n        if vm is None:\n            logger.error(f\"VM with hostname {hostname} not found.\")\n            return jsonify({\"error\": \"VM not found.\"}), 404\n\n        # If the logs are empty but the vm is initializing, return a 503 status\n        logs = database.get_vm_logs(hostname=hostname)\n        status = vm.get(\"status\")\n        if logs is None and status == \"initializing\":\n            return jsonify({\"error\": \"VM is installing CloudWatch agent.\"}), 503\n\n        return jsonify({\"hostname\": hostname, \"logs\": logs}), 200\n    except Exception as e:\n        logger.error(f\"Error getting VM logs: {e}\")\n        return jsonify({\"error\": \"Failed to get VM logs.\"}), 500\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.get_vm_status","title":"<code>get_vm_status(hostname)</code>","text":"Source code in <code>packages/allocator/src/lablink_allocator_service/main.py</code> <pre><code>@app.route(\"/api/vm-status/&lt;hostname&gt;\", methods=[\"GET\"])\ndef get_vm_status(hostname):\n    try:\n        status = database.get_status_by_hostname(hostname=hostname)\n        if status is None:\n            return jsonify({\"error\": \"VM not found.\"}), 404\n\n        return jsonify({\"hostname\": hostname, \"status\": status}), 200\n    except Exception as e:\n        logger.error(f\"Error getting VM status: {e}\")\n        return jsonify({\"error\": \"Failed to get VM status.\"}), 500\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.home","title":"<code>home()</code>","text":"Source code in <code>packages/allocator/src/lablink_allocator_service/main.py</code> <pre><code>@app.route(\"/\")\ndef home():\n    return render_template(\"index.html\")\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.init_database","title":"<code>init_database()</code>","text":"<p>Initialize the database connection.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/main.py</code> <pre><code>def init_database():\n    \"\"\"Initialize the database connection.\"\"\"\n    global database\n    database = PostgresqlDatabase(\n        dbname=cfg.db.dbname,\n        user=cfg.db.user,\n        password=cfg.db.password,\n        host=cfg.db.host,\n        port=cfg.db.port,\n        table_name=cfg.db.table_name,\n        message_channel=cfg.db.message_channel,\n    )\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.launch","title":"<code>launch()</code>","text":"Source code in <code>packages/allocator/src/lablink_allocator_service/main.py</code> <pre><code>@app.route(\"/api/launch\", methods=[\"POST\"])\n@auth.login_required\ndef launch():\n    # Get and validate num_vms input\n    try:\n        num_vms_str = request.form.get(\"num_vms\")\n        if not num_vms_str:\n            return render_template(\"dashboard.html\", error=\"Number of VMs is required.\")\n        num_vms = int(num_vms_str)\n        if num_vms &lt;= 0:\n            return render_template(\n                \"dashboard.html\", error=\"Number of VMs must be greater than 0.\"\n            )\n    except ValueError:\n        return render_template(\n            \"dashboard.html\",\n            error=\"Invalid number of VMs. Please enter a valid integer.\",\n        )\n\n    runtime_file = TERRAFORM_DIR / \"terraform.runtime.tfvars\"\n\n    try:\n        # Calculate the number of VMs to launch\n        total_vms = num_vms + database.get_row_count()\n\n        logger.debug(f\"Machine type: {cfg.machine.machine_type}\")\n        logger.debug(f\"Image name: {cfg.machine.image}\")\n        logger.debug(f\"client VM AMI ID: {cfg.machine.ami_id}\")\n        logger.debug(f\"GitHub repository: {cfg.machine.repository}\")\n        logger.debug(f\"Subject Software: {cfg.machine.software}\")\n        logger.debug(f\"Region: {cfg.app.region}\")\n        logger.debug(f\"Allocator IP: {allocator_ip}\")\n        logger.debug(f\"Cloud Init Output Log Group: {cloud_init_output_log_group}\")\n\n        if not allocator_ip or not key_name:\n            logger.error(\"Missing allocator outputs.\")\n            return render_template(\n                \"dashboard.html\", error=\"Allocator outputs not found.\"\n            )\n\n        logger.debug(f\"Allocator IP: {allocator_ip}\")\n        logger.debug(f\"Key Name: {key_name}\")\n        logger.debug(f\"ENVIRONMENT: {ENVIRONMENT}\")\n\n        # Check if GPU is supported\n        gpu_support_bool = check_support_nvidia(machine_type=cfg.machine.machine_type)\n\n        # Process GPU support so that it can be used in the runtime file\n        if gpu_support_bool:\n            logger.info(\"GPU support is enabled for the machine type.\")\n            gpu_support = \"true\"\n        else:\n            logger.info(\"GPU support is not enabled for the machine type.\")\n            gpu_support = \"false\"\n\n        # Generate allocator URL based on DNS and SSL configuration\n        allocator_url, protocol = get_allocator_url(cfg, allocator_ip)\n        logger.info(f\"Using allocator URL: {allocator_url} (protocol: {protocol})\")\n\n        # Write the runtime variables to the file\n        with runtime_file.open(\"w\") as f:\n            f.write(f'allocator_ip = \"{allocator_ip}\"\\n')\n            f.write(f'allocator_url = \"{allocator_url}\"\\n')\n            f.write(f'machine_type = \"{cfg.machine.machine_type}\"\\n')\n            f.write(f'image_name = \"{cfg.machine.image}\"\\n')\n            f.write(f'repository = \"{cfg.machine.repository}\"\\n')\n            f.write(f'client_ami_id = \"{cfg.machine.ami_id}\"\\n')\n            f.write(f'subject_software = \"{cfg.machine.software}\"\\n')\n            f.write(f'resource_suffix = \"{ENVIRONMENT}\"\\n')\n            f.write(f'gpu_support = \"{gpu_support}\"\\n')\n            f.write(f'cloud_init_output_log_group = \"{cloud_init_output_log_group}\"\\n')\n            f.write(f'region = \"{cfg.app.region}\"\\n')\n            f.write(f'startup_on_error = \"{cfg.startup_script.on_error}\"\\n')\n\n        # Apply with the new number of instances\n        apply_cmd = [\n            \"terraform\",\n            \"apply\",\n            \"-auto-approve\",\n            \"-var-file=terraform.runtime.tfvars\",\n            f\"-var=instance_count={total_vms}\",\n        ]\n\n        logger.debug(f\"Running command: {' '.join(apply_cmd)}\")\n\n        # Run the Terraform apply command\n        result = subprocess.run(\n            apply_cmd, cwd=TERRAFORM_DIR, check=True, capture_output=True, text=True\n        )\n\n        # Format the output to remove ANSI escape codes\n        clean_output = ANSI_ESCAPE.sub(\"\", result.stdout)\n\n        # Upload the runtime file to S3\n        logger.debug(f\"Uploading runtime file to S3 bucket: {cfg.bucket_name}...\")\n        upload_to_s3(\n            local_path=runtime_file,\n            env=ENVIRONMENT,\n            bucket_name=cfg.bucket_name,\n            region=cfg.app.region,\n        )\n\n        # Store timing outputs in the database\n        timing_data = get_instance_timings(terraform_dir=TERRAFORM_DIR)\n        logger.debug(f\"Timing data: {timing_data}\")\n\n        for hostname, times in timing_data.items():\n            start_time = datetime.fromisoformat(\n                times[\"start_time\"].replace(\"Z\", \"+00:00\")\n            )\n            end_time = datetime.fromisoformat(times[\"end_time\"].replace(\"Z\", \"+00:00\"))\n            database.update_terraform_timing(\n                hostname=hostname,\n                per_instance_seconds=float(times[\"seconds\"]),\n                per_instance_start_time=start_time,\n                per_instance_end_time=end_time,\n            )\n\n        return render_template(\"dashboard.html\", output=clean_output)\n\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"Error during Terraform apply: {e}\")\n        error_output = e.stderr or e.stdout\n        clean_output = ANSI_ESCAPE.sub(\"\", error_output or \"\")\n        return render_template(\"dashboard.html\", error=clean_output)\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.list_scheduled_destructions","title":"<code>list_scheduled_destructions()</code>","text":"<p>List all scheduled destructions.</p> Query parameters <p>status (optional): Filter by status (scheduled, executing, completed,     failed, cancelled)</p> <p>Returns:</p> Name Type Description <code>Response</code> <code>Response | tuple[Response, int]</code> <p>JSON with list of schedules, or error with status code.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/main.py</code> <pre><code>@app.route(\"/api/schedule-destruction\", methods=[\"GET\"])\n@auth.login_required\ndef list_scheduled_destructions() -&gt; Response | tuple[Response, int]:\n    \"\"\"\n    List all scheduled destructions.\n\n    Query parameters:\n        status (optional): Filter by status (scheduled, executing, completed,\n            failed, cancelled)\n\n    Returns:\n        Response: JSON with list of schedules, or error with status code.\n    \"\"\"\n\n    status_filter = request.args.get(\"status\")\n\n    if status_filter and status_filter not in [\n        \"scheduled\",\n        \"executing\",\n        \"completed\",\n        \"failed\",\n        \"cancelled\",\n    ]:\n        return jsonify(\n            {\"success\": False, \"message\": f\"Invalid status filter: {status_filter}\"}\n        ), 400\n\n    schedules = database.get_all_scheduled_destructions(status=status_filter)\n\n    return jsonify({\"success\": True, \"schedules\": schedules, \"count\": len(schedules)})\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.main","title":"<code>main()</code>","text":"<p>Main entry point for the allocator service.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/main.py</code> <pre><code>def main():\n    \"\"\"Main entry point for the allocator service.\"\"\"\n    global scheduler_service\n\n    try:\n        with app.app_context():\n            db.create_all()\n            init_database()\n\n        # Initialize scheduler service\n        logger.info(\"Initializing scheduler service...\")\n        db_url = (\n            f\"postgresql://{cfg.db.user}:{cfg.db.password}\"\n            f\"@{cfg.db.host}:{cfg.db.port}/{cfg.db.dbname}\"\n        )\n        scheduler_service = ScheduledDestructionService(\n            database=database, db_url=db_url\n        )\n        scheduler_service.start()\n        atexit.register(scheduler_service.stop)\n        logger.info(\"Scheduler service started successfully\")\n\n        # Terraform initialization\n        if not (TERRAFORM_DIR / \"terraform.runtime.tfvars\").exists():\n            logger.info(\"Initializing Terraform...\")\n            if ENVIRONMENT not in [\"prod\", \"test\", \"ci-test\"]:\n                (TERRAFORM_DIR / \"backend.tf\").unlink(missing_ok=True)\n                subprocess.run(\n                    [\"terraform\", \"init\"],\n                    cwd=TERRAFORM_DIR,\n                    check=True,\n                )\n            else:\n                # Use bucket_name from config for client VM terraform state\n                default_bucket = \"tf-state-lablink-allocator-bucket\"\n                bucket_name = (\n                    cfg.bucket_name if hasattr(cfg, \"bucket_name\") else default_bucket\n                )\n                logger.info(f\"Initializing Terraform with S3 backend: {bucket_name}\")\n                subprocess.run(\n                    [\n                        \"terraform\",\n                        \"init\",\n                        f\"-backend-config=backend-client-{ENVIRONMENT}.hcl\",\n                        f\"-backend-config=bucket={bucket_name}\",\n                        f\"-backend-config=region={cfg.app.region}\",\n                    ],\n                    cwd=TERRAFORM_DIR,\n                    check=True,\n                )\n\n        logger.info(\"Starting Flask application...\")\n        app.run(host=\"0.0.0.0\", port=5000, threaded=True)\n\n    except Exception as e:\n        logger.error(f\"Failed to start allocator service: {e}\", exc_info=True)\n\n        # Clean up scheduler if it was initialized\n        if scheduler_service is not None:\n            try:\n                logger.info(\"Stopping scheduler service due to startup failure...\")\n                scheduler_service.stop()\n            except Exception as cleanup_error:\n                logger.error(\n                    f\"Error stopping scheduler during cleanup: {cleanup_error}\"\n                )\n\n        # Re-raise the exception to exit with error code\n        raise\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.notify_participants","title":"<code>notify_participants()</code>","text":"<p>Trigger function to notify participant VMs.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/main.py</code> <pre><code>def notify_participants():\n    \"\"\"Trigger function to notify participant VMs.\"\"\"\n    conn = psycopg2.connect(os.getenv(\"DATABASE_URL\"))\n    cursor = conn.cursor()\n    cursor.execute(\"LISTEN vm_updates;\")\n    conn.commit()\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.receive_vm_logs","title":"<code>receive_vm_logs()</code>","text":"Source code in <code>packages/allocator/src/lablink_allocator_service/main.py</code> <pre><code>@app.route(\"/api/vm-logs\", methods=[\"POST\"])\ndef receive_vm_logs():\n    try:\n        data = request.get_json()\n        log_group = data.get(\"log_group\")\n        log_stream = data.get(\"log_stream\")\n        messages = data.get(\"messages\", [])\n\n        if not log_group or not log_stream or not messages:\n            return (\n                jsonify({\"error\": \"Log group, stream, and messages are required.\"}),\n                400,\n            )\n\n        # Check if the VM exists in the database\n        if not database.vm_exists(log_stream):\n            logger.error(f\"VM with log stream {log_stream} does not exist.\")\n            return jsonify({\"error\": \"VM not found.\"}), 404\n\n        # Process the logs (e.g., save to a file, database, etc.)\n        logger.info(\n            f\"Received logs for {log_group}/{log_stream}: {len(messages)} messages\"\n        )\n\n        # Save the logs to the database\n        new_logs = \"\\n\".join(messages)\n        vm_log = database.get_vm_logs(hostname=log_stream)\n        if vm_log is not None:\n            vm_log += \"\\n\" + new_logs\n        else:\n            vm_log = new_logs\n        database.save_logs_by_hostname(hostname=log_stream, logs=vm_log)\n\n        return jsonify({\"message\": \"VM logs posted successfully.\"}), 200\n    except Exception as e:\n        logger.error(f\"Error receiving VM logs: {e}\")\n        return jsonify({\"error\": \"Failed to post VM logs.\"}), 500\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.receive_vm_metrics","title":"<code>receive_vm_metrics(hostname)</code>","text":"<p>Receive and store VM Cloud init metrics.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/main.py</code> <pre><code>@app.route(\"/api/vm-metrics/&lt;hostname&gt;\", methods=[\"POST\"])\ndef receive_vm_metrics(hostname):\n    \"\"\"Receive and store VM Cloud init metrics.\"\"\"\n    try:\n        data = request.get_json()\n\n        if not database.vm_exists(hostname=hostname):\n            logger.error(f\"VM with hostname {hostname} does not exist.\")\n            return jsonify({\"error\": \"VM not found.\"}), 404\n\n        # Update VM metrics and calculate total startup time atomically\n        # This combines two database operations into one for better performance\n        database.update_vm_metrics_atomic(hostname=hostname, metrics=data)\n\n        logger.info(f\"Received metrics for {hostname}: {data}\")\n        return jsonify({\"message\": \"VM metrics posted successfully.\"}), 200\n\n    except Exception as e:\n        logger.error(f\"Error receiving VM metrics for {hostname}: {e}\", exc_info=True)\n        return jsonify({\"error\": \"Failed to post VM metrics.\"}), 500\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.scheduled_destruction_page","title":"<code>scheduled_destruction_page()</code>","text":"<p>Render scheduled destruction management page.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/main.py</code> <pre><code>@app.route(\"/admin/scheduled-destruction\", methods=[\"GET\"])\n@auth.login_required\ndef scheduled_destruction_page():\n    \"\"\"Render scheduled destruction management page.\"\"\"\n    return render_template(\"scheduled-destruction.html\")\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.submit_vm_details","title":"<code>submit_vm_details()</code>","text":"Source code in <code>packages/allocator/src/lablink_allocator_service/main.py</code> <pre><code>@app.route(\"/api/request_vm\", methods=[\"POST\"])\ndef submit_vm_details():\n    try:\n        data = request.form\n        email = data.get(\"email\")\n        crd_command = data.get(\"crd_command\")\n\n        # If email or crd_command is not provided, return an error\n        if not email or not crd_command:\n            return render_template(\n                \"index.html\", error=\"Email and CRD command are required.\"\n            )\n\n        # Check if the CRD command is valid\n        if not check_crd_input(crd_command=crd_command):\n            logger.error(\"Invalid CRD command: --code not found.\")\n            return render_template(\n                \"index.html\",\n                error=\"Invalid CRD command received. \"\n                \"Please ask your instructor for help.\",\n            )\n\n        # Check if there are any available VMs\n        if len(database.get_unassigned_vms()) == 0:\n            logger.error(\"No available VMs found.\")\n            return render_template(\n                \"index.html\",\n                error=\"No available VMs. Please try again later. Please ask your \"\n                \"instructor for help\",\n            )\n\n        # Assign the VM\n        database.assign_vm(email=email, crd_command=crd_command, pin=PIN)\n\n        # Display success message\n        assigned_vm = database.get_vm_details(email=email)\n        return render_template(\"success.html\", host=assigned_vm[0], pin=assigned_vm[1])\n    except Exception as e:\n        logger.error(f\"Error in submit_vm_details: {e}\")\n        return render_template(\n            \"index.html\",\n            error=\"An unexpected error occurred while processing your request. \"\n            \"Please ask your instructor for help.\",\n        )\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.update_gpu_health","title":"<code>update_gpu_health()</code>","text":"<p>Check the health of the GPU.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/main.py</code> <pre><code>@app.route(\"/api/gpu_health\", methods=[\"POST\"])\ndef update_gpu_health():\n    \"\"\"Check the health of the GPU.\"\"\"\n    data = request.get_json()\n    gpu_status = data.get(\"gpu_status\")\n    hostname = data.get(\"hostname\")\n    if gpu_status is None or hostname is None:\n        return jsonify({\"error\": \"GPU status and hostname are required.\"}), 400\n\n    try:\n        database.update_health(hostname=hostname, healthy=gpu_status)\n        logger.info(f\"Updated GPU health status for {hostname} to {gpu_status}\")\n        return jsonify({\"message\": \"GPU health status updated successfully.\"}), 200\n    except Exception as e:\n        logger.error(f\"Error updating GPU health status: {e}\")\n        return jsonify({\"error\": \"Failed to update GPU health status.\"}), 500\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.update_inuse_status","title":"<code>update_inuse_status()</code>","text":"<p>Update the in-use status of a VM.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/main.py</code> <pre><code>@app.route(\"/api/update_inuse_status\", methods=[\"POST\"])\ndef update_inuse_status():\n    \"\"\"Update the in-use status of a VM.\"\"\"\n    data = request.get_json()\n    hostname = data.get(\"hostname\")\n    in_use = data.get(\"status\")\n\n    logger.debug(f\"Updating in-use status for {hostname} to {in_use}\")\n\n    if not hostname:\n        return jsonify({\"error\": \"Hostname is required.\"}), 400\n\n    try:\n        database.update_vm_in_use(hostname=hostname, in_use=in_use)\n        return jsonify({\"message\": \"In-use status updated successfully.\"}), 200\n    except Exception as e:\n        logger.error(f\"Error updating in-use status: {e}\")\n        return jsonify({\"error\": \"Failed to update in-use status.\"}), 500\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.update_vm_status","title":"<code>update_vm_status()</code>","text":"Source code in <code>packages/allocator/src/lablink_allocator_service/main.py</code> <pre><code>@app.route(\"/api/vm-status\", methods=[\"POST\"])\ndef update_vm_status():\n    try:\n        data = request.get_json()\n        hostname = data.get(\"hostname\")\n        status = data.get(\"status\")\n\n        if not hostname or status is None:\n            return jsonify({\"error\": \"Hostname and status are required.\"}), 400\n\n        database.update_vm_status(hostname=hostname, status=status)\n\n        return jsonify({\"message\": \"VM status updated successfully.\"}), 200\n    except Exception as e:\n        logger.error(f\"Error updating VM status: {e}\")\n        return jsonify({\"error\": \"Failed to update VM status.\"}), 500\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.verify_password","title":"<code>verify_password(username, password)</code>","text":"<p>Verify the username and password against the stored users. Args:     username (str): The username to verify.     password (str): The password to verify. Returns:     str: The username if the credentials are valid, None otherwise.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/main.py</code> <pre><code>@auth.verify_password\ndef verify_password(username, password):\n    \"\"\"Verify the username and password against the stored users.\n    Args:\n        username (str): The username to verify.\n        password (str): The password to verify.\n    Returns:\n        str: The username if the credentials are valid, None otherwise.\n    \"\"\"\n    if username in users and check_password_hash(users.get(username), password):\n        return username\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.view_instances","title":"<code>view_instances()</code>","text":"Source code in <code>packages/allocator/src/lablink_allocator_service/main.py</code> <pre><code>@app.route(\"/admin/instances\")\n@auth.login_required\ndef view_instances():\n    instances = database.get_all_vms()\n    return render_template(\"instances.html\", instances=instances)\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/main/#lablink_allocator_service.main.vm_startup","title":"<code>vm_startup()</code>","text":"Source code in <code>packages/allocator/src/lablink_allocator_service/main.py</code> <pre><code>@app.route(\"/vm_startup\", methods=[\"POST\"])\ndef vm_startup():\n    data = request.get_json()\n    hostname = data.get(\"hostname\")\n\n    if not hostname:\n        return jsonify({\"error\": \"Hostname is required.\"}), 400\n\n    # Check if the VM exists in the database\n    vm = database.get_vm_by_hostname(hostname)\n    if not vm:\n        return jsonify({\"error\": \"VM not found.\"}), 404\n\n    result = database.listen_for_notifications(\n        channel=MESSAGE_CHANNEL, target_hostname=hostname\n    )\n\n    return jsonify(result), 200\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/scheduler/","title":"lablink_allocator_service.scheduler","text":""},{"location":"reference/allocator/lablink_allocator_service/scheduler/#lablink_allocator_service.scheduler","title":"<code>lablink_allocator_service.scheduler</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/scheduler/#lablink_allocator_service.scheduler-attributes","title":"Attributes","text":""},{"location":"reference/allocator/lablink_allocator_service/scheduler/#lablink_allocator_service.scheduler.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/scheduler/#lablink_allocator_service.scheduler-classes","title":"Classes","text":""},{"location":"reference/allocator/lablink_allocator_service/scheduler/#lablink_allocator_service.scheduler.ScheduledDestructionService","title":"<code>ScheduledDestructionService(database, db_url, terraform_dir=None)</code>","text":"<p>Initialize the scheduler service.</p> <p>Parameters:</p> Name Type Description Default <code>database</code> <code>PostgresqlDatabase</code> <p>PostgresqlDatabase instance</p> required <code>db_url</code> <code>str</code> <p>PostgreSQL connection URL for APScheduler job store</p> required <code>terraform_dir</code> <code>Optional[str]</code> <p>Path to Terraform directory (optional, auto-detected if None)</p> <code>None</code> Source code in <code>packages/allocator/src/lablink_allocator_service/scheduler.py</code> <pre><code>def __init__(\n    self,\n    database: PostgresqlDatabase,\n    db_url: str,\n    terraform_dir: Optional[str] = None,\n):\n    \"\"\"Initialize the scheduler service.\n\n    Args:\n        database: PostgresqlDatabase instance\n        db_url: PostgreSQL connection URL for APScheduler job store\n        terraform_dir: Path to Terraform directory (optional, auto-detected if None)\n    \"\"\"\n    self.database: PostgresqlDatabase = database\n    self.db_url = db_url\n\n    self.terraform_dir = terraform_dir or os.path.join(\n        os.path.dirname(__file__), \"terraform\"\n    )\n\n    # Configure APScheduler with SQLAlchemy job store\n    jobstores = {\"default\": SQLAlchemyJobStore(url=db_url)}\n\n    executors = {\"default\": ThreadPoolExecutor(max_workers=2)}\n\n    job_defaults = {\n        \"coalesce\": False,\n        \"max_instances\": 1,\n        \"misfire_grace_time\": 300,\n    }\n\n    self.scheduler = BackgroundScheduler(\n        jobstores=jobstores,\n        executors=executors,\n        job_defaults=job_defaults,\n        timezone=\"UTC\",\n    )\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/scheduler/#lablink_allocator_service.scheduler.ScheduledDestructionService-attributes","title":"Attributes","text":""},{"location":"reference/allocator/lablink_allocator_service/scheduler/#lablink_allocator_service.scheduler.ScheduledDestructionService.database","title":"<code>database = database</code>  <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/scheduler/#lablink_allocator_service.scheduler.ScheduledDestructionService.db_url","title":"<code>db_url = db_url</code>  <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/scheduler/#lablink_allocator_service.scheduler.ScheduledDestructionService.scheduler","title":"<code>scheduler = BackgroundScheduler(jobstores=jobstores, executors=executors, job_defaults=job_defaults, timezone='UTC')</code>  <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/scheduler/#lablink_allocator_service.scheduler.ScheduledDestructionService.terraform_dir","title":"<code>terraform_dir = terraform_dir or os.path.join(os.path.dirname(__file__), 'terraform')</code>  <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/scheduler/#lablink_allocator_service.scheduler.ScheduledDestructionService-functions","title":"Functions","text":""},{"location":"reference/allocator/lablink_allocator_service/scheduler/#lablink_allocator_service.scheduler.ScheduledDestructionService.cancel_scheduled_destruction","title":"<code>cancel_scheduled_destruction(schedule_id)</code>","text":"<p>Cancel a scheduled destruction.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/scheduler.py</code> <pre><code>def cancel_scheduled_destruction(self, schedule_id: int) -&gt; None:\n    \"\"\"Cancel a scheduled destruction.\"\"\"\n\n    # Remove from APScheduler\n    job_id = f\"destruction_{schedule_id}\"\n    if self.scheduler.get_job(job_id):\n        self.scheduler.remove_job(job_id)\n\n    # Update database\n    self.database.cancel_scheduled_destruction(schedule_id)\n\n    logger.info(f\"Cancelled scheduled destruction ID: {schedule_id}\")\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/scheduler/#lablink_allocator_service.scheduler.ScheduledDestructionService.schedule_destruction","title":"<code>schedule_destruction(schedule_name, destruction_time, recurrence_rule=None, created_by=None, notification_enabled=True, notification_hours_before=1)</code>","text":"<p>Schedule a new destruction job.</p> <p>Parameters:</p> Name Type Description Default <code>schedule_name</code> <code>str</code> <p>Unique name for the schedule</p> required <code>destruction_time</code> <code>datetime</code> <p>Initial destruction time (UTC)</p> required <code>recurrence_rule</code> <code>Optional[str]</code> <p>Optional recurrence rule in RRULE format</p> <code>None</code> <code>created_by</code> <code>Optional[str]</code> <p>User who created the schedule</p> <code>None</code> <code>notification_enabled</code> <code>bool</code> <p>Whether to enable notifications</p> <code>True</code> <code>notification_hours_before</code> <code>int</code> <p>Hours before destruction to notify</p> <code>1</code> <p>Returns:</p> Type Description <code>int</code> <p>ID of the created schedule in the database</p> Source code in <code>packages/allocator/src/lablink_allocator_service/scheduler.py</code> <pre><code>def schedule_destruction(\n    self,\n    schedule_name: str,\n    destruction_time: datetime,\n    recurrence_rule: Optional[str] = None,\n    created_by: Optional[str] = None,\n    notification_enabled: bool = True,\n    notification_hours_before: int = 1,\n) -&gt; int:\n    \"\"\"Schedule a new destruction job.\n\n    Args:\n        schedule_name: Unique name for the schedule\n        destruction_time: Initial destruction time (UTC)\n        recurrence_rule: Optional recurrence rule in RRULE format\n        created_by: User who created the schedule\n        notification_enabled: Whether to enable notifications\n        notification_hours_before: Hours before destruction to notify\n\n    Returns:\n        ID of the created schedule in the database\n    \"\"\"\n    # Create database record\n    # This will raise ValueError for duplicate names or RuntimeError for\n    # other DB errors\n    schedule_id = self.database.create_scheduled_destruction(\n        schedule_name=schedule_name,\n        destruction_time=destruction_time,\n        recurrence_rule=recurrence_rule,\n        created_by=created_by,\n        notification_enabled=notification_enabled,\n        notification_hours_before=notification_hours_before,\n    )\n\n    # Add job to APScheduler\n    self._add_scheduler_job(\n        schedule_id=schedule_id,\n        destruction_time=destruction_time,\n        recurrence_rule=recurrence_rule,\n    )\n\n    logger.info(\n        f\"Scheduled destruction '{schedule_name}' (ID: {schedule_id}) \"\n        f\"for {destruction_time}\"\n    )\n    return schedule_id\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/scheduler/#lablink_allocator_service.scheduler.ScheduledDestructionService.start","title":"<code>start()</code>","text":"<p>Start the scheduler and load existing schedules</p> Source code in <code>packages/allocator/src/lablink_allocator_service/scheduler.py</code> <pre><code>def start(self):\n    \"\"\"Start the scheduler and load existing schedules\"\"\"\n    logger.info(\"Starting Scheduled Destruction Service...\")\n    self.scheduler.start()\n\n    # Load existing schedules from the database\n    self._load_scheduled_destructions()\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/scheduler/#lablink_allocator_service.scheduler.ScheduledDestructionService.stop","title":"<code>stop()</code>","text":"<p>Stop the scheduler.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/scheduler.py</code> <pre><code>def stop(self):\n    \"\"\"Stop the scheduler.\"\"\"\n    logger.info(\"Stopping scheduled destruction service\")\n    self.scheduler.shutdown(wait=True)\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/scheduler/#lablink_allocator_service.scheduler-functions","title":"Functions","text":""},{"location":"reference/allocator/lablink_allocator_service/scheduler/#lablink_allocator_service.scheduler.execute_scheduled_destruction_job","title":"<code>execute_scheduled_destruction_job(schedule_id, terraform_dir)</code>","text":"<p>Execute a scheduled destruction job.</p> <p>This is a standalone function (not a method) to avoid pickling issues with APScheduler's SQLAlchemy job store.</p> <p>Database credentials are read from the config system at runtime to avoid storing sensitive data in the APScheduler job store.</p> <p>Parameters:</p> Name Type Description Default <code>schedule_id</code> <code>int</code> <p>ID of the scheduled destruction</p> required <code>terraform_dir</code> <code>str</code> <p>Path to Terraform directory</p> required Source code in <code>packages/allocator/src/lablink_allocator_service/scheduler.py</code> <pre><code>def execute_scheduled_destruction_job(\n    schedule_id: int,\n    terraform_dir: str,\n):\n    \"\"\"\n    Execute a scheduled destruction job.\n\n    This is a standalone function (not a method) to avoid pickling issues\n    with APScheduler's SQLAlchemy job store.\n\n    Database credentials are read from the config system at runtime to avoid\n    storing sensitive data in the APScheduler job store.\n\n    Args:\n        schedule_id: ID of the scheduled destruction\n        terraform_dir: Path to Terraform directory\n    \"\"\"\n    from lablink_allocator_service.database import PostgresqlDatabase\n    from lablink_allocator_service.get_config import get_config\n\n    # Load config at runtime to get credentials (avoids storing passwords in job store)\n    cfg = get_config()\n\n    # Create a fresh database connection for this job\n    database = PostgresqlDatabase(\n        dbname=cfg.db.dbname,\n        user=cfg.db.user,\n        password=cfg.db.password,\n        host=cfg.db.host,\n        port=cfg.db.port,\n        table_name=cfg.db.table_name,\n        message_channel=cfg.db.message_channel,\n    )\n\n    logger.info(f\"Executing scheduled destruction ID: {schedule_id}\")\n\n    try:\n        # Mark as executing\n        database.update_scheduled_destruction_status(\n            schedule_id=schedule_id,\n            status=\"executing\",\n        )\n\n        # Run terraform destroy\n        logger.info(\"Running terraform destroy\")\n        cmd = [\n            \"terraform\",\n            \"destroy\",\n            \"-auto-approve\",\n            \"-var-file=terraform.runtime.tfvars\",\n        ]\n\n        subprocess.run(\n            cmd,\n            cwd=terraform_dir,\n            check=True,\n            capture_output=True,\n            text=True,\n            timeout=600,  # 10 min timeout\n        )\n\n        # Clear database\n        logger.info(\"Clearing all VMs from database\")\n        database.clear_database()\n\n        # Mark as completed\n        database.update_scheduled_destruction_status(\n            schedule_id=schedule_id,\n            status=\"completed\",\n            execution_result=\"All VMs destroyed successfully\",\n        )\n\n        logger.info(f\"Scheduled destruction {schedule_id} completed successfully\")\n\n    except subprocess.CalledProcessError as e:\n        error_msg = f\"Terraform destroy failed: {e.stderr}\"\n        logger.error(error_msg)\n\n        database.update_scheduled_destruction_status(\n            schedule_id=schedule_id,\n            status=\"failed\",\n            execution_result=error_msg,\n        )\n\n        # Note: Retry logic would need to be implemented in the scheduler\n        # if this fails, as we can't easily schedule retries from here\n\n    except Exception as e:\n        error_msg = f\"Destruction failed: {str(e)}\"\n        logger.error(error_msg)\n\n        database.update_scheduled_destruction_status(\n            schedule_id=schedule_id,\n            status=\"failed\",\n            execution_result=error_msg,\n        )\n\n    finally:\n        # Close the database connection manually\n        if hasattr(database, \"cursor\") and database.cursor:\n            database.cursor.close()\n        if hasattr(database, \"conn\") and database.conn:\n            database.conn.close()\n        logger.debug(\"Database connection closed.\")\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/validate_config/","title":"lablink_allocator_service.validate_config","text":""},{"location":"reference/allocator/lablink_allocator_service/validate_config/#lablink_allocator_service.validate_config","title":"<code>lablink_allocator_service.validate_config</code>","text":"<p>Config validation CLI for LabLink Allocator Service.</p> <p>This module provides a command-line tool to validate configuration files against the Hydra/OmegaConf schema before deployment. This enables fail-fast validation during CI/CD pipelines.</p>"},{"location":"reference/allocator/lablink_allocator_service/validate_config/#lablink_allocator_service.validate_config-attributes","title":"Attributes","text":""},{"location":"reference/allocator/lablink_allocator_service/validate_config/#lablink_allocator_service.validate_config.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/validate_config/#lablink_allocator_service.validate_config-functions","title":"Functions","text":""},{"location":"reference/allocator/lablink_allocator_service/validate_config/#lablink_allocator_service.validate_config.main","title":"<code>main()</code>","text":"<p>Main entry point for the config validation CLI.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/validate_config.py</code> <pre><code>def main():\n    \"\"\"Main entry point for the config validation CLI.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Validate LabLink allocator configuration file against schema\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  # Validate deployment config\n  lablink-validate-config config/config.yaml\n\n  # Validate runtime config in Docker\n  lablink-validate-config /config/config.yaml\n\n  # Validate bundled config\n  lablink-validate-config \\\\\n      packages/allocator/src/lablink_allocator_service/conf/config.yaml\n\nNOTE: Config file MUST be named 'config.yaml' for schema validation.\n\nExit codes:\n  0 - Config is valid\n  1 - Config is invalid or error occurred\n        \"\"\",\n    )\n\n    parser.add_argument(\n        \"config_path\",\n        nargs=\"?\",\n        default=\"/config/config.yaml\",\n        help=\"Path to the config.yaml file to validate (default: /config/config.yaml)\",\n    )\n\n    parser.add_argument(\n        \"-v\",\n        \"--verbose\",\n        action=\"store_true\",\n        help=\"Enable verbose output\",\n    )\n\n    args = parser.parse_args()\n\n    # Set logging level based on verbosity\n    if args.verbose:\n        logging.getLogger().setLevel(logging.INFO)\n        logging.getLogger(\"lablink_allocator_service\").setLevel(logging.INFO)\n\n    # Validate the configuration\n    is_valid, message = validate_config(args.config_path)\n\n    # Print the result\n    print(message)\n\n    # Exit with appropriate code\n    sys.exit(0 if is_valid else 1)\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/validate_config/#lablink_allocator_service.validate_config.validate_config","title":"<code>validate_config(config_path)</code>","text":"<p>Validate a configuration file against the schema.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the config.yaml file to validate</p> required <p>Returns:</p> Type Description <code>Tuple[bool, str]</code> <p>Tuple of (is_valid, message): - is_valid: True if config is valid, False otherwise - message: Success or error message</p> Source code in <code>packages/allocator/src/lablink_allocator_service/validate_config.py</code> <pre><code>def validate_config(config_path: str) -&gt; Tuple[bool, str]:\n    \"\"\"Validate a configuration file against the schema.\n\n    Args:\n        config_path: Path to the config.yaml file to validate\n\n    Returns:\n        Tuple of (is_valid, message):\n            - is_valid: True if config is valid, False otherwise\n            - message: Success or error message\n    \"\"\"\n    path = Path(config_path)\n\n    # Check if file exists\n    if not path.exists():\n        return False, f\"[FAIL] Config file not found: {config_path}\"\n\n    if not path.is_file():\n        return False, f\"[FAIL] Config path is not a file: {config_path}\"\n\n    # Require config.yaml filename for Hydra schema matching\n    if path.name != \"config.yaml\":\n        return False, (\n            f\"[FAIL] Config file must be named 'config.yaml'\\n\"\n            f\"       Found: {path.name}\\n\"\n            f\"       Rename your file to enable strict schema validation\"\n        )\n\n    try:\n        # Use get_config() with explicit path - it validates automatically\n        cfg = get_config(config_path=path.as_posix())\n\n        # Run logic validation\n        is_valid, error_msg = validate_config_logic(cfg)\n        if not is_valid:\n            return False, error_msg\n\n        return True, \"[PASS] Config validation passed\"\n\n    except ConfigCompositionException as e:\n        # This is the error from your Docker logs - extract the key info\n        error_msg = (\n            \"[FAIL] Config validation failed: Error merging config with schema\\n\"\n        )\n        error_str = str(e)\n\n        # Try to extract the key that caused the problem\n        if \"Key '\" in error_str and \"' not in\" in error_str:\n            # Extract key name from error message\n            key_start = error_str.find(\"Key '\") + 5\n            key_end = error_str.find(\"'\", key_start)\n            bad_key = error_str[key_start:key_end]\n            error_msg += f\"       Unknown key: '{bad_key}'\\n\"\n            error_msg += \"       This key is not defined in the Config schema\\n\"\n        else:\n            error_msg += f\"       {error_str}\\n\"\n\n        return False, error_msg\n\n    except ConfigKeyError as e:\n        error_msg = \"[FAIL] Config validation failed: Unknown configuration key\\n\"\n        error_msg += f\"       Key '{e.key}' not found in schema\"\n        if hasattr(e, \"full_key\") and e.full_key:\n            error_msg += f\"\\n       Full key path: {e.full_key}\"\n        if hasattr(e, \"object_type\") and e.object_type:\n            type_name = (\n                e.object_type.__name__\n                if hasattr(e.object_type, \"__name__\")\n                else str(e.object_type)\n            )\n            error_msg += f\"\\n       Expected in schema: {type_name}\"\n        error_msg += \"\\n\"\n        return False, error_msg\n\n    except ValidationError as e:\n        error_msg = \"[FAIL] Config validation failed: Schema validation error\\n\"\n        error_msg += f\"       {str(e)}\\n\"\n        return False, error_msg\n\n    except Exception as e:\n        logger.exception(\"Unexpected error during config validation\")\n        error_msg = f\"[FAIL] Config validation failed: {type(e).__name__}\\n\"\n        error_msg += f\"       {str(e)}\\n\"\n        return False, error_msg\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/validate_config/#lablink_allocator_service.validate_config.validate_config_logic","title":"<code>validate_config_logic(cfg)</code>","text":"<p>Validate configuration logic and dependencies.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Loaded Hydra/OmegaConf configuration object.</p> required <p>Returns:</p> Type Description <code>Tuple[bool, str]</code> <p>Tuple of (is_valid, error_message)</p> Source code in <code>packages/allocator/src/lablink_allocator_service/validate_config.py</code> <pre><code>def validate_config_logic(cfg: DictConfig) -&gt; Tuple[bool, str]:\n    \"\"\"Validate configuration logic and dependencies.\n\n    Args:\n        cfg: Loaded Hydra/OmegaConf configuration object.\n\n    Returns:\n        Tuple of (is_valid, error_message)\n    \"\"\"\n    errors = []\n\n    # DNS enabled requires non-empty domain\n    if cfg.dns.enabled and not cfg.dns.domain:\n        errors.append(\"DNS enabled requires non-empty domain field\")\n\n    # Validate domain format\n    is_valid, error_msg = validate_domain_format(cfg.dns.domain)\n    if not is_valid:\n        errors.append(error_msg)\n\n    # SSL (non-\"none\") requires DNS\n    if cfg.ssl.provider != \"none\" and not cfg.dns.enabled:\n        errors.append(\n            'SSL requires DNS to be enabled (use provider=\"none\" for HTTP-only)'\n        )\n\n    # Let's Encrypt requires email\n    if cfg.ssl.provider == \"letsencrypt\" and not cfg.ssl.email:\n        errors.append(\"Let's Encrypt requires email address\")\n\n    # ACM requires certificate_arn\n    if cfg.ssl.provider == \"acm\" and not cfg.ssl.certificate_arn:\n        errors.append(\"ACM provider requires certificate_arn\")\n\n    # CloudFlare SSL requires external DNS (terraform_managed=false)\n    if cfg.ssl.provider == \"cloudflare\" and cfg.dns.terraform_managed:\n        errors.append(\n            \"CloudFlare SSL requires terraform_managed=false (external DNS management)\"\n        )\n\n    if errors:\n        error_msg = \"[FAIL] Config validation failed:\\n\"\n        for error in errors:\n            error_msg += f\"       - {error}\\n\"\n        return False, error_msg\n\n    return True, \"\"\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/validate_config/#lablink_allocator_service.validate_config.validate_domain_format","title":"<code>validate_domain_format(domain)</code>","text":"<p>Validate domain format to prevent malformed domains.</p> <p>Parameters:</p> Name Type Description Default <code>domain</code> <code>str</code> <p>Domain name to validate</p> required <p>Returns:</p> Type Description <code>Tuple[bool, str]</code> <p>Tuple of (is_valid, error_message)</p> Source code in <code>packages/allocator/src/lablink_allocator_service/validate_config.py</code> <pre><code>def validate_domain_format(domain: str) -&gt; Tuple[bool, str]:\n    \"\"\"Validate domain format to prevent malformed domains.\n\n    Args:\n        domain: Domain name to validate\n\n    Returns:\n        Tuple of (is_valid, error_message)\n    \"\"\"\n    if not domain:\n        return True, \"\"  # Empty is allowed when DNS disabled\n\n    # Check for leading/trailing dots\n    if domain.startswith(\".\"):\n        return False, \"Domain cannot start with a dot\"\n    if domain.endswith(\".\"):\n        return False, \"Domain cannot end with a dot\"\n\n    return True, \"\"\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/conf/","title":"lablink_allocator_service.conf","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/#lablink_allocator_service.conf","title":"<code>lablink_allocator_service.conf</code>","text":"<p>Configuration module for LabLink Allocator Service.</p>"},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/","title":"lablink_allocator_service.conf.structured_config","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config","title":"<code>lablink_allocator_service.conf.structured_config</code>","text":"<p>This module defines the database configuration structure for the LabLink Allocator Service.</p>"},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config-attributes","title":"Attributes","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.cs","title":"<code>cs = ConfigStore.instance()</code>  <code>module-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config-classes","title":"Classes","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.AllocatorConfig","title":"<code>AllocatorConfig(image_tag='linux-amd64-latest')</code>  <code>dataclass</code>","text":"<p>Configuration for allocator service deployment.</p> <p>This section is used by infrastructure deployment (Terraform) to specify which Docker image tag to use for the allocator service. The allocator service itself doesn't use this field, but it must be present in the schema to accept infrastructure configuration files.</p> <p>Attributes:</p> Name Type Description <code>image_tag</code> <code>str</code> <p>Docker image tag for the allocator service. Examples: \"linux-amd64-latest-test\", \"linux-amd64-v1.2.3\"</p>"},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.AllocatorConfig-attributes","title":"Attributes","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.AllocatorConfig.image_tag","title":"<code>image_tag = field(default='linux-amd64-latest')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.AppConfig","title":"<code>AppConfig(admin_user='admin', admin_password='admin_password', region='us-west-2')</code>  <code>dataclass</code>","text":"<p>Configuration for the LabLink Allocator Service application.</p> <p>Attributes:</p> Name Type Description <code>admin_user</code> <code>str</code> <p>The username for the admin user.</p> <code>admin_password</code> <code>str</code> <p>The password for the admin user.</p> <code>region</code> <code>str</code> <p>The AWS region where the service is deployed.</p>"},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.AppConfig-attributes","title":"Attributes","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.AppConfig.admin_password","title":"<code>admin_password = field(default='admin_password')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.AppConfig.admin_user","title":"<code>admin_user = field(default='admin')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.AppConfig.region","title":"<code>region = field(default='us-west-2')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.BudgetConfig","title":"<code>BudgetConfig(enabled=False, monthly_budget_usd=500)</code>  <code>dataclass</code>","text":"<p>Configuration for budget limits.</p> <p>Attributes:</p> Name Type Description <code>enabled</code> <code>bool</code> <p>Whether budget monitoring is enabled.</p> <code>monthly_budget_usd</code> <code>int</code> <p>Monthly budget in USD.</p>"},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.BudgetConfig-attributes","title":"Attributes","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.BudgetConfig.enabled","title":"<code>enabled = field(default=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.BudgetConfig.monthly_budget_usd","title":"<code>monthly_budget_usd = field(default=500)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.CloudTrailConfig","title":"<code>CloudTrailConfig(retention_days=90)</code>  <code>dataclass</code>","text":"<p>Configuration for CloudTrail logging.</p> <p>Attributes:</p> Name Type Description <code>retention_days</code> <code>int</code> <p>Number of days to retain CloudTrail logs.</p>"},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.CloudTrailConfig-attributes","title":"Attributes","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.CloudTrailConfig.retention_days","title":"<code>retention_days = field(default=90)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.Config","title":"<code>Config(db=DatabaseConfig(), machine=MachineConfig(), app=AppConfig(), dns=DNSConfig(), eip=EIPConfig(), ssl=SSLConfig(), allocator=AllocatorConfig(), bucket_name='tf-state-lablink-allocator-bucket', startup_script=StartupConfig(), monitoring=MonitoringConfig())</code>  <code>dataclass</code>","text":"<p>Configuration for the LabLink Allocator Service. This class aggregates the database, machine, and application configurations.</p> <p>Attributes:</p> Name Type Description <code>db</code> <code>DatabaseConfig</code> <p>The database configuration.</p> <code>machine</code> <code>MachineConfig</code> <p>The machine configuration.</p> <code>app</code> <code>AppConfig</code> <p>The application configuration.</p> <code>dns</code> <code>DNSConfig</code> <p>The DNS configuration.</p> <code>eip</code> <code>EIPConfig</code> <p>The EIP management configuration.</p> <code>ssl</code> <code>SSLConfig</code> <p>The SSL certificate configuration.</p> <code>allocator</code> <code>AllocatorConfig</code> <p>The allocator deployment configuration.</p> <code>bucket_name</code> <code>str</code> <p>The S3 bucket name for Terraform state.</p>"},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.Config-attributes","title":"Attributes","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.Config.allocator","title":"<code>allocator = field(default_factory=AllocatorConfig)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.Config.app","title":"<code>app = field(default_factory=AppConfig)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.Config.bucket_name","title":"<code>bucket_name = field(default='tf-state-lablink-allocator-bucket')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.Config.db","title":"<code>db = field(default_factory=DatabaseConfig)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.Config.dns","title":"<code>dns = field(default_factory=DNSConfig)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.Config.eip","title":"<code>eip = field(default_factory=EIPConfig)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.Config.machine","title":"<code>machine = field(default_factory=MachineConfig)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.Config.monitoring","title":"<code>monitoring = field(default_factory=MonitoringConfig)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.Config.ssl","title":"<code>ssl = field(default_factory=SSLConfig)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.Config.startup_script","title":"<code>startup_script = field(default_factory=StartupConfig)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.DNSConfig","title":"<code>DNSConfig(enabled=False, terraform_managed=True, domain='', zone_id='')</code>  <code>dataclass</code>","text":"<p>Configuration for DNS and domain setup.</p> <p>This class defines DNS settings for Route 53 hosted zones and records. DNS can be disabled entirely, or configured with external DNS providers.</p> <p>Attributes:</p> Name Type Description <code>enabled</code> <code>bool</code> <p>Whether DNS is enabled. If False, only IP addresses are used.</p> <code>terraform_managed</code> <code>bool</code> <p>Whether Terraform creates/destroys DNS records. - True: Terraform manages Route53 DNS records (creates and destroys) - False: External DNS management (CloudFlare, manual Route53, etc.)</p> <code>domain</code> <code>str</code> <p>Full domain name for the allocator (e.g., \"lablink.sleap.ai\", \"test.lablink.sleap.ai\"). Required when enabled=true. Supports sub-subdomains for environment separation.</p> <code>zone_id</code> <code>str</code> <p>Optional Route53 hosted zone ID. If provided, skips zone lookup. Use this when zone lookup finds the wrong zone (e.g., parent vs subdomain).</p>"},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.DNSConfig-attributes","title":"Attributes","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.DNSConfig.domain","title":"<code>domain = field(default='')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.DNSConfig.enabled","title":"<code>enabled = field(default=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.DNSConfig.terraform_managed","title":"<code>terraform_managed = field(default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.DNSConfig.zone_id","title":"<code>zone_id = field(default='')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.DatabaseConfig","title":"<code>DatabaseConfig(dbname='lablink_db', user='lablink', password='lablink', host='localhost', port=5432, table_name='vm_table', message_channel='vm_updates')</code>  <code>dataclass</code>","text":"<p>Configuration for the database used in the LabLink Allocator Service. This class defines the connection parameters for the database, including the name, user, password, host, port, table name, and message channel.</p> <p>Attributes:</p> Name Type Description <code>dbname</code> <code>str</code> <p>The name of the database.</p> <code>user</code> <code>str</code> <p>The username for the database.</p> <code>password</code> <code>str</code> <p>The password for the database.</p> <code>host</code> <code>str</code> <p>The host where the database is located.</p> <code>port</code> <code>int</code> <p>The port on which the database is running.</p> <code>table_name</code> <code>str</code> <p>The name of the table to store VM information.</p> <code>message_channel</code> <code>str</code> <p>The name of the message channel for updates.</p>"},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.DatabaseConfig-attributes","title":"Attributes","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.DatabaseConfig.dbname","title":"<code>dbname = field(default='lablink_db')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.DatabaseConfig.host","title":"<code>host = field(default='localhost')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.DatabaseConfig.message_channel","title":"<code>message_channel = field(default='vm_updates')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.DatabaseConfig.password","title":"<code>password = field(default='lablink')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.DatabaseConfig.port","title":"<code>port = field(default=5432)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.DatabaseConfig.table_name","title":"<code>table_name = field(default='vm_table')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.DatabaseConfig.user","title":"<code>user = field(default='lablink')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.EIPConfig","title":"<code>EIPConfig(strategy='dynamic', tag_name='lablink-eip')</code>  <code>dataclass</code>","text":"<p>Configuration for Elastic IP management strategy.</p> <p>Attributes:</p> Name Type Description <code>strategy</code> <code>str</code> <p>EIP allocation strategy. Options: - \"persistent\": Reuse existing tagged EIP across deployments - \"dynamic\": Create new EIP for each deployment</p> <code>tag_name</code> <code>str</code> <p>Name tag value to identify reusable EIPs</p>"},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.EIPConfig-attributes","title":"Attributes","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.EIPConfig.strategy","title":"<code>strategy = field(default='dynamic')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.EIPConfig.tag_name","title":"<code>tag_name = field(default='lablink-eip')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.MachineConfig","title":"<code>MachineConfig(machine_type='g4dn.xlarge', repository=None, image='ghcr.io/talmolab/lablink-client-base-image:latest', ami_id='ami-00c257e12d6828491', software='sleap', extension='slp')</code>  <code>dataclass</code>","text":"<p>Configuration for the machine used in the LabLink Allocator Service. This class defines the machine type, repository, image, AMI ID, and software to be used.</p> <p>Attributes:</p> Name Type Description <code>machine_type</code> <code>str</code> <p>The type of the machine to be used.</p> <code>repository</code> <code>Optional[str]</code> <p>The repository URL for the machine image.</p> <code>image</code> <code>str</code> <p>The Docker image ID to be used for the machine.</p> <code>ami_id</code> <code>str</code> <p>The Amazon Machine Image (AMI) ID for the machine.</p> <code>software</code> <code>str</code> <p>The software to be installed on the machine.</p> <code>extension</code> <code>str</code> <p>The file extension associated with the software.</p>"},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.MachineConfig-attributes","title":"Attributes","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.MachineConfig.ami_id","title":"<code>ami_id = field(default='ami-00c257e12d6828491')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.MachineConfig.extension","title":"<code>extension = field(default='slp')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.MachineConfig.image","title":"<code>image = field(default='ghcr.io/talmolab/lablink-client-base-image:latest')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.MachineConfig.machine_type","title":"<code>machine_type = field(default='g4dn.xlarge')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.MachineConfig.repository","title":"<code>repository = field(default=None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.MachineConfig.software","title":"<code>software = field(default='sleap')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.MonitoringConfig","title":"<code>MonitoringConfig(enabled=False, email='', thresholds=ThresholdsConfig(), budget=BudgetConfig(), cloudtrail=CloudTrailConfig())</code>  <code>dataclass</code>","text":"<p>Configuration for monitoring and logging.</p> <p>Attributes:</p> Name Type Description <code>enabled</code> <code>bool</code> <p>Whether monitoring is enabled.</p> <code>email</code> <code>str</code> <p>Email address to send alerts to.</p> <code>thresholds</code> <code>ThresholdsConfig</code> <p>Resource usage thresholds for triggering alerts.</p> <code>budget</code> <code>BudgetConfig</code> <p>Budget limits for monitoring costs.</p> <code>cloudtrail</code> <code>CloudTrailConfig</code> <p>CloudTrail logging configuration.</p>"},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.MonitoringConfig-attributes","title":"Attributes","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.MonitoringConfig.budget","title":"<code>budget = field(default_factory=BudgetConfig)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.MonitoringConfig.cloudtrail","title":"<code>cloudtrail = field(default_factory=CloudTrailConfig)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.MonitoringConfig.email","title":"<code>email = field(default='')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.MonitoringConfig.enabled","title":"<code>enabled = field(default=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.MonitoringConfig.thresholds","title":"<code>thresholds = field(default_factory=ThresholdsConfig)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.SSLConfig","title":"<code>SSLConfig(provider='letsencrypt', email='', certificate_arn='')</code>  <code>dataclass</code>","text":"<p>Configuration for SSL/TLS certificate management.</p> <p>Attributes:</p> Name Type Description <code>provider</code> <code>str</code> <p>SSL provider. Options: - \"none\": HTTP only, no SSL - \"letsencrypt\": Automatic SSL via Caddy + Let's Encrypt - \"cloudflare\": CloudFlare proxy handles SSL   (requires terraform_managed=false) - \"acm\": AWS Certificate Manager (requires ALB, certificate_arn)</p> <code>email</code> <code>str</code> <p>Email address for Let's Encrypt notifications (required when provider=\"letsencrypt\")</p> <code>certificate_arn</code> <code>str</code> <p>AWS ACM certificate ARN (required when provider=\"acm\")</p>"},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.SSLConfig-attributes","title":"Attributes","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.SSLConfig.certificate_arn","title":"<code>certificate_arn = field(default='')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.SSLConfig.email","title":"<code>email = field(default='')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.SSLConfig.provider","title":"<code>provider = field(default='letsencrypt')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.StartupConfig","title":"<code>StartupConfig(enabled=False, path='', on_error='continue')</code>  <code>dataclass</code>","text":"<p>Configuration for startup behavior of the allocator service. Attributes:     enabled (bool): Whether startup script execution is enabled.     path (str): Path to the startup script to be executed.     on_error (str): Behavior on startup script error. Options:         - \"continue\": Log the error and continue startup.         - \"fail\": Abort startup on error.</p>"},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.StartupConfig-attributes","title":"Attributes","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.StartupConfig.enabled","title":"<code>enabled = field(default=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.StartupConfig.on_error","title":"<code>on_error = field(default='continue')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.StartupConfig.path","title":"<code>path = field(default='')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.ThresholdsConfig","title":"<code>ThresholdsConfig(max_instances_per_5min=10, max_terminations_per_5min=20, max_unauthorized_calls_per_15min=5)</code>  <code>dataclass</code>","text":"<p>Configuration for resource usage thresholds.</p> <p>Attributes:</p> Name Type Description <code>max_instances_per_5min</code> <code>int</code> <p>Maximum number of instances that can be created within a 5-minute window.</p> <code>max_terminations_per_5min</code> <code>int</code> <p>Maximum number of instances that can be terminated within a 5-minute window.</p> <code>max_unauthorized_calls_per_15min</code> <code>int</code> <p>Maximum number of unauthorized API calls that can be made within a 15-minute window.</p>"},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.ThresholdsConfig-attributes","title":"Attributes","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.ThresholdsConfig.max_instances_per_5min","title":"<code>max_instances_per_5min = field(default=10)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.ThresholdsConfig.max_terminations_per_5min","title":"<code>max_terminations_per_5min = field(default=20)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/conf/structured_config/#lablink_allocator_service.conf.structured_config.ThresholdsConfig.max_unauthorized_calls_per_15min","title":"<code>max_unauthorized_calls_per_15min = field(default=5)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/utils/","title":"lablink_allocator_service.utils","text":""},{"location":"reference/allocator/lablink_allocator_service/utils/#lablink_allocator_service.utils","title":"<code>lablink_allocator_service.utils</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/utils/aws_utils/","title":"lablink_allocator_service.utils.aws_utils","text":""},{"location":"reference/allocator/lablink_allocator_service/utils/aws_utils/#lablink_allocator_service.utils.aws_utils","title":"<code>lablink_allocator_service.utils.aws_utils</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/utils/aws_utils/#lablink_allocator_service.utils.aws_utils-attributes","title":"Attributes","text":""},{"location":"reference/allocator/lablink_allocator_service/utils/aws_utils/#lablink_allocator_service.utils.aws_utils.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/utils/aws_utils/#lablink_allocator_service.utils.aws_utils-functions","title":"Functions","text":""},{"location":"reference/allocator/lablink_allocator_service/utils/aws_utils/#lablink_allocator_service.utils.aws_utils.check_support_nvidia","title":"<code>check_support_nvidia(machine_type)</code>","text":"<p>Check if a given EC2 instance type supports NVIDIA GPUs. Args:     machine_type (str): The EC2 instance type to check. Returns:     bool: True if the instance type supports NVIDIA GPUs, False otherwise.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/utils/aws_utils.py</code> <pre><code>def check_support_nvidia(machine_type) -&gt; bool:\n    \"\"\"Check if a given EC2 instance type supports NVIDIA GPUs.\n    Args:\n        machine_type (str): The EC2 instance type to check.\n    Returns:\n        bool: True if the instance type supports NVIDIA GPUs, False otherwise.\n    \"\"\"\n    kwargs = {\n        \"region_name\": os.getenv(\"AWS_REGION\", \"us-west-2\"),\n        \"aws_access_key_id\": os.getenv(\"AWS_ACCESS_KEY_ID\"),\n        \"aws_secret_access_key\": os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n    }\n    if os.getenv(\"AWS_SESSION_TOKEN\"):\n        kwargs[\"aws_session_token\"] = os.getenv(\"AWS_SESSION_TOKEN\")\n\n    ec2 = boto3.client(\"ec2\", **kwargs)\n    try:\n        response = ec2.describe_instance_types(InstanceTypes=[machine_type])\n        gpu_info = response[\"InstanceTypes\"][0].get(\"GpuInfo\", {})\n\n        # Check if GPU is present\n        if not gpu_info:\n            logger.debug(f\"No GPU info found for instance type {machine_type}.\")\n            return False\n\n        # Check if any GPU supports NVIDIA\n        for gpu in gpu_info.get(\"Gpus\", []):\n            if \"NVIDIA\" in gpu.get(\"Manufacturer\", \"\"):\n                logger.info(f\"Instance type {machine_type} supports NVIDIA GPUs.\")\n                return True\n\n        logger.debug(f\"Instance type {machine_type} does not support NVIDIA GPUs.\")\n    except ClientError as e:\n        logger.error(f\"Error checking NVIDIA support for {machine_type}: {e}\")\n    return False\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/utils/aws_utils/#lablink_allocator_service.utils.aws_utils.get_all_instance_types","title":"<code>get_all_instance_types(region='us-west-2')</code>","text":"<p>Fetch all available EC2 instance types in a given AWS region. Args:     region (str): The AWS region to query for instance types. Returns:     list: A list of available EC2 instance types in the specified region.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/utils/aws_utils.py</code> <pre><code>def get_all_instance_types(region=\"us-west-2\"):\n    \"\"\"Fetch all available EC2 instance types in a given AWS region.\n    Args:\n        region (str): The AWS region to query for instance types.\n    Returns:\n        list: A list of available EC2 instance types in the specified region.\n    \"\"\"\n    kwargs = {\n        \"region_name\": region,\n        \"aws_access_key_id\": os.getenv(\"AWS_ACCESS_KEY_ID\"),\n        \"aws_secret_access_key\": os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n    }\n    if os.getenv(\"AWS_SESSION_TOKEN\"):\n        kwargs[\"aws_session_token\"] = os.getenv(\"AWS_SESSION_TOKEN\")\n\n    ec2 = boto3.client(\"ec2\", **kwargs)\n    instance_types = []\n    paginator = ec2.get_paginator(\"describe_instance_types\")\n    for page in paginator.paginate():\n        for itype in page[\"InstanceTypes\"]:\n            instance_types.append(itype[\"InstanceType\"])\n    return instance_types\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/utils/aws_utils/#lablink_allocator_service.utils.aws_utils.upload_to_s3","title":"<code>upload_to_s3(local_path, env, bucket_name, region, kms_key_id=None)</code>","text":"<p>Uploads a file to an S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>local_path</code> <code>Path</code> <p>The local file path to upload.</p> required <code>env</code> <code>str</code> <p>The environment (e.g., dev, test, prod) for the upload.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the S3 bucket to upload to.</p> required <code>region</code> <code>str</code> <p>The AWS region where the S3 bucket is located.</p> required <code>kms_key_id</code> <code>Optional[str]</code> <p>The KMS key ID for server-side encryption. Defaults to None.</p> <code>None</code> Source code in <code>packages/allocator/src/lablink_allocator_service/utils/aws_utils.py</code> <pre><code>def upload_to_s3(\n    local_path: Path,\n    env: str,\n    bucket_name: str,\n    region: str,\n    kms_key_id: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Uploads a file to an S3 bucket.\n\n    Args:\n        local_path (Path): The local file path to upload.\n        env (str): The environment (e.g., dev, test, prod) for the upload.\n        bucket_name (str): The name of the S3 bucket to upload to.\n        region (str): The AWS region where the S3 bucket is located.\n        kms_key_id (Optional[str], optional): The KMS key ID for server-side encryption.\n            Defaults to None.\n    \"\"\"\n    s3 = boto3.client(\"s3\", region_name=region)\n    key = f\"{env}/client/{local_path.name}\"\n    extra = {\"ContentType\": \"text/plain\"}\n    if kms_key_id:\n        extra.update({\"ServerSideEncryption\": \"aws:kms\", \"SSEKMSKeyId\": kms_key_id})\n\n    # Upload the variable file\n    s3.upload_file(local_path, bucket_name, key, ExtraArgs=extra)\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/utils/config_helpers/","title":"lablink_allocator_service.utils.config_helpers","text":""},{"location":"reference/allocator/lablink_allocator_service/utils/config_helpers/#lablink_allocator_service.utils.config_helpers","title":"<code>lablink_allocator_service.utils.config_helpers</code>","text":"<p>Configuration helper functions for building URLs and determining settings.</p>"},{"location":"reference/allocator/lablink_allocator_service/utils/config_helpers/#lablink_allocator_service.utils.config_helpers-attributes","title":"Attributes","text":""},{"location":"reference/allocator/lablink_allocator_service/utils/config_helpers/#lablink_allocator_service.utils.config_helpers.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/utils/config_helpers/#lablink_allocator_service.utils.config_helpers-functions","title":"Functions","text":""},{"location":"reference/allocator/lablink_allocator_service/utils/config_helpers/#lablink_allocator_service.utils.config_helpers.get_allocator_url","title":"<code>get_allocator_url(cfg, allocator_ip)</code>","text":"<p>Build the allocator URL based on configuration.</p> <p>Priority order: 1. ALLOCATOR_FQDN environment variable (set by Terraform) 2. DNS configuration from config 3. IP address fallback</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Hydra/OmegaConf configuration object.</p> required <code>allocator_ip</code> <code>str</code> <p>Public IP address of allocator.</p> required <p>Returns:</p> Type Description <code>Tuple[str, str]</code> <p>Tuple of (base_url, protocol)</p> <p>Examples:</p> <p>ALLOCATOR_FQDN environment variable:     (\"https://test.lablink.sleap.ai\", \"https\")</p> <p>DNS enabled + Let's Encrypt SSL:     (\"https://test.lablink.sleap.ai\", \"https\")</p> <p>DNS disabled + No SSL:     (\"http://52.40.142.146\", \"http\")</p> Source code in <code>packages/allocator/src/lablink_allocator_service/utils/config_helpers.py</code> <pre><code>def get_allocator_url(cfg: DictConfig, allocator_ip: str) -&gt; Tuple[str, str]:\n    \"\"\"\n    Build the allocator URL based on configuration.\n\n    Priority order:\n    1. ALLOCATOR_FQDN environment variable (set by Terraform)\n    2. DNS configuration from config\n    3. IP address fallback\n\n    Args:\n        cfg: Hydra/OmegaConf configuration object.\n        allocator_ip: Public IP address of allocator.\n\n    Returns:\n        Tuple of (base_url, protocol)\n\n    Examples:\n        ALLOCATOR_FQDN environment variable:\n            (\"https://test.lablink.sleap.ai\", \"https\")\n\n        DNS enabled + Let's Encrypt SSL:\n            (\"https://test.lablink.sleap.ai\", \"https\")\n\n        DNS disabled + No SSL:\n            (\"http://52.40.142.146\", \"http\")\n    \"\"\"\n    # Priority 1: Check for ALLOCATOR_FQDN environment variable (set by Terraform)\n    allocator_fqdn = os.getenv(\"ALLOCATOR_FQDN\")\n    if allocator_fqdn:\n        # FQDN already includes protocol\n        if allocator_fqdn.startswith(\"https://\"):\n            protocol = \"https\"\n        elif allocator_fqdn.startswith(\"http://\"):\n            protocol = \"http\"\n        else:\n            # Default to http if no protocol specified\n            protocol = \"http\"\n            allocator_fqdn = f\"{protocol}://{allocator_fqdn}\"\n\n        logger.info(f\"Using ALLOCATOR_FQDN from environment: {allocator_fqdn}\")\n        return allocator_fqdn, protocol\n\n    # Priority 2: Build from DNS configuration\n    # Determine protocol based on SSL provider\n    if hasattr(cfg, \"ssl\") and cfg.ssl.provider != \"none\":\n        protocol = \"https\"\n    else:\n        protocol = \"http\"\n\n    # Determine host based on DNS configuration\n    if hasattr(cfg, \"dns\") and cfg.dns.enabled and cfg.dns.domain:\n        # Use DNS domain directly (now includes full domain)\n        host = cfg.dns.domain\n\n        # Remove leading dots if present (safety check)\n        if host.startswith(\".\"):\n            host = host[1:]\n            logger.warning(f\"Removed leading dot from domain: {host}\")\n\n        logger.info(f\"Using domain from config: {host}\")\n    else:\n        # Priority 3: Use IP address\n        host = allocator_ip\n        logger.info(f\"Using IP-only mode: {host}\")\n\n    base_url = f\"{protocol}://{host}\"\n\n    return base_url, protocol\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/utils/config_helpers/#lablink_allocator_service.utils.config_helpers.should_use_dns","title":"<code>should_use_dns(cfg)</code>","text":"<p>Check if DNS is enabled in config.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/utils/config_helpers.py</code> <pre><code>def should_use_dns(cfg) -&gt; bool:\n    \"\"\"Check if DNS is enabled in config.\"\"\"\n    return hasattr(cfg, \"dns\") and cfg.dns.enabled\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/utils/config_helpers/#lablink_allocator_service.utils.config_helpers.should_use_https","title":"<code>should_use_https(cfg)</code>","text":"<p>Check if HTTPS is enabled in config.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/utils/config_helpers.py</code> <pre><code>def should_use_https(cfg) -&gt; bool:\n    \"\"\"Check if HTTPS is enabled in config.\"\"\"\n    return hasattr(cfg, \"ssl\") and cfg.ssl.provider != \"none\"\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/utils/scp/","title":"lablink_allocator_service.utils.scp","text":""},{"location":"reference/allocator/lablink_allocator_service/utils/scp/#lablink_allocator_service.utils.scp","title":"<code>lablink_allocator_service.utils.scp</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/utils/scp/#lablink_allocator_service.utils.scp-attributes","title":"Attributes","text":""},{"location":"reference/allocator/lablink_allocator_service/utils/scp/#lablink_allocator_service.utils.scp.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/utils/scp/#lablink_allocator_service.utils.scp-functions","title":"Functions","text":""},{"location":"reference/allocator/lablink_allocator_service/utils/scp/#lablink_allocator_service.utils.scp.extract_files_from_docker","title":"<code>extract_files_from_docker(ip, key_path, files)</code>","text":"<p>SSH into the EC2 VM and extract files from the running container to the EC2 host file system under /home/ubuntu/extracted_files. Args:     ip (str): The public IP address of the EC2 instance.     key_path (str): The path to the SSH private key file.     files (list[str]): A list of file paths to extract from the container. Raises:     subprocess.CalledProcessError: If the SSH command fails.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/utils/scp.py</code> <pre><code>def extract_files_from_docker(ip: str, key_path: str, files: list[str]) -&gt; None:\n    \"\"\"\n    SSH into the EC2 VM and extract files from the running container to the EC2\n    host file system under /home/ubuntu/extracted_files.\n    Args:\n        ip (str): The public IP address of the EC2 instance.\n        key_path (str): The path to the SSH private key file.\n        files (list[str]): A list of file paths to extract from the container.\n    Raises:\n        subprocess.CalledProcessError: If the SSH command fails.\n    \"\"\"\n    for file in files:\n        rel_path = file.replace(\"/home/client/Desktop/\", \"\")\n        dest_path = f\"/home/ubuntu/extracted_files/{rel_path}\"\n        dest_dir = Path(dest_path).parent\n        cmd = (\n            \"cid=$(sudo docker ps -q | head -n1) &amp;&amp; \"\n            f\"mkdir -p {dest_dir} &amp;&amp; \"\n            f\"sudo docker cp $cid:'{file}' '{dest_path}'\"\n        )\n        ssh_cmd = [\n            \"ssh\",\n            \"-o\",\n            \"StrictHostKeyChecking=no\",\n            \"-i\",\n            key_path,\n            f\"ubuntu@{ip}\",\n            cmd,\n        ]\n\n        try:\n            subprocess.run(ssh_cmd, check=True)\n        except subprocess.CalledProcessError as e:\n            logging.error(f\"Failed to copy {file} from container on {ip}: {e}\")\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/utils/scp/#lablink_allocator_service.utils.scp.find_files_in_container","title":"<code>find_files_in_container(ip, key_path, extension)</code>","text":"<p>SSH into the EC2 VM and find all files in the running Docker container. Args:     ip (str): The public IP address of the EC2 instance.     key_path (str): The path to the SSH private key file.     extension (str): The file extension to search for. Returns:     list[str]: A list of paths to files found in the container.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/utils/scp.py</code> <pre><code>def find_files_in_container(ip: str, key_path: str, extension: str) -&gt; list[str]:\n    \"\"\"SSH into the EC2 VM and find all files in the running Docker container.\n    Args:\n        ip (str): The public IP address of the EC2 instance.\n        key_path (str): The path to the SSH private key file.\n        extension (str): The file extension to search for.\n    Returns:\n        list[str]: A list of paths to files found in the container.\n    \"\"\"\n    cmd = (\n        \"cid=$(sudo docker ps -q | head -n1) &amp;&amp; \"\n        f\"sudo docker exec $cid find /home/client/Desktop -name '*.{extension}' \"\n        \"-not -path '*/models/*' -not -path '*/predictions/*'\"\n    )\n    ssh_cmd = [\n        \"ssh\",\n        \"-o\",\n        \"StrictHostKeyChecking=no\",\n        \"-i\",\n        key_path,\n        f\"ubuntu@{ip}\",\n        cmd,\n    ]\n\n    try:\n        result = subprocess.run(ssh_cmd, check=True, capture_output=True, text=True)\n        files = result.stdout.strip().split(\"\\n\")\n        return [f for f in files if f.strip()]\n    except subprocess.CalledProcessError as e:\n        logging.error(f\"Error finding .{extension} files in container on {ip}: {e}\")\n        return []\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/utils/scp/#lablink_allocator_service.utils.scp.has_files","title":"<code>has_files(ip, key_path, extension)</code>","text":"<p>Check if the target VM has specific files in /home/ubuntu/extracted_files. Args:     ip (str): The public IP address of the EC2 instance.     key_path (str): The path to the SSH private key file.     extension (str): The file extension to check for.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/utils/scp.py</code> <pre><code>def has_files(ip: str, key_path: str, extension: str) -&gt; bool:\n    \"\"\"Check if the target VM has specific files in /home/ubuntu/extracted_files.\n    Args:\n        ip (str): The public IP address of the EC2 instance.\n        key_path (str): The path to the SSH private key file.\n        extension (str): The file extension to check for.\n    \"\"\"\n    ssh_cmd = [\n        \"ssh\",\n        \"-o\",\n        \"StrictHostKeyChecking=no\",\n        \"-i\",\n        key_path,\n        f\"ubuntu@{ip}\",\n        (\n            f\"sh -c 'ls /home/ubuntu/extracted_files/*.{extension} \"\n            \"1&gt;/dev/null 2&gt;/dev/null \"\n            \"&amp;&amp; echo exists || echo missing'\"\n        ),\n    ]\n    result = subprocess.run(ssh_cmd, capture_output=True, text=True)\n    return result.stdout.strip() == \"exists\"\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/utils/scp/#lablink_allocator_service.utils.scp.rsync_files_to_allocator","title":"<code>rsync_files_to_allocator(ip, key_path, local_dir, extension)</code>","text":"<p>Copy specific files from the target VM's file system to the allocator's docker container.</p> <p>Parameters:</p> Name Type Description Default <code>ip</code> <code>str</code> <p>The public IP address of the EC2 instance.</p> required <code>key_path</code> <code>str</code> <p>The path to the SSH private key file.</p> required <code>extension</code> <code>str</code> <p>The file extension to copy.</p> required <code>local_dir</code> <code>str</code> <p>The local directory where the files will be copied.</p> required Source code in <code>packages/allocator/src/lablink_allocator_service/utils/scp.py</code> <pre><code>def rsync_files_to_allocator(\n    ip: str, key_path: str, local_dir: str, extension: str\n) -&gt; None:\n    \"\"\"Copy specific files from the target VM's file system to the allocator's docker\n    container.\n\n    Args:\n        ip (str): The public IP address of the EC2 instance.\n        key_path (str): The path to the SSH private key file.\n        extension (str): The file extension to copy.\n        local_dir (str): The local directory where the files will be copied.\n    \"\"\"\n    logger.debug(f\"Copying the .{extension} files from VM {ip} to allocator...\")\n    cmd = [\n        \"rsync\",\n        \"-avz\",\n        \"--include\",\n        \"**/\",\n        \"--include\",\n        f\"**.{extension}\",\n        \"--exclude\",\n        \"*\",\n        \"-e\",\n        f\"ssh -o StrictHostKeyChecking=no -i {key_path}\",\n        f\"ubuntu@{ip}:/home/ubuntu/extracted_files/\",\n        f\"{local_dir}/\",\n    ]\n\n    if has_files(ip, key_path, extension):\n        logger.debug(f\"Copying .{extension} files from {ip} to {local_dir}\")\n        try:\n            result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n            logger.info(f\"Rsync stdout:\\n{result.stdout}\")\n            logger.debug(f\"Data downloaded to {local_dir}\")\n        except subprocess.CalledProcessError as e:\n            logger.error(f\"Error copying .{extension} files from {ip}: {e}\")\n            logger.error(\"Rsync stderr:\\n\" + e.stderr)\n            raise\n    else:\n        logger.info(f\"No .{extension} files found on VM {ip}. Skipping...\")\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/utils/terraform_utils/","title":"lablink_allocator_service.utils.terraform_utils","text":""},{"location":"reference/allocator/lablink_allocator_service/utils/terraform_utils/#lablink_allocator_service.utils.terraform_utils","title":"<code>lablink_allocator_service.utils.terraform_utils</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/utils/terraform_utils/#lablink_allocator_service.utils.terraform_utils-attributes","title":"Attributes","text":""},{"location":"reference/allocator/lablink_allocator_service/utils/terraform_utils/#lablink_allocator_service.utils.terraform_utils.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/allocator/lablink_allocator_service/utils/terraform_utils/#lablink_allocator_service.utils.terraform_utils-functions","title":"Functions","text":""},{"location":"reference/allocator/lablink_allocator_service/utils/terraform_utils/#lablink_allocator_service.utils.terraform_utils.get_instance_ids","title":"<code>get_instance_ids(terraform_dir)</code>","text":"<p>Get the instance IDs of the instances created by Terraform. Args:     terraform_dir (str): The directory where the Terraform configuration is located. Raises:     RuntimeError: Error running terraform output command.     RuntimeError: Error decoding JSON output.     ValueError: Expected output to be a list of instance IDs. Returns:     list: A list of instance IDs of the instances.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/utils/terraform_utils.py</code> <pre><code>def get_instance_ids(terraform_dir: str) -&gt; list:\n    \"\"\"Get the instance IDs of the instances created by Terraform.\n    Args:\n        terraform_dir (str): The directory where the Terraform configuration is located.\n    Raises:\n        RuntimeError: Error running terraform output command.\n        RuntimeError: Error decoding JSON output.\n        ValueError: Expected output to be a list of instance IDs.\n    Returns:\n        list: A list of instance IDs of the instances.\n    \"\"\"\n    terraform_dir = Path(terraform_dir)\n    try:\n        result = subprocess.run(\n            [\"terraform\", \"output\", \"-json\", \"vm_instance_ids\"],\n            cwd=terraform_dir,\n            capture_output=True,\n            text=True,\n            check=True,\n        )\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Error running terraform output: {e.stderr}\")\n    try:\n        output = json.loads(result.stdout)\n    except json.JSONDecodeError as e:\n        raise RuntimeError(f\"Error decoding JSON output: {e}\")\n    if not isinstance(output, list):\n        raise ValueError(\"Expected output to be a list of instance IDs\")\n    return output\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/utils/terraform_utils/#lablink_allocator_service.utils.terraform_utils.get_instance_ips","title":"<code>get_instance_ips(terraform_dir)</code>","text":"<p>Get the public IP addresses of the instances created by Terraform. Args:     terraform_dir (str): The directory where the Terraform configuration is located. Raises:     RuntimeError: Error running terraform output command.     RuntimeError: Error decoding JSON output.     ValueError: Expected output to be a list of IP addresses. Returns:     list: A list of public IP addresses of the instances.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/utils/terraform_utils.py</code> <pre><code>def get_instance_ips(terraform_dir: str) -&gt; list:\n    \"\"\"Get the public IP addresses of the instances created by Terraform.\n    Args:\n        terraform_dir (str): The directory where the Terraform configuration is located.\n    Raises:\n        RuntimeError: Error running terraform output command.\n        RuntimeError: Error decoding JSON output.\n        ValueError: Expected output to be a list of IP addresses.\n    Returns:\n        list: A list of public IP addresses of the instances.\n    \"\"\"\n    terraform_dir = Path(terraform_dir)\n    try:\n        result = subprocess.run(\n            [\"terraform\", \"output\", \"-json\", \"vm_public_ips\"],\n            cwd=terraform_dir,\n            capture_output=True,\n            text=True,\n            check=True,\n        )\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Error running terraform output: {e.stderr}\")\n    try:\n        output = json.loads(result.stdout)\n    except json.JSONDecodeError as e:\n        raise RuntimeError(f\"Error decoding JSON output: {e}\")\n    if not isinstance(output, list):\n        raise ValueError(\"Expected output to be a list of IP addresses\")\n    return output\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/utils/terraform_utils/#lablink_allocator_service.utils.terraform_utils.get_instance_names","title":"<code>get_instance_names(terraform_dir)</code>","text":"<p>Get the names of the instances created by Terraform. Args:     terraform_dir (str): The directory where the Terraform configuration is located. Raises:     RuntimeError: Error running terraform output command.     RuntimeError: Error decoding JSON output.     ValueError: Expected output to be a list of instance names. Returns:     list: A list of names assigned to the EC2 instances.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/utils/terraform_utils.py</code> <pre><code>def get_instance_names(terraform_dir: str) -&gt; list:\n    \"\"\"Get the names of the instances created by Terraform.\n    Args:\n        terraform_dir (str): The directory where the Terraform configuration is located.\n    Raises:\n        RuntimeError: Error running terraform output command.\n        RuntimeError: Error decoding JSON output.\n        ValueError: Expected output to be a list of instance names.\n    Returns:\n        list: A list of names assigned to the EC2 instances.\n    \"\"\"\n    terraform_dir = Path(terraform_dir)\n    try:\n        result = subprocess.run(\n            [\"terraform\", \"output\", \"-json\", \"vm_instance_names\"],\n            cwd=terraform_dir,\n            capture_output=True,\n            text=True,\n            check=True,\n        )\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Error running terraform output: {e.stderr}\")\n    try:\n        output = json.loads(result.stdout)\n    except json.JSONDecodeError as e:\n        raise RuntimeError(f\"Error decoding JSON output: {e}\")\n    if not isinstance(output, list):\n        raise ValueError(\"Expected output to be a list of instance names\")\n    return output\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/utils/terraform_utils/#lablink_allocator_service.utils.terraform_utils.get_instance_timings","title":"<code>get_instance_timings(terraform_dir)</code>","text":"<p>Get the launch times of the instances created by Terraform. Args:     terraform_dir (str): The directory where the Terraform configuration is located. Raises:     RuntimeError: Error running terraform output command.     RuntimeError: Error decoding JSON output.     ValueError: Expected output to be a dictionary of launch times. Returns:     dict: A dictionary mapping instance names to their launch times.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/utils/terraform_utils.py</code> <pre><code>def get_instance_timings(terraform_dir: str) -&gt; dict:\n    \"\"\"Get the launch times of the instances created by Terraform.\n    Args:\n        terraform_dir (str): The directory where the Terraform configuration is located.\n    Raises:\n        RuntimeError: Error running terraform output command.\n        RuntimeError: Error decoding JSON output.\n        ValueError: Expected output to be a dictionary of launch times.\n    Returns:\n        dict: A dictionary mapping instance names to their launch times.\n    \"\"\"\n    terraform_dir = Path(terraform_dir)\n    try:\n        result = subprocess.run(\n            [\"terraform\", \"output\", \"-json\", \"instance_terraform_apply_times\"],\n            cwd=terraform_dir,\n            capture_output=True,\n            text=True,\n            check=True,\n        )\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Error running terraform output: {e.stderr}\")\n    try:\n        timing_data = json.loads(result.stdout)\n    except json.JSONDecodeError as e:\n        raise RuntimeError(f\"Error decoding JSON output: {e}\")\n    if not isinstance(timing_data, dict):\n        raise ValueError(\"Expected output to be a dictionary of launch times.\")\n\n    return timing_data\n</code></pre>"},{"location":"reference/allocator/lablink_allocator_service/utils/terraform_utils/#lablink_allocator_service.utils.terraform_utils.get_ssh_private_key","title":"<code>get_ssh_private_key(terraform_dir)</code>","text":"<p>Get the SSH private key used for connecting to the instances. Args:     terraform_dir (str): The directory where the Terraform configuration is located. Raises:     RuntimeError: Error running terraform output command. Returns:     str: The path to the SSH private key file.</p> Source code in <code>packages/allocator/src/lablink_allocator_service/utils/terraform_utils.py</code> <pre><code>def get_ssh_private_key(terraform_dir: str) -&gt; str:\n    \"\"\"Get the SSH private key used for connecting to the instances.\n    Args:\n        terraform_dir (str): The directory where the Terraform configuration is located.\n    Raises:\n        RuntimeError: Error running terraform output command.\n    Returns:\n        str: The path to the SSH private key file.\n    \"\"\"\n    terraform_dir = Path(terraform_dir)\n    try:\n        result = subprocess.run(\n            [\"terraform\", \"output\", \"-raw\", \"lablink_private_key_pem\"],\n            cwd=terraform_dir,\n            capture_output=True,\n            text=True,\n            check=True,\n        )\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Error running terraform output: {e.stderr}\")\n    key_path = \"/tmp/lablink_key.pem\"\n    with open(key_path, \"w\") as f:\n        f.write(result.stdout)\n    os.chmod(key_path, 0o400)\n    return key_path\n</code></pre>"},{"location":"reference/client/SUMMARY/","title":"SUMMARY","text":"<ul> <li>lablink_client_service<ul> <li>check_gpu</li> <li>conf<ul> <li>structured_config</li> </ul> </li> <li>connect_crd</li> <li>logger_config</li> <li>logger_utils</li> <li>subscribe</li> <li>update_inuse_status</li> </ul> </li> </ul>"},{"location":"reference/client/lablink_client_service/","title":"lablink_client_service","text":""},{"location":"reference/client/lablink_client_service/#lablink_client_service","title":"<code>lablink_client_service</code>","text":""},{"location":"reference/client/lablink_client_service/check_gpu/","title":"lablink_client_service.check_gpu","text":""},{"location":"reference/client/lablink_client_service/check_gpu/#lablink_client_service.check_gpu","title":"<code>lablink_client_service.check_gpu</code>","text":""},{"location":"reference/client/lablink_client_service/check_gpu/#lablink_client_service.check_gpu-attributes","title":"Attributes","text":""},{"location":"reference/client/lablink_client_service/check_gpu/#lablink_client_service.check_gpu.MAX_REPORT_RETRIES","title":"<code>MAX_REPORT_RETRIES = 5</code>  <code>module-attribute</code>","text":""},{"location":"reference/client/lablink_client_service/check_gpu/#lablink_client_service.check_gpu.REPORT_RETRY_DELAY","title":"<code>REPORT_RETRY_DELAY = 10</code>  <code>module-attribute</code>","text":""},{"location":"reference/client/lablink_client_service/check_gpu/#lablink_client_service.check_gpu.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/client/lablink_client_service/check_gpu/#lablink_client_service.check_gpu-classes","title":"Classes","text":""},{"location":"reference/client/lablink_client_service/check_gpu/#lablink_client_service.check_gpu-functions","title":"Functions","text":""},{"location":"reference/client/lablink_client_service/check_gpu/#lablink_client_service.check_gpu.check_gpu_health","title":"<code>check_gpu_health(allocator_url, interval=20)</code>","text":"<p>Check the health of the GPU.</p> <p>Parameters:</p> Name Type Description Default <code>allocator_url</code> <code>str</code> <p>The base URL of the allocator service.</p> required <code>interval</code> <code>int</code> <p>The interval in seconds to check the GPU health.</p> <code>20</code> Source code in <code>packages/client/src/lablink_client_service/check_gpu.py</code> <pre><code>def check_gpu_health(allocator_url: str, interval: int = 20):\n    \"\"\"Check the health of the GPU.\n\n    Args:\n        allocator_url (str): The base URL of the allocator service.\n        interval (int, optional): The interval in seconds to check the GPU health.\n    \"\"\"\n    logger.debug(\"Starting GPU health check...\")\n    last_status = None\n    base_url = allocator_url.rstrip(\"/\")\n    base_url = base_url.replace(\"://.\", \"://\")\n\n    while True:\n        curr_status = None\n        break_now = False\n\n        try:\n            # Run the nvidia-smi command to check GPU health\n            result = subprocess.run(\n                [\"nvidia-smi\"],\n                capture_output=True,\n                text=True,\n                check=True,\n            )\n            curr_status = \"Healthy\"\n            logger.info(f\"GPU Health Check: {result.stdout.strip()}\")\n\n        except subprocess.CalledProcessError as e:\n            logger.error(f\"Failed to check GPU health: {e}\")\n            # Command not found -&gt; likely nvidia-smi is not installed\n            if e.returncode == 127:\n                logger.error(\n                    \"nvidia-smi command not found. Ensure NVIDIA drivers are installed.\"\n                )\n                curr_status = \"N/A\"\n                break_now = True\n\n            else:\n                logger.error(\n                    f\"nvidia-smi command failed with error: {e.stderr.strip()}\"\n                )\n                curr_status = \"Unhealthy\"\n\n        except FileNotFoundError as e:\n            # This exception is raised if the nvidia-smi command is not found\n            logger.error(f\"File not found: {e}\")\n            curr_status = \"N/A\"\n            break_now = True\n\n        except Exception as e:\n            curr_status = \"Unhealthy\"\n            logger.error(f\"An unexpected error occurred: {e}\")\n\n        if curr_status != last_status or break_now:\n            logger.info(\n                f\"GPU health status changed: {curr_status}. Reporting to allocator.\"\n            )\n            report_retry_count = 0\n\n            while report_retry_count &lt; MAX_REPORT_RETRIES:\n                try:\n                    response = requests.post(\n                        f\"{base_url}/api/gpu_health\",\n                        json={\n                            \"hostname\": os.getenv(\"VM_NAME\"),\n                            \"gpu_status\": curr_status,\n                        },\n                        # (connect_timeout, read_timeout): 10s to connect, 20s to read\n                        timeout=(10, 20),\n                    )\n                    response.raise_for_status()\n                    logger.info(\n                        f\"Successfully reported GPU health status: {curr_status}\"\n                    )\n                    last_status = curr_status\n                    break  # Break out of the report retry loop on success\n                except requests.exceptions.Timeout:\n                    logger.error(\n                        f\"GPU health report timed out \"\n                        f\"(Attempt {report_retry_count + 1}/{MAX_REPORT_RETRIES}). \"\n                        f\"Retrying...\"\n                    )\n                except requests.exceptions.RequestException as e:\n                    logger.error(\n                        f\"Failed to report GPU health: {e} \"\n                        f\"(Attempt {report_retry_count + 1}/{MAX_REPORT_RETRIES}). \"\n                        f\"Retrying...\"\n                    )\n                except Exception as e:\n                    logger.error(\n                        f\"An unexpected error occurred while reporting GPU health: {e} \"\n                        f\"(Attempt {report_retry_count + 1}/{MAX_REPORT_RETRIES}). \"\n                        f\"Retrying...\"\n                    )\n\n                report_retry_count += 1\n                if report_retry_count &lt; MAX_REPORT_RETRIES:\n                    jitter = random.uniform(0, 5)\n                    time.sleep(REPORT_RETRY_DELAY + jitter)\n            else:\n                logger.error(\n                    f\"Failed to report GPU health status after {MAX_REPORT_RETRIES} \"\n                    f\"attempts. Allocator might be unreachable or experiencing issues.\"\n                )\n\n                # Fallback to avoid repeated failed reports\n                last_status = curr_status\n\n        if break_now:\n            break\n        time.sleep(interval)\n</code></pre>"},{"location":"reference/client/lablink_client_service/check_gpu/#lablink_client_service.check_gpu.main","title":"<code>main(cfg)</code>","text":"Source code in <code>packages/client/src/lablink_client_service/check_gpu.py</code> <pre><code>@hydra.main(version_base=None, config_name=\"config\")\ndef main(cfg: Config) -&gt; None:\n    global logger\n    logger = CloudAndConsoleLogger(\n        module_name=\"check_gpu\",\n    )\n    # Check GPU health\n    # Use ALLOCATOR_URL env var if set (supports HTTPS),\n    # otherwise use host:port with HTTP\n    allocator_url_env = os.getenv(\"ALLOCATOR_URL\")\n    if allocator_url_env:\n        allocator_url = allocator_url_env\n    else:\n        allocator_url = f\"http://{cfg.allocator.host}:{cfg.allocator.port}\"\n\n    allocator_url = allocator_url.replace(\"://.\", \"://\")\n\n    check_gpu_health(allocator_url=allocator_url)\n</code></pre>"},{"location":"reference/client/lablink_client_service/connect_crd/","title":"lablink_client_service.connect_crd","text":""},{"location":"reference/client/lablink_client_service/connect_crd/#lablink_client_service.connect_crd","title":"<code>lablink_client_service.connect_crd</code>","text":""},{"location":"reference/client/lablink_client_service/connect_crd/#lablink_client_service.connect_crd-attributes","title":"Attributes","text":""},{"location":"reference/client/lablink_client_service/connect_crd/#lablink_client_service.connect_crd.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/client/lablink_client_service/connect_crd/#lablink_client_service.connect_crd-functions","title":"Functions","text":""},{"location":"reference/client/lablink_client_service/connect_crd/#lablink_client_service.connect_crd.cleanup_logs","title":"<code>cleanup_logs()</code>","text":"Source code in <code>packages/client/src/lablink_client_service/connect_crd.py</code> <pre><code>def cleanup_logs():\n    try:\n        for handler in logger.handlers:\n            if hasattr(handler, \"flush\"):\n                handler.flush()\n\n        time.sleep(1.5)\n\n        logging.shutdown()\n    except Exception as e:\n        logger.error(f\"Error during log cleanup: {e}\")\n</code></pre>"},{"location":"reference/client/lablink_client_service/connect_crd/#lablink_client_service.connect_crd.connect_to_crd","title":"<code>connect_to_crd(command, pin)</code>","text":"Source code in <code>packages/client/src/lablink_client_service/connect_crd.py</code> <pre><code>def connect_to_crd(command, pin):\n    # Parse the command line arguments\n    command = reconstruct_command(command)\n\n    # input the pin code with verification\n    input_pin = pin + \"\\n\"\n    input_pin_verification = input_pin + input_pin\n\n    # Execute the command\n    result = subprocess.run(\n        command,\n        input=input_pin_verification,\n        shell=True,\n        capture_output=True,\n        text=True,\n    )\n\n    # Check the result\n    logger.debug(f\"Output:\\n {result.stdout}\")\n\n    if result.stderr:\n        logger.error(f\"Error: {result.stderr}\")\n</code></pre>"},{"location":"reference/client/lablink_client_service/connect_crd/#lablink_client_service.connect_crd.construct_command","title":"<code>construct_command(args)</code>","text":"<p>Constructs the Linux CRD command to connect to a remote machine.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The command line arguments.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The Linux CRD command to connect to a remote machine.</p> Source code in <code>packages/client/src/lablink_client_service/connect_crd.py</code> <pre><code>def construct_command(args) -&gt; str:\n    \"\"\"Constructs the Linux CRD command to connect to a remote machine.\n\n    Args:\n        args (argparse.Namespace): The command line arguments.\n\n    Returns:\n        str: The Linux CRD command to connect to a remote machine.\n    \"\"\"\n\n    redirect_url = \"'https://remotedesktop.google.com/_/oauthredirect'\"\n    name = os.getenv(\"VM_NAME\", \"$(hostname)\")\n\n    if args.code is None:\n        raise ValueError(\"Code must be provided to construct the command.\")\n\n    command = \"DISPLAY= /opt/google/chrome-remote-desktop/start-host\"\n    command += f\" --code={args.code}\"\n    command += f\" --redirect-url={redirect_url}\"\n    command += f\" --name={name}\"\n\n    return command\n</code></pre>"},{"location":"reference/client/lablink_client_service/connect_crd/#lablink_client_service.connect_crd.create_parser","title":"<code>create_parser()</code>","text":"<p>Creates a parser for the command line arguments.</p> <p>Returns:</p> Type Description <code>ArgumentParser</code> <p>argparse.ArgumentParser: The parser for the command line arguments.</p> Source code in <code>packages/client/src/lablink_client_service/connect_crd.py</code> <pre><code>def create_parser() -&gt; argparse.ArgumentParser:\n    \"\"\"Creates a parser for the command line arguments.\n\n    Returns:\n        argparse.ArgumentParser: The parser for the command line arguments.\n    \"\"\"\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--code\",\n        help=\"Unique code to allow connection via CRD with specific Google login.\",\n        type=str,\n        default=None,\n    )\n\n    return parser\n</code></pre>"},{"location":"reference/client/lablink_client_service/connect_crd/#lablink_client_service.connect_crd.reconstruct_command","title":"<code>reconstruct_command(command)</code>","text":"<p>Reconstructs the Chrome Remote Desktop command.</p> <p>Parameters:</p> Name Type Description Default <code>command</code> <code>str</code> <p>CRD command to connect to the machine.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Reconstructed command to connect to the machine.</p> Source code in <code>packages/client/src/lablink_client_service/connect_crd.py</code> <pre><code>def reconstruct_command(command: str) -&gt; str:\n    \"\"\"Reconstructs the Chrome Remote Desktop command.\n\n    Args:\n        command (str): CRD command to connect to the machine.\n\n    Returns:\n        str: Reconstructed command to connect to the machine.\n    \"\"\"\n    arg_to_parse = command.split()\n\n    # Parse the command line arguments\n    parser = create_parser()\n    args, _ = parser.parse_known_args(args=arg_to_parse)\n\n    logger.debug(vars(args))\n\n    # Construct the command to be executed\n    command = construct_command(args)\n    logger.debug(f\"Command to be executed: {command}\")\n\n    return command\n</code></pre>"},{"location":"reference/client/lablink_client_service/connect_crd/#lablink_client_service.connect_crd.set_logger","title":"<code>set_logger(external_logger)</code>","text":"Source code in <code>packages/client/src/lablink_client_service/connect_crd.py</code> <pre><code>def set_logger(external_logger):\n    global logger\n    logger = external_logger\n</code></pre>"},{"location":"reference/client/lablink_client_service/logger_config/","title":"lablink_client_service.logger_config","text":""},{"location":"reference/client/lablink_client_service/logger_config/#lablink_client_service.logger_config","title":"<code>lablink_client_service.logger_config</code>","text":""},{"location":"reference/client/lablink_client_service/logger_config/#lablink_client_service.logger_config-functions","title":"Functions","text":""},{"location":"reference/client/lablink_client_service/logger_config/#lablink_client_service.logger_config.setup_logger","title":"<code>setup_logger(name='lablink_client_logger', level=logging.DEBUG)</code>","text":"Source code in <code>packages/client/src/lablink_client_service/logger_config.py</code> <pre><code>def setup_logger(\n    name: str = \"lablink_client_logger\", level=logging.DEBUG\n) -&gt; logging.Logger:\n    logger = logging.getLogger(name)\n\n    # Ensure the logger's level is set\n    logger.setLevel(level)\n\n    # Prevent adding duplicate handlers\n    if not any(\n        isinstance(handler, logging.StreamHandler) and handler.stream == sys.stdout\n        for handler in logger.handlers\n    ):\n        handler = logging.StreamHandler(sys.stdout)\n        formatter = logging.Formatter(\n            \"%(asctime)s %(name)s [%(levelname)s]: %(message)s\", \"%H:%M\"\n        )\n        handler.setFormatter(formatter)\n        logger.addHandler(handler)\n\n    return logger\n</code></pre>"},{"location":"reference/client/lablink_client_service/logger_utils/","title":"lablink_client_service.logger_utils","text":""},{"location":"reference/client/lablink_client_service/logger_utils/#lablink_client_service.logger_utils","title":"<code>lablink_client_service.logger_utils</code>","text":""},{"location":"reference/client/lablink_client_service/logger_utils/#lablink_client_service.logger_utils-classes","title":"Classes","text":""},{"location":"reference/client/lablink_client_service/logger_utils/#lablink_client_service.logger_utils.CloudAndConsoleLogger","title":"<code>CloudAndConsoleLogger(module_name, level=logging.DEBUG, format=None, log_group=None, region=None)</code>","text":"Source code in <code>packages/client/src/lablink_client_service/logger_utils.py</code> <pre><code>def __init__(\n    self,\n    module_name: str,\n    level=logging.DEBUG,\n    format=None,\n    log_group=None,\n    region=None,\n):\n    self.name = module_name\n\n    final_format = format or \"%(name)s[%(levelname)s]: %(message)s\"\n    formatter = logging.Formatter(final_format)\n\n    # Get group name from env vars or use defaults\n    self.log_group = log_group or os.environ.get(\n        \"CLOUD_INIT_LOG_GROUP\", \"lablink-client-service-logs\"\n    )\n\n    self.log_stream = os.getenv(\"VM_NAME\", \"lablink-client-service-stream\")\n    self.region = region or os.environ.get(\"AWS_REGION\", \"us-west-2\")\n\n    # Set up both console and cloud logging\n    self.console_logger = self.setup_console_logger(\n        level=level, formatter=formatter\n    )\n    self.cloud_logger = self.setup_cloud_logging(level=level, formatter=formatter)\n</code></pre>"},{"location":"reference/client/lablink_client_service/logger_utils/#lablink_client_service.logger_utils.CloudAndConsoleLogger-attributes","title":"Attributes","text":""},{"location":"reference/client/lablink_client_service/logger_utils/#lablink_client_service.logger_utils.CloudAndConsoleLogger.cloud_logger","title":"<code>cloud_logger = self.setup_cloud_logging(level=level, formatter=formatter)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/client/lablink_client_service/logger_utils/#lablink_client_service.logger_utils.CloudAndConsoleLogger.console_logger","title":"<code>console_logger = self.setup_console_logger(level=level, formatter=formatter)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/client/lablink_client_service/logger_utils/#lablink_client_service.logger_utils.CloudAndConsoleLogger.log_group","title":"<code>log_group = log_group or os.environ.get('CLOUD_INIT_LOG_GROUP', 'lablink-client-service-logs')</code>  <code>instance-attribute</code>","text":""},{"location":"reference/client/lablink_client_service/logger_utils/#lablink_client_service.logger_utils.CloudAndConsoleLogger.log_stream","title":"<code>log_stream = os.getenv('VM_NAME', 'lablink-client-service-stream')</code>  <code>instance-attribute</code>","text":""},{"location":"reference/client/lablink_client_service/logger_utils/#lablink_client_service.logger_utils.CloudAndConsoleLogger.name","title":"<code>name = module_name</code>  <code>instance-attribute</code>","text":""},{"location":"reference/client/lablink_client_service/logger_utils/#lablink_client_service.logger_utils.CloudAndConsoleLogger.region","title":"<code>region = region or os.environ.get('AWS_REGION', 'us-west-2')</code>  <code>instance-attribute</code>","text":""},{"location":"reference/client/lablink_client_service/logger_utils/#lablink_client_service.logger_utils.CloudAndConsoleLogger-functions","title":"Functions","text":""},{"location":"reference/client/lablink_client_service/logger_utils/#lablink_client_service.logger_utils.CloudAndConsoleLogger.pprint","title":"<code>pprint(obj, level=logging.INFO)</code>","text":"<p>Pretty-print an object and log the output.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>object</code> <p>The object to pretty-print and log.</p> required <code>level</code> <code>int</code> <p>The logging level. Defaults to logging.INFO.</p> <code>INFO</code> Source code in <code>packages/client/src/lablink_client_service/logger_utils.py</code> <pre><code>def pprint(self, obj: object, level: int = logging.INFO) -&gt; None:\n    \"\"\"Pretty-print an object and log the output.\n\n    Args:\n        obj: The object to pretty-print and log.\n        level: The logging level. Defaults to logging.INFO.\n    \"\"\"\n\n    pp = pprint.PrettyPrinter(indent=4)\n    pretty_str = pp.pformat(obj)\n    self.console_logger.log(level, pretty_str)\n    self.cloud_logger.log(level, pretty_str)\n</code></pre>"},{"location":"reference/client/lablink_client_service/logger_utils/#lablink_client_service.logger_utils.CloudAndConsoleLogger.setup_cloud_logging","title":"<code>setup_cloud_logging(level=logging.DEBUG, formatter=None)</code>","text":"<p>Set up logging to AWS CloudWatch Logs.</p> Source code in <code>packages/client/src/lablink_client_service/logger_utils.py</code> <pre><code>def setup_cloud_logging(\n    self, level=logging.DEBUG, formatter: logging.Formatter = None\n):\n    \"\"\"Set up logging to AWS CloudWatch Logs.\"\"\"\n    try:\n        session = boto3.client(service_name=\"logs\", region_name=self.region)\n        handler = watchtower.CloudWatchLogHandler(\n            log_group_name=self.log_group,\n            log_stream_name=self.log_stream,\n            boto3_client=session,\n            create_log_group=True,\n            create_log_stream=True,\n        )\n        handler.setFormatter(formatter)\n\n        logger = logging.getLogger(f\"{self.name}_cloud_logger\")\n        logger.setLevel(level)\n        logger.propagate = False\n\n        if not logger.handlers:\n            logger.addHandler(handler)\n\n        logger.debug(\"CloudWatch logging is set up.\")\n\n        return logger\n    except Exception as e:\n        console_logger = logging.getLogger(self.name)\n        console_logger.error(\n            f\"Failed to set up CloudWatch logging: {e}. \"\n            \"Falling back to console logging only.\"\n        )\n        return None\n</code></pre>"},{"location":"reference/client/lablink_client_service/logger_utils/#lablink_client_service.logger_utils.CloudAndConsoleLogger.setup_console_logger","title":"<code>setup_console_logger(level=logging.DEBUG, formatter=None)</code>","text":"<p>Set up a console logger.</p> Source code in <code>packages/client/src/lablink_client_service/logger_utils.py</code> <pre><code>def setup_console_logger(\n    self, level=logging.DEBUG, formatter: logging.Formatter = None\n):\n    \"\"\"Set up a console logger.\"\"\"\n    # Create a console logger\n    logger = logging.getLogger(self.name)\n    logger.setLevel(level)\n    logger.propagate = False\n\n    # Create a console handler\n    if not logger.handlers:\n        console_handler = logging.StreamHandler(sys.stdout)\n        console_handler.setLevel(level)\n        console_handler.setFormatter(formatter)\n        logger.addHandler(console_handler)\n\n    return logger\n</code></pre>"},{"location":"reference/client/lablink_client_service/subscribe/","title":"lablink_client_service.subscribe","text":""},{"location":"reference/client/lablink_client_service/subscribe/#lablink_client_service.subscribe","title":"<code>lablink_client_service.subscribe</code>","text":""},{"location":"reference/client/lablink_client_service/subscribe/#lablink_client_service.subscribe-attributes","title":"Attributes","text":""},{"location":"reference/client/lablink_client_service/subscribe/#lablink_client_service.subscribe.MAX_RETRIES","title":"<code>MAX_RETRIES = None</code>  <code>module-attribute</code>","text":""},{"location":"reference/client/lablink_client_service/subscribe/#lablink_client_service.subscribe.RETRY_DELAY","title":"<code>RETRY_DELAY = 10</code>  <code>module-attribute</code>","text":""},{"location":"reference/client/lablink_client_service/subscribe/#lablink_client_service.subscribe.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/client/lablink_client_service/subscribe/#lablink_client_service.subscribe-classes","title":"Classes","text":""},{"location":"reference/client/lablink_client_service/subscribe/#lablink_client_service.subscribe-functions","title":"Functions","text":""},{"location":"reference/client/lablink_client_service/subscribe/#lablink_client_service.subscribe.main","title":"<code>main(cfg)</code>","text":"Source code in <code>packages/client/src/lablink_client_service/subscribe.py</code> <pre><code>@hydra.main(version_base=None, config_name=\"config\")\ndef main(cfg: Config) -&gt; None:\n    subscribe(cfg)\n</code></pre>"},{"location":"reference/client/lablink_client_service/subscribe/#lablink_client_service.subscribe.subscribe","title":"<code>subscribe(cfg)</code>","text":"Source code in <code>packages/client/src/lablink_client_service/subscribe.py</code> <pre><code>def subscribe(cfg: Config) -&gt; None:\n    global logger\n    logger = CloudAndConsoleLogger(module_name=\"subscribe\")\n    set_logger(logger)  # Set the logger for connect_crd\n\n    logger.debug(\"Starting the lablink client service...\")\n    logger.debug(f\"Configuration: {OmegaConf.to_yaml(cfg)}\")\n\n    # Define the URL for the POST request\n    # Use ALLOCATOR_URL env var if set (supports HTTPS),\n    # otherwise use host:port with HTTP\n    allocator_url = os.getenv(\"ALLOCATOR_URL\")\n    logger.debug(f\"Allocator URL from env: {allocator_url}\")\n    if allocator_url:\n        base_url = allocator_url.rstrip(\"/\")\n    else:\n        base_url = f\"http://{cfg.allocator.host}:{cfg.allocator.port}\"\n\n    # Sanitize URL to remove prepended dots\n    # Handles cases like:\n    # - http://.lablink.sleap.ai -&gt; http://lablink.sleap.ai\n    # - https://.lablink.sleap.ai -&gt; https://lablink.sleap.ai\n    # - .lablink.sleap.ai -&gt; lablink.sleap.ai\n    base_url = base_url.replace(\"://.\", \"://\")\n    if base_url.startswith(\".\"):\n        base_url = base_url[1:]\n\n    url = f\"{base_url}/vm_startup\"\n    logger.debug(f\"URL: {url}\")\n\n    # Define hostname for the client\n    hostname = os.getenv(\"VM_NAME\")\n    logger.debug(f\"Hostname: {hostname}\")\n\n    # Retry loop: Keep trying to connect until successful or VM is terminated\n    # This ensures the VM can connect to CRD even if there are transient network issues\n    retry_count = 0\n\n    while True:\n        if retry_count &gt; 0:\n            logger.info(\n                f\"Retrying connection to allocator in {RETRY_DELAY} seconds... \"\n                f\"(Attempt {retry_count + 1})\"\n            )\n            jitter = random.uniform(0, 5)\n            time.sleep(RETRY_DELAY + jitter)\n\n        try:\n            # Send a POST request to the specified URL\n            # Note: This endpoint blocks until a user assigns a CRD command,\n            # so we use a very long timeout.\n            # The allocator uses PostgreSQL LISTEN/NOTIFY to wait for VM\n            # assignment. Timeout tuple: (connect, read) = (30s, 7 days)\n            logger.debug(\n                f\"Attempting to connect to allocator \" f\"(attempt {retry_count + 1})\"\n            )\n            response = requests.post(\n                url, json={\"hostname\": hostname}, timeout=(30, 604800)\n            )\n\n            # Check if the request was successful\n            if response.status_code == 200:\n                logger.debug(\"POST request was successful.\")\n                data = response.json()\n                if data.get(\"status\") == \"success\":\n                    logger.info(\n                        \"Successfully connected to allocator and received command.\"\n                    )\n                    command = data[\"command\"]\n                    pin = data[\"pin\"]\n                    logger.debug(f\"Command received: {command}\")\n                    logger.debug(f\"Pin received: {pin}\")\n\n                    # Execute the command\n                    connect_to_crd(pin=pin, command=command)\n                    logger.info(\"Command executed successfully. Exiting retry loop.\")\n                    break  # Success - exit retry loop\n                else:\n                    logger.error(\"Received error response from server.\")\n                    logger.error(f\"Error message: {data.get('message')}\")\n                    logger.info(\"Server explicitly rejected the request. Not retrying.\")\n                    break  # Server explicitly rejected - don't retry\n            else:\n                logger.error(\n                    f\"POST request failed with status code: \"\n                    f\"{response.status_code}. Retrying...\"\n                )\n                # Will retry after delay\n\n        except requests.exceptions.Timeout as e:\n            logger.error(f\"Request to {url} timed out: {e}. Retrying...\")\n            # Will retry after delay\n\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Request to {url} failed: {e}. Retrying...\")\n            # Will retry after delay\n\n        # Increment retry count\n        retry_count += 1\n</code></pre>"},{"location":"reference/client/lablink_client_service/update_inuse_status/","title":"lablink_client_service.update_inuse_status","text":""},{"location":"reference/client/lablink_client_service/update_inuse_status/#lablink_client_service.update_inuse_status","title":"<code>lablink_client_service.update_inuse_status</code>","text":""},{"location":"reference/client/lablink_client_service/update_inuse_status/#lablink_client_service.update_inuse_status-attributes","title":"Attributes","text":""},{"location":"reference/client/lablink_client_service/update_inuse_status/#lablink_client_service.update_inuse_status.API_RETRY_DELAY","title":"<code>API_RETRY_DELAY = 10</code>  <code>module-attribute</code>","text":""},{"location":"reference/client/lablink_client_service/update_inuse_status/#lablink_client_service.update_inuse_status.MAX_API_RETRIES","title":"<code>MAX_API_RETRIES = 5</code>  <code>module-attribute</code>","text":""},{"location":"reference/client/lablink_client_service/update_inuse_status/#lablink_client_service.update_inuse_status.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/client/lablink_client_service/update_inuse_status/#lablink_client_service.update_inuse_status-classes","title":"Classes","text":""},{"location":"reference/client/lablink_client_service/update_inuse_status/#lablink_client_service.update_inuse_status-functions","title":"Functions","text":""},{"location":"reference/client/lablink_client_service/update_inuse_status/#lablink_client_service.update_inuse_status.api_callback","title":"<code>api_callback(process_name, url)</code>","text":"<p>Callback to call the API when process state changes.</p> Source code in <code>packages/client/src/lablink_client_service/update_inuse_status.py</code> <pre><code>def api_callback(process_name: str, url: str):\n    \"\"\"Callback to call the API when process state changes.\"\"\"\n    call_api(process_name, url)\n</code></pre>"},{"location":"reference/client/lablink_client_service/update_inuse_status/#lablink_client_service.update_inuse_status.call_api","title":"<code>call_api(process_name, url)</code>","text":"Source code in <code>packages/client/src/lablink_client_service/update_inuse_status.py</code> <pre><code>def call_api(process_name, url):\n    logger.debug(f\"Calling API for process: {process_name}\")\n    hostname = os.getenv(\"VM_NAME\")\n    status = is_process_running(process_name=process_name)\n\n    retry_count = 0\n\n    while retry_count &lt; MAX_API_RETRIES:\n        try:\n            response = requests.post(\n                url,\n                json={\"hostname\": hostname, \"status\": status},\n                # (connect_timeout, read_timeout): 10s to connect, 20s to read\n                timeout=(10, 20),\n            )\n            response.raise_for_status()\n            logger.debug(f\"API call successful: {response.json()}\")\n            logger.info(\n                f\"Successfully updated in-use status for {process_name} to {status}\"\n            )\n            break  # Success, exit retry loop\n        except requests.exceptions.Timeout:\n            logger.error(\n                f\"Status update timed out after 30 seconds \"\n                f\"(Attempt {retry_count + 1}/{MAX_API_RETRIES}). Retrying...\"\n            )\n        except requests.RequestException as e:\n            logger.error(\n                f\"API call failed: {e} \"\n                f\"(Attempt {retry_count + 1}/{MAX_API_RETRIES}). Retrying...\"\n            )\n\n        retry_count += 1\n        if retry_count &lt; MAX_API_RETRIES:\n            jitter = random.uniform(0, 5)\n            time.sleep(API_RETRY_DELAY + jitter)\n    else:\n        logger.error(\n            f\"Failed to update in-use status after {MAX_API_RETRIES} attempts. \"\n            \"Allocator might be unreachable.\"\n        )\n</code></pre>"},{"location":"reference/client/lablink_client_service/update_inuse_status/#lablink_client_service.update_inuse_status.default_callback","title":"<code>default_callback(process_name)</code>","text":"<p>Default callback to log when a process is detected.</p> Source code in <code>packages/client/src/lablink_client_service/update_inuse_status.py</code> <pre><code>def default_callback(process_name: str):\n    \"\"\"Default callback to log when a process is detected.\"\"\"\n    logger.info(f\"Process '{process_name}' started.\")\n</code></pre>"},{"location":"reference/client/lablink_client_service/update_inuse_status/#lablink_client_service.update_inuse_status.is_process_running","title":"<code>is_process_running(process_name)</code>","text":"<p>Check if a specific process is running.</p> Source code in <code>packages/client/src/lablink_client_service/update_inuse_status.py</code> <pre><code>def is_process_running(process_name: str) -&gt; bool:\n    \"\"\"\n    Check if a specific process is running.\n    \"\"\"\n    for proc in psutil.process_iter():\n        try:\n            if any(process_name in part for part in proc.cmdline()):\n                if \"update_inuse_status\" in \" \".join(proc.cmdline()):\n                    logger.debug(\n                        f\"Skipping process '{process_name}' as it is the current script\"\n                    )\n                    continue\n                logger.debug(f\"Found process: {proc.cmdline()}\")\n                logger.debug(f\"Process '{process_name}' is running.\")\n                return True\n        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):\n            pass\n    logger.debug(f\"Process '{process_name}' is not running.\")\n    return False\n</code></pre>"},{"location":"reference/client/lablink_client_service/update_inuse_status/#lablink_client_service.update_inuse_status.listen_for_process","title":"<code>listen_for_process(process_name, interval=20, callback_func=None)</code>","text":"<p>Listen for a specific process to start or stop.</p> <p>Parameters:</p> Name Type Description Default <code>process_name</code> <code>str</code> <p>The name of the process to listen for.</p> required <code>interval</code> <code>int</code> <p>The interval (in seconds) to check the process status.</p> <code>20</code> <code>callback_func</code> <code>callable</code> <p>A callback function to execute when the process state changes.</p> <code>None</code> Source code in <code>packages/client/src/lablink_client_service/update_inuse_status.py</code> <pre><code>def listen_for_process(\n    process_name: str, interval: int = 20, callback_func=None\n) -&gt; None:\n    \"\"\"Listen for a specific process to start or stop.\n\n    Args:\n        process_name (str): The name of the process to listen for.\n        interval (int, optional): The interval (in seconds) to check the process status.\n        callback_func (callable, optional): A callback function to execute when the\n            process state changes.\n    \"\"\"\n\n    # Set up a default callback function if none is provided\n    if callback_func is None:\n        callback_func = lambda: default_callback(process_name)\n\n    logger.debug(f\"Listening for process '{process_name}' every {interval} seconds.\")\n\n    # Continuously check if the process is running\n    process_running_prev = is_process_running(process_name)\n    while True:\n        # Check if the process is running now\n        process_running_curr = is_process_running(process_name)\n\n        # Compare the current state with the previous state\n        if process_running_prev != process_running_curr:\n            if callback_func:\n                logger.info(f\"Process '{process_name}' state changed.\")\n                callback_func()\n\n        # Update the previous state\n        process_running_prev = process_running_curr\n\n        # Wait for the specified interval before checking again\n        jitter = random.uniform(0, 5)\n        time.sleep(interval + jitter)\n</code></pre>"},{"location":"reference/client/lablink_client_service/update_inuse_status/#lablink_client_service.update_inuse_status.main","title":"<code>main(cfg)</code>","text":"Source code in <code>packages/client/src/lablink_client_service/update_inuse_status.py</code> <pre><code>@hydra.main(version_base=None, config_path=\"conf\", config_name=\"config\")\ndef main(cfg: Config) -&gt; None:\n    # Configure the logger\n    global logger\n    logger = CloudAndConsoleLogger(module_name=\"update_inuse_status\")\n    logger.debug(\"Starting the update_inuse_status service...\")\n\n    # Define the URL for the POST request\n    # Use ALLOCATOR_URL env var if set (supports HTTPS),\n    # otherwise use host:port with HTTP\n    allocator_url = os.getenv(\"ALLOCATOR_URL\")\n    if allocator_url:\n        base_url = allocator_url.rstrip(\"/\")\n    else:\n        base_url = f\"http://{cfg.allocator.host}:{cfg.allocator.port}\"\n    url = f\"{base_url}/api/update_inuse_status\"\n\n    url = url.replace(\"://.\", \"://\")\n\n    # Start listening for the process\n    listen_for_process(\n        process_name=cfg.client.software,\n        interval=20,\n        callback_func=lambda: api_callback(cfg.client.software, url),\n    )\n</code></pre>"},{"location":"reference/client/lablink_client_service/conf/","title":"lablink_client_service.conf","text":""},{"location":"reference/client/lablink_client_service/conf/#lablink_client_service.conf","title":"<code>lablink_client_service.conf</code>","text":""},{"location":"reference/client/lablink_client_service/conf/structured_config/","title":"lablink_client_service.conf.structured_config","text":""},{"location":"reference/client/lablink_client_service/conf/structured_config/#lablink_client_service.conf.structured_config","title":"<code>lablink_client_service.conf.structured_config</code>","text":""},{"location":"reference/client/lablink_client_service/conf/structured_config/#lablink_client_service.conf.structured_config-attributes","title":"Attributes","text":""},{"location":"reference/client/lablink_client_service/conf/structured_config/#lablink_client_service.conf.structured_config.cs","title":"<code>cs = ConfigStore.instance()</code>  <code>module-attribute</code>","text":""},{"location":"reference/client/lablink_client_service/conf/structured_config/#lablink_client_service.conf.structured_config-classes","title":"Classes","text":""},{"location":"reference/client/lablink_client_service/conf/structured_config/#lablink_client_service.conf.structured_config.AllocatorConfig","title":"<code>AllocatorConfig(host='localhost', port=5000)</code>  <code>dataclass</code>","text":""},{"location":"reference/client/lablink_client_service/conf/structured_config/#lablink_client_service.conf.structured_config.AllocatorConfig-attributes","title":"Attributes","text":""},{"location":"reference/client/lablink_client_service/conf/structured_config/#lablink_client_service.conf.structured_config.AllocatorConfig.host","title":"<code>host = field(default='localhost')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/client/lablink_client_service/conf/structured_config/#lablink_client_service.conf.structured_config.AllocatorConfig.port","title":"<code>port = field(default=5000)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/client/lablink_client_service/conf/structured_config/#lablink_client_service.conf.structured_config.ClientConfig","title":"<code>ClientConfig(software='sleap')</code>  <code>dataclass</code>","text":""},{"location":"reference/client/lablink_client_service/conf/structured_config/#lablink_client_service.conf.structured_config.ClientConfig-attributes","title":"Attributes","text":""},{"location":"reference/client/lablink_client_service/conf/structured_config/#lablink_client_service.conf.structured_config.ClientConfig.software","title":"<code>software = field(default='sleap')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/client/lablink_client_service/conf/structured_config/#lablink_client_service.conf.structured_config.Config","title":"<code>Config(allocator=AllocatorConfig(), client=ClientConfig())</code>  <code>dataclass</code>","text":""},{"location":"reference/client/lablink_client_service/conf/structured_config/#lablink_client_service.conf.structured_config.Config-attributes","title":"Attributes","text":""},{"location":"reference/client/lablink_client_service/conf/structured_config/#lablink_client_service.conf.structured_config.Config.allocator","title":"<code>allocator = field(default_factory=AllocatorConfig)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/client/lablink_client_service/conf/structured_config/#lablink_client_service.conf.structured_config.Config.client","title":"<code>client = field(default_factory=ClientConfig)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""}]}