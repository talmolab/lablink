{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LabLink","text":"<p>Dynamic VM allocation system for computational research workflows. Deploy research software to AWS in minutes.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>For Deployment: Deploy LabLink infrastructure to AWS in 15 minutes</p> <p>Use the LabLink Template Repository to deploy the allocator and client VMs:</p> <pre><code># Clone the template repository\ngit clone https://github.com/talmolab/lablink-template.git\ncd lablink-template/lablink-infrastructure\n\n# Deploy infrastructure (requires AWS credentials)\nterraform init &amp;&amp; terraform apply\n</code></pre> <p>See the Quickstart Guide for detailed deployment instructions.</p> <p>For Development: Develop LabLink packages</p> <p>This repository contains the Python packages and Docker images. See the Contributing Guide to develop packages locally.</p>"},{"location":"#what-is-lablink","title":"What is LabLink?","text":"<p>LabLink automatically provisions and manages cloud-based virtual machines for research software. It handles:</p> <ul> <li>VM Allocation - Request VMs through a web interface</li> <li>Auto-scaling - Create dozens of VMs in parallel</li> <li>Health Monitoring - Track VM status and GPU health</li> <li>Custom Software - Deploy any Docker image or GitHub repo</li> </ul>"},{"location":"#core-components","title":"Core Components","text":"<p>Allocator - Web service managing VM requests and database (Python package + Docker image)</p> <p>Client VMs - EC2 instances running your research software (Python package + Docker image)</p> <p>Infrastructure - Terraform templates for AWS deployment (lablink-template)</p>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Prerequisites - AWS account setup</li> <li>Quickstart - Deploy in 15 minutes</li> <li>Configuration - Customize your deployment</li> <li>Troubleshooting - Common issues and fixes</li> </ul>"},{"location":"#resources","title":"Resources","text":"<ul> <li>GitHub</li> <li>Template</li> <li>Issues</li> </ul>"},{"location":"adapting/","title":"Adapting LabLink for Your Research Software","text":"<p>This guide walks you through customizing LabLink for your own research software, beyond the default SLEAP configuration.</p>"},{"location":"adapting/#overview","title":"Overview","text":"<p>LabLink is designed to be software-agnostic. While it ships with SLEAP as the default research software, you can adapt it for any computational workflow that can run in Docker.</p>"},{"location":"adapting/#adaptation-checklist","title":"Adaptation Checklist","text":"<ul> <li>[ ] Create custom Docker image with your software</li> <li>[ ] Configure Git repository (if needed)</li> <li>[ ] Update LabLink configuration</li> <li>[ ] Test locally</li> <li>[ ] Deploy to AWS</li> </ul>"},{"location":"adapting/#step-by-step-guide","title":"Step-by-Step Guide","text":""},{"location":"adapting/#step-1-create-your-docker-image","title":"Step 1: Create Your Docker Image","text":"<p>Your Docker image should contain:</p> <ol> <li>Base OS (Ubuntu, Debian, etc.)</li> <li>Your research software and dependencies</li> <li>LabLink client service (optional, for health monitoring)</li> </ol>"},{"location":"adapting/#option-a-extend-lablink-client-base","title":"Option A: Extend LabLink Client Base","text":"<p>Build on top of the existing LabLink client image:</p> <p><code>Dockerfile</code>: <pre><code>FROM ghcr.io/talmolab/lablink-client-base-image:latest\n\n# Install your software\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    your-dependencies \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Install your Python package\nCOPY requirements.txt /app/\nRUN pip install -r /app/requirements.txt\n\n# Copy your code\nCOPY your_software/ /app/your_software/\n\n# Set entrypoint\nCMD [\"python\", \"/app/your_software/main.py\"]\n</code></pre></p>"},{"location":"adapting/#option-b-build-from-scratch","title":"Option B: Build from Scratch","text":"<p>Create a completely custom image:</p> <p><code>Dockerfile</code>: <pre><code>FROM ubuntu:20.04\n\n# Install basic dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    python3.9 \\\n    python3-pip \\\n    git \\\n    curl \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Install your research software\nRUN pip3 install your-research-package\n\n# Optional: Include LabLink client for monitoring\nCOPY --from=ghcr.io/talmolab/lablink-client-base-image:latest \\\n    /app/lablink-client-service \\\n    /app/lablink-client-service\n\n# Your startup script\nCOPY entrypoint.sh /entrypoint.sh\nRUN chmod +x /entrypoint.sh\n\nENTRYPOINT [\"/entrypoint.sh\"]\n</code></pre></p>"},{"location":"adapting/#build-and-push","title":"Build and Push","text":"<pre><code># Build your image\ndocker build -t ghcr.io/your-org/your-research-image:latest .\n\n# Test locally\ndocker run -it ghcr.io/your-org/your-research-image:latest\n\n# Push to registry\ndocker login ghcr.io\ndocker push ghcr.io/your-org/your-research-image:latest\n</code></pre> <p>GitHub Container Registry</p> <p>Use GitHub Container Registry (ghcr.io) for free image hosting. See GitHub Packages.</p>"},{"location":"adapting/#step-2-prepare-your-code-repository","title":"Step 2: Prepare Your Code Repository","text":"<p>If your research code lives in a Git repository, LabLink can automatically clone it onto each VM.</p> <p>Requirements: - Public repository (or configure SSH keys for private) - Code that can run non-interactively - Dependencies installable via package manager</p> <p>Example Repository Structure: <pre><code>your-research-repo/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 setup.py (or pyproject.toml)\n\u251c\u2500\u2500 your_package/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 analysis.py\n\u251c\u2500\u2500 configs/\n\u2502   \u2514\u2500\u2500 default_config.yaml\n\u2514\u2500\u2500 scripts/\n    \u2514\u2500\u2500 run_analysis.sh\n</code></pre></p> <p>Entrypoint: Ensure your code has a clear entrypoint (script, main function, etc.)</p>"},{"location":"adapting/#step-3-update-lablink-configuration","title":"Step 3: Update LabLink Configuration","text":"<p>Edit the allocator configuration to use your custom image and repository.</p> <p><code>lablink-infrastructure/config/config.yaml</code>:</p> <pre><code>machine:\n  machine_type: \"g4dn.xlarge\"  # Choose appropriate instance type\n  image: \"ghcr.io/your-org/your-research-image:latest\"\n  ami_id: \"ami-067cc81f948e50e06\"  # Ubuntu 20.04 + Docker (us-west-2)\n  repository: \"https://github.com/your-org/your-research-code.git\"\n  software: \"your-software-name\"\n\n# ... rest of config\n</code></pre> <p>Key Fields:</p> <ul> <li><code>image</code>: Your Docker image from Step 1</li> <li><code>repository</code>: Your Git repository (or empty string if none)</li> <li><code>software</code>: Identifier for your software (used by client service)</li> <li><code>machine_type</code>: EC2 instance type appropriate for your workload</li> </ul>"},{"location":"adapting/#step-4-test-locally","title":"Step 4: Test Locally","text":"<p>Before deploying to AWS, test your setup locally.</p>"},{"location":"adapting/#run-your-docker-image","title":"Run Your Docker Image","text":"<pre><code>docker run -d \\\n  --name test-client \\\n  -e ALLOCATOR_HOST=localhost \\\n  -e ALLOCATOR_PORT=5000 \\\n  ghcr.io/your-org/your-research-image:latest\n</code></pre>"},{"location":"adapting/#check-logs","title":"Check Logs","text":"<pre><code>docker logs test-client\n</code></pre> <p>Verify: - Image starts without errors - Dependencies are available - Your code runs as expected</p>"},{"location":"adapting/#test-full-stack","title":"Test Full Stack","text":"<p>Run both allocator and your client locally:</p> <pre><code># Terminal 1: Start allocator\ndocker run -d -p 5000:5000 --name allocator \\\n  ghcr.io/talmolab/lablink-allocator-image:latest\n\n# Terminal 2: Start your client\ndocker run -d --name client \\\n  -e ALLOCATOR_HOST=host.docker.internal \\\n  -e ALLOCATOR_PORT=5000 \\\n  ghcr.io/your-org/your-research-image:latest\n\n# Check allocator web UI\nopen http://localhost:5000\n</code></pre>"},{"location":"adapting/#step-5-deploy-to-aws","title":"Step 5: Deploy to AWS","text":"<p>Once local testing succeeds, deploy to AWS.</p>"},{"location":"adapting/#update-configuration","title":"Update Configuration","text":"<p>Commit your configuration changes:</p> <pre><code>git add lablink-infrastructure/config/config.yaml\ngit commit -m \"Configure LabLink for [your software]\"\ngit push\n</code></pre>"},{"location":"adapting/#deploy-via-terraform","title":"Deploy via Terraform","text":"<pre><code>cd lablink-infrastructure\n\nterraform init\n\nterraform apply \\\n  -var=\"resource_suffix=dev\" \\\n  -var=\"allocator_image_tag=linux-amd64-latest-test\"\n</code></pre>"},{"location":"adapting/#verify-deployment","title":"Verify Deployment","text":"<ol> <li> <p>Get allocator IP:    <pre><code>terraform output ec2_public_ip\n</code></pre></p> </li> <li> <p>Access web interface: <code>http://&lt;ec2_ip&gt;:80</code></p> </li> <li> <p>Create client VMs via admin interface</p> </li> <li> <p>Monitor VM creation and status</p> </li> </ol>"},{"location":"adapting/#advanced-customization","title":"Advanced Customization","text":""},{"location":"adapting/#custom-ami","title":"Custom AMI","text":"<p>For faster VM startup, create a custom AMI with your software pre-installed:</p> <pre><code># Launch base instance\naws ec2 run-instances --image-id ami-067cc81f948e50e06 ...\n\n# SSH in and install your software\nssh -i key.pem ubuntu@&lt;instance-ip&gt;\nsudo apt-get update\n# ... install your software\n\n# Create AMI\naws ec2 create-image \\\n  --instance-id i-xxxxx \\\n  --name \"your-software-ami\" \\\n  --description \"Custom AMI with your software\"\n\n# Use in config.yaml\nmachine:\n  ami_id: \"ami-your-custom-ami\"\n</code></pre> <p>Benefits: - Faster VM startup - Pre-installed dependencies - Consistent environment</p>"},{"location":"adapting/#environment-specific-configurations","title":"Environment-Specific Configurations","text":"<p>Create separate configs for different workloads:</p> <p><code>conf/config-cpu.yaml</code>: <pre><code>machine:\n  machine_type: \"c5.2xlarge\"  # CPU-optimized\n  image: \"ghcr.io/your-org/your-research-image-cpu:latest\"\n  software: \"your-software-cpu\"\n</code></pre></p> <p><code>conf/config-gpu.yaml</code>: <pre><code>machine:\n  machine_type: \"p3.2xlarge\"  # GPU-optimized\n  image: \"ghcr.io/your-org/your-research-image-gpu:latest\"\n  software: \"your-software-gpu\"\n</code></pre></p> <p>Use with: <pre><code>python main.py --config-name=config-gpu\n</code></pre></p>"},{"location":"adapting/#multi-software-support","title":"Multi-Software Support","text":"<p>Support multiple research software packages in one deployment:</p> <pre><code># Use software identifier to select behavior\nmachine:\n  software: \"multi\"  # Or pass dynamically\n\n# Client code checks software identifier:\n# if config.client.software == \"sleap\":\n#     run_sleap()\n# elif config.client.software == \"your_tool\":\n#     run_your_tool()\n</code></pre>"},{"location":"adapting/#private-docker-registries","title":"Private Docker Registries","text":"<p>Use private registries (Docker Hub, ECR):</p> <ol> <li>Store registry credentials in AWS Secrets Manager</li> <li>Update user data script to authenticate:    <pre><code>aws ecr get-login-password --region us-west-2 | \\\n  docker login --username AWS --password-stdin &lt;account&gt;.dkr.ecr.us-west-2.amazonaws.com\n</code></pre></li> <li>Reference private image:    <pre><code>image: \"&lt;account&gt;.dkr.ecr.us-west-2.amazonaws.com/your-image:latest\"\n</code></pre></li> </ol>"},{"location":"adapting/#custom-health-checks","title":"Custom Health Checks","text":"<p>Implement software-specific health monitoring:</p> <p><code>your_health_check.py</code>: <pre><code>import requests\n\ndef check_health():\n    \"\"\"Check if your software is healthy.\"\"\"\n    # Example: Check GPU availability\n    try:\n        import torch\n        assert torch.cuda.is_available()\n    except:\n        return False\n\n    # Example: Check disk space\n    import shutil\n    _, _, free = shutil.disk_usage(\"/\")\n    if free &lt; 10 * 1024**3:  # Less than 10GB\n        return False\n\n    return True\n\ndef report_to_allocator(status):\n    \"\"\"Report status to allocator.\"\"\"\n    requests.post(\n        f\"http://{ALLOCATOR_HOST}:{ALLOCATOR_PORT}/health\",\n        json={\"status\": \"healthy\" if status else \"unhealthy\"}\n    )\n</code></pre></p>"},{"location":"adapting/#example-adapting-for-pytorch-training","title":"Example: Adapting for PyTorch Training","text":"<p>Complete example for a PyTorch training workflow.</p>"},{"location":"adapting/#dockerfile","title":"Dockerfile","text":"<pre><code>FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu20.04\n\n# Install Python and dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    python3.9 python3-pip git \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Install PyTorch\nRUN pip3 install torch torchvision torchaudio \\\n    --index-url https://download.pytorch.org/whl/cu118\n\n# Install your training code dependencies\nCOPY requirements.txt /app/\nRUN pip3 install -r /app/requirements.txt\n\n# Copy training scripts\nCOPY train.py /app/\nCOPY utils/ /app/utils/\n\nWORKDIR /app\nCMD [\"python3\", \"train.py\"]\n</code></pre>"},{"location":"adapting/#configuration","title":"Configuration","text":"<pre><code>machine:\n  machine_type: \"g5.2xlarge\"  # A10G GPU\n  image: \"ghcr.io/your-org/pytorch-training:latest\"\n  repository: \"https://github.com/your-org/training-data.git\"\n  software: \"pytorch-training\"\n  ami_id: \"ami-067cc81f948e50e06\"\n</code></pre>"},{"location":"adapting/#training-script","title":"Training Script","text":"<p><code>train.py</code>: <pre><code>import torch\nimport sys\n\ndef main():\n    # Check GPU\n    if not torch.cuda.is_available():\n        print(\"ERROR: No GPU available\")\n        sys.exit(1)\n\n    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n\n    # Your training code here\n    model = YourModel().cuda()\n    optimizer = torch.optim.Adam(model.parameters())\n\n    # Training loop\n    for epoch in range(100):\n        train_epoch(model, optimizer)\n        print(f\"Epoch {epoch} complete\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre></p>"},{"location":"adapting/#troubleshooting","title":"Troubleshooting","text":""},{"location":"adapting/#image-wont-start","title":"Image Won't Start","text":"<p>Check: <pre><code>docker logs &lt;container&gt;\n</code></pre></p> <p>Common issues: - Missing dependencies - Incorrect entrypoint - Permission errors</p>"},{"location":"adapting/#repository-clone-fails","title":"Repository Clone Fails","text":"<p>Check: - Repository URL is correct - Repository is public (or SSH keys configured) - Network connectivity from VM</p>"},{"location":"adapting/#software-not-found","title":"Software Not Found","text":"<p>Check: - Software installed in Docker image - PATH environment variable set correctly - Dependencies installed</p>"},{"location":"adapting/#performance-issues","title":"Performance Issues","text":"<p>Check: - Instance type appropriate for workload - GPU drivers installed (for GPU instances) - Sufficient disk space</p>"},{"location":"adapting/#best-practices","title":"Best Practices","text":"<ol> <li>Test locally first: Always test Docker images locally before AWS deployment</li> <li>Pin versions: Use specific tags (<code>v1.0.0</code>) not <code>:latest</code> in production</li> <li>Minimize image size: Remove unnecessary dependencies</li> <li>Document requirements: Clear README for your custom setup</li> <li>Version your images: Tag images with version numbers</li> <li>Use multi-stage builds: Reduce final image size</li> <li>Cache dependencies: Layer Dockerfile for faster builds</li> </ol>"},{"location":"adapting/#next-steps","title":"Next Steps","text":"<ul> <li>Deployment: Deploy your customized setup</li> <li>Workflows: Set up CI/CD for your images</li> <li>Configuration: Fine-tune your settings</li> <li>FAQ: Common customization questions</li> </ul>"},{"location":"adapting/#need-help","title":"Need Help?","text":"<ul> <li>Check Troubleshooting for common issues</li> <li>Review example configurations (if available)</li> <li>Open an issue on GitHub</li> </ul>"},{"location":"architecture/","title":"Architecture","text":"<p>This page describes LabLink's architecture, components, and how they interact.</p>"},{"location":"architecture/#system-overview","title":"System Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         GitHub                               \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502\n\u2502  \u2502  Source Code   \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2502 GitHub Actions   \u2502              \u2502\n\u2502  \u2502  Repository    \u2502      \u2502  (CI/CD)         \u2502              \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                     \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502                                 \u2502\n           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n           \u2502  Docker Images  \u2502           \u2502   Terraform Apply     \u2502\n           \u2502  ghcr.io        \u2502           \u2502   Infrastructure      \u2502\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                     \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        AWS Cloud                   \u2502             \u2502\n\u2502                                                    \u2502             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502              Allocator EC2 Instance                        \u2502 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 \u2502\n\u2502  \u2502  \u2502  Docker Container: lablink-allocator                 \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  \u2502  Flask App     \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2502  PostgreSQL DB   \u2502       \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  \u2502  (Port 80)     \u2502      \u2502  (Port 5432)     \u2502       \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  \u2502                \u2502      \u2502                  \u2502       \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  \u2502  - Web UI      \u2502      \u2502  - VM table      \u2502       \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  \u2502  - API         \u2502      \u2502  - Triggers      \u2502       \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  \u2502  - Terraform   \u2502      \u2502  - Listen/Notify \u2502       \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502  \u2502 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                            \u2502                                     \u2502\n\u2502                            \u2502 spawns                              \u2502\n\u2502                            \u25bc                                     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502          Client EC2 Instances (Dynamic)                  \u2502   \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502   \u2502\n\u2502  \u2502  \u2502  Docker Container: lablink-client                  \u2502  \u2502   \u2502\n\u2502  \u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502  \u2502   \u2502\n\u2502  \u2502  \u2502  \u2502  Subscribe   \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502  Research Code  \u2502       \u2502  \u2502   \u2502\n\u2502  \u2502  \u2502  \u2502  Service     \u2502       \u2502  (User Repo)    \u2502       \u2502  \u2502   \u2502\n\u2502  \u2502  \u2502  \u2502              \u2502       \u2502                 \u2502       \u2502  \u2502   \u2502\n\u2502  \u2502  \u2502  \u2502  - Heartbeat \u2502       \u2502  - SLEAP/Custom \u2502       \u2502  \u2502   \u2502\n\u2502  \u2502  \u2502  \u2502  - GPU Check \u2502       \u2502  - Your Software\u2502       \u2502  \u2502   \u2502\n\u2502  \u2502  \u2502  \u2502  - Status    \u2502       \u2502                 \u2502       \u2502  \u2502   \u2502\n\u2502  \u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502  \u2502   \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502   \u2502\n\u2502  \u2502                                                           \u2502   \u2502\n\u2502  \u2502  (Multiple instances, dynamically created)               \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  Security Groups \u2502  \u2502  Elastic IPs  \u2502  \u2502  S3 Bucket     \u2502   \u2502\n\u2502  \u2502  - Port 80       \u2502  \u2502  (Static IPs) \u2502  \u2502  (TF State)    \u2502   \u2502\n\u2502  \u2502  - Port 22       \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502  \u2502  - Port 5432     \u2502                                           \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/#component-details","title":"Component Details","text":""},{"location":"architecture/#allocator-service","title":"Allocator Service","text":"<p>Purpose: Central management server for VM allocation and orchestration.</p> <p>Technology Stack: - Flask: Web application framework - PostgreSQL: Relational database for VM state - SQLAlchemy: ORM for database operations - Terraform: Infrastructure provisioning - Docker: Containerization</p> <p>Key Responsibilities:</p> <ol> <li>Web Interface:</li> <li>Admin dashboard for VM management</li> <li>VM creation interface</li> <li> <p>Instance listing and monitoring</p> </li> <li> <p>API Endpoints:</p> </li> <li><code>/request_vm</code>: Allocate VM to user</li> <li><code>/admin/create</code>: Create new VM instances</li> <li><code>/admin/instances</code>: List all instances</li> <li><code>/admin/destroy</code>: Destroy instances</li> <li> <p><code>/vm_startup</code>: Client registration</p> </li> <li> <p>Database Management:</p> </li> <li>Tracks VM states (available, in-use, failed)</li> <li>PostgreSQL listen/notify for real-time updates</li> <li> <p>Automated triggers for state changes</p> </li> <li> <p>Infrastructure Orchestration:</p> </li> <li>Spawns client VMs via Terraform</li> <li>Manages AWS credentials</li> <li>Handles security group configuration</li> </ol> <p>Configuration: See <code>packages/allocator/src/lablink_allocator/conf/structured_config.py</code></p>"},{"location":"architecture/#client-service","title":"Client Service","text":"<p>Purpose: Runs on dynamically created VMs to execute research workloads.</p> <p>Technology Stack: - Python: Service implementation - Docker: Container runtime - Custom Software: SLEAP or user-defined</p> <p>Key Responsibilities:</p> <ol> <li>Health Monitoring:</li> <li>GPU health checks (every 20 seconds)</li> <li>System resource monitoring</li> <li> <p>Reports status to allocator</p> </li> <li> <p>Allocator Communication:</p> </li> <li>Heartbeat mechanism</li> <li>Status updates (in-use, available)</li> <li> <p>Failure reporting</p> </li> <li> <p>Research Execution:</p> </li> <li>Clones configured repository</li> <li>Runs containerized research software</li> <li>Executes user-defined CRD commands</li> </ol> <p>Configuration: See <code>packages/client/src/lablink_client/conf/structured_config.py</code></p>"},{"location":"architecture/#database-schema","title":"Database Schema","text":"<p>Table: <code>vms</code></p> Column Type Description <code>id</code> SERIAL Primary key <code>hostname</code> VARCHAR(255) VM hostname/identifier <code>email</code> VARCHAR(255) User email <code>status</code> VARCHAR(50) VM status (available/in-use/failed) <code>crd_command</code> TEXT Command to execute on VM <code>created_at</code> TIMESTAMP Creation timestamp <code>updated_at</code> TIMESTAMP Last update timestamp <p>Triggers: - <code>notify_vm_update</code>: Sends PostgreSQL NOTIFY on row changes</p>"},{"location":"architecture/#infrastructure-components","title":"Infrastructure Components","text":""},{"location":"architecture/#security-groups","title":"Security Groups","text":"<p>Allocator Security Group: - Port 80 (HTTP): Web interface and API - Port 22 (SSH): Administrative access - Port 5432 (PostgreSQL): Database connections from clients</p> <p>Client Security Groups: - Port 22 (SSH): Administrative access - Egress: Full internet access for package downloads</p>"},{"location":"architecture/#networking","title":"Networking","text":"<ul> <li>Elastic IPs: Static IPs for allocators (one per environment)</li> <li>VPC: Default VPC or custom (configurable)</li> <li>Route 53 (Optional): DNS management for friendly URLs</li> </ul>"},{"location":"architecture/#storage","title":"Storage","text":"<ul> <li>S3 Buckets: Terraform state storage</li> <li>Separate state per environment (dev/test/prod)</li> <li>Versioning enabled</li> <li> <p>Encrypted at rest</p> </li> <li> <p>EBS Volumes: Instance root volumes</p> </li> <li>Allocator: 30GB (configurable)</li> <li>Clients: Depends on AMI</li> </ul>"},{"location":"architecture/#data-flow","title":"Data Flow","text":""},{"location":"architecture/#vm-request-flow","title":"VM Request Flow","text":"<pre><code>1. User submits request via Web UI or API\n   \u2502\n   \u25bc\n2. Flask app receives request\n   \u2502\n   \u25bc\n3. Query database for available VM\n   \u2502\n   \u251c\u2500 If available:\n   \u2502  \u251c\u2500 Update VM status to \"in-use\"\n   \u2502  \u251c\u2500 Return VM details to user\n   \u2502  \u2514\u2500 Notify client via PostgreSQL NOTIFY\n   \u2502\n   \u2514\u2500 If none available:\n      \u2514\u2500 Return error / queue request\n</code></pre>"},{"location":"architecture/#vm-creation-flow","title":"VM Creation Flow","text":"<pre><code>1. Admin creates VMs via /admin/create\n   \u2502\n   \u25bc\n2. Flask app calls Terraform subprocess\n   \u2502\n   \u25bc\n3. Terraform provisions EC2 instances\n   \u2502\n   \u251c\u2500 Creates security group\n   \u251c\u2500 Generates SSH key pair\n   \u251c\u2500 Launches EC2 with user_data script\n   \u2514\u2500 Returns instance details\n   \u2502\n   \u25bc\n4. User data script runs on boot:\n   \u2502\n   \u251c\u2500 Pulls Docker image\n   \u251c\u2500 Clones repository\n   \u251c\u2500 Starts client service\n   \u2514\u2500 Registers with allocator\n</code></pre>"},{"location":"architecture/#health-check-flow","title":"Health Check Flow","text":"<pre><code>Client VM (every 20s):\n\u2502\n\u251c\u2500 Check GPU status\n\u251c\u2500 Check system resources\n\u2502\n\u25bc\nSend status to allocator API\n\u2502\n\u25bc\nAllocator updates database\n\u2502\n\u2514\u2500 If unhealthy: Mark as \"failed\"\n</code></pre>"},{"location":"architecture/#deployment-environments","title":"Deployment Environments","text":"<p>LabLink supports multiple isolated environments:</p> Environment Purpose Image Tag Terraform Backend <code>dev</code> Local development <code>*-test</code> Local state <code>test</code> Staging/testing <code>*-test</code> <code>backend-test.hcl</code> <code>prod</code> Production Pinned tags <code>backend-prod.hcl</code> <p>Each environment has: - Separate Terraform state - Unique resource naming (<code>-dev</code>, <code>-test</code>, <code>-prod</code> suffix) - Independent AWS resources</p>"},{"location":"architecture/#cicd-pipeline","title":"CI/CD Pipeline","text":"<p>See Workflows for detailed CI/CD architecture.</p> <p>Key Workflows:</p> <ol> <li>Build Images (<code>lablink-images.yml</code>):</li> <li>Triggers on code changes</li> <li>Builds allocator and client Docker images</li> <li> <p>Pushes to GitHub Container Registry</p> </li> <li> <p>Terraform Deploy (<code>lablink-allocator-terraform.yml</code>):</p> </li> <li>Triggers on branch push or manual dispatch</li> <li>Applies infrastructure changes</li> <li> <p>Supports environment selection</p> </li> <li> <p>Destroy (<code>lablink-allocator-destroy.yml</code>):</p> </li> <li>Manual trigger only</li> <li>Safely destroys environment resources</li> </ol>"},{"location":"architecture/#security-architecture","title":"Security Architecture","text":"<ul> <li>OIDC Authentication: GitHub Actions authenticate to AWS without stored credentials</li> <li>SSH Keys: Auto-generated per environment, ephemeral artifacts</li> <li>Secrets: Managed via GitHub Secrets and AWS Secrets Manager</li> <li>Network: Security groups restrict access by port and source</li> </ul> <p>See Security for detailed security considerations.</p>"},{"location":"architecture/#scalability-considerations","title":"Scalability Considerations","text":"<p>Current Architecture: - Single allocator per environment - Multiple clients per allocator - Database handles concurrent requests</p> <p>Scaling Options: - Horizontal: Multiple allocators with load balancer - Vertical: Larger instance types for allocator - Database: RDS for managed PostgreSQL at scale</p>"},{"location":"architecture/#technology-choices","title":"Technology Choices","text":"Component Technology Rationale Web Framework Flask Lightweight, Python ecosystem Database PostgreSQL LISTEN/NOTIFY, ACID compliance IaC Terraform Declarative, AWS support Containers Docker Portability, dependency isolation CI/CD GitHub Actions Native GitHub integration Config Hydra/OmegaConf Structured configs, easy overrides"},{"location":"architecture/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration: Customize components</li> <li>Deployment: Deploy the system</li> <li>Testing: Test the codebase</li> </ul>"},{"location":"aws-setup/","title":"AWS Setup from Scratch","text":"<p>This comprehensive guide walks you through setting up all required AWS resources for LabLink deployment from scratch.</p>"},{"location":"aws-setup/#overview","title":"Overview","text":"<p>To deploy LabLink, you'll need:</p> <ol> <li>AWS account with appropriate permissions</li> <li>S3 bucket for Terraform state</li> <li>Elastic IPs for each environment</li> <li>IAM role for GitHub Actions (OIDC)</li> <li>Optional: Route 53 hosted zone for DNS</li> </ol>"},{"location":"aws-setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>AWS account with admin access (or appropriate IAM permissions)</li> <li>AWS CLI installed and configured</li> <li>Basic understanding of AWS services</li> <li>Chosen AWS region for deployment</li> </ul>"},{"location":"aws-setup/#choosing-an-aws-region","title":"Choosing an AWS Region","text":"<p>Before starting, select the AWS region where you'll deploy LabLink. This is an important decision that affects performance, cost, and compliance.</p>"},{"location":"aws-setup/#region-selection-criteria","title":"Region Selection Criteria","text":"<p>1. Latency &amp; Geographic Proximity - Choose a region closest to your users for best performance - Lower latency = better user experience for VM access - Test latency: <code>ping ec2.{region}.amazonaws.com</code></p> <p>2. Instance Availability - Not all instance types are available in all regions - GPU instances (g4dn, g5, p3) have limited regional availability - Check availability: AWS Regional Services</p> <p>3. Pricing - EC2 pricing varies by region (5-30% difference) - US regions are typically cheaper than EU/Asia - Check pricing: EC2 Pricing Calculator</p> <p>4. Compliance &amp; Data Residency - GDPR (Europe): Use <code>eu-west-1</code>, <code>eu-central-1</code> - HIPAA (US Healthcare): Any US region with BAA - Data sovereignty requirements may mandate specific regions</p> <p>5. Service Availability - All LabLink features require: EC2, VPC, S3, Route 53 - These are available in all commercial regions</p>"},{"location":"aws-setup/#recommended-regions","title":"Recommended Regions","text":"Region Code Best For Notes US East (N. Virginia) <code>us-east-1</code> US East Coast, lowest cost Largest region, occasional availability issues US West (Oregon) <code>us-west-2</code> US West Coast, default Good balance of cost and stability Europe (Ireland) <code>eu-west-1</code> Europe, GDPR Best EU region for cost and availability Asia Pacific (Tokyo) <code>ap-northeast-1</code> Asia Good for Asian users Asia Pacific (Singapore) <code>ap-southeast-1</code> Southeast Asia Alternative for Asian users"},{"location":"aws-setup/#list-all-available-regions","title":"List All Available Regions","text":"<pre><code># AWS CLI\naws ec2 describe-regions --output table\n\n# Or with region names\naws ec2 describe-regions --query \"Regions[*].[RegionName,OptInStatus]\" --output table\n</code></pre>"},{"location":"aws-setup/#test-latency-to-regions","title":"Test Latency to Regions","text":"<pre><code># Test ping to various regions (macOS/Linux)\nfor region in us-east-1 us-west-2 eu-west-1 ap-northeast-1; do\n  echo -n \"$region: \"\n  ping -c 3 ec2.$region.amazonaws.com | grep avg | awk -F'/' '{print $5 \" ms\"}'\ndone\n</code></pre>"},{"location":"aws-setup/#configure-your-region-choice","title":"Configure Your Region Choice","text":"<p>Once you've selected a region, you'll need to configure it in two places:</p> <ol> <li>GitHub Secret: <code>AWS_REGION</code> (covered in Step 4.6)</li> <li>config.yaml: Must match the secret (covered later)</li> </ol> <p>Example: <pre><code># lablink-infrastructure/config/config.yaml\napp:\n  region: \"us-west-2\"  # Must match AWS_REGION secret\n</code></pre></p>"},{"location":"aws-setup/#step-1-iam-permissions-setup","title":"Step 1: IAM Permissions Setup","text":""},{"location":"aws-setup/#create-iam-user-if-needed","title":"Create IAM User (If Needed)","text":"<p>If you don't have an IAM user with sufficient permissions:</p> <pre><code>aws iam create-user --user-name lablink-admin\n</code></pre>"},{"location":"aws-setup/#attach-required-policies","title":"Attach Required Policies","text":"<p>Attach these managed policies for LabLink operations:</p> <pre><code>aws iam attach-user-policy \\\n  --user-name lablink-admin \\\n  --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess\n\naws iam attach-user-policy \\\n  --user-name lablink-admin \\\n  --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess\n\naws iam attach-user-policy \\\n  --user-name lablink-admin \\\n  --policy-arn arn:aws:iam::aws:policy/IAMFullAccess\n</code></pre>"},{"location":"aws-setup/#create-access-keys","title":"Create Access Keys","text":"<p>For local Terraform usage:</p> <pre><code>aws iam create-access-key --user-name lablink-admin\n</code></pre> <p>Save the <code>AccessKeyId</code> and <code>SecretAccessKey</code> securely.</p> <p>Configure AWS CLI: <pre><code>aws configure\n# Enter AccessKeyId\n# Enter SecretAccessKey\n# Enter region (e.g., us-west-2)\n# Enter output format (json)\n</code></pre></p>"},{"location":"aws-setup/#step-2-s3-bucket-for-terraform-state","title":"Step 2: S3 Bucket for Terraform State","text":""},{"location":"aws-setup/#create-s3-bucket","title":"Create S3 Bucket","text":"<p>Choose a globally unique bucket name:</p> <pre><code>export BUCKET_NAME=\"tf-state-lablink-allocator-bucket-$(date +%s)\"\n\naws s3api create-bucket \\\n  --bucket $BUCKET_NAME \\\n  --region us-west-2 \\\n  --create-bucket-configuration LocationConstraint=us-west-2\n</code></pre>"},{"location":"aws-setup/#enable-versioning","title":"Enable Versioning","text":"<p>Protect against accidental deletions:</p> <pre><code>aws s3api put-bucket-versioning \\\n  --bucket $BUCKET_NAME \\\n  --versioning-configuration Status=Enabled\n</code></pre>"},{"location":"aws-setup/#enable-encryption","title":"Enable Encryption","text":"<p>Encrypt state files at rest:</p> <pre><code>aws s3api put-bucket-encryption \\\n  --bucket $BUCKET_NAME \\\n  --server-side-encryption-configuration '{\n    \"Rules\": [{\n      \"ApplyServerSideEncryptionByDefault\": {\n        \"SSEAlgorithm\": \"AES256\"\n      }\n    }]\n  }'\n</code></pre>"},{"location":"aws-setup/#block-public-access","title":"Block Public Access","text":"<p>Ensure bucket is private:</p> <pre><code>aws s3api put-public-access-block \\\n  --bucket $BUCKET_NAME \\\n  --public-access-block-configuration \\\n    BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true\n</code></pre>"},{"location":"aws-setup/#update-configuration","title":"Update Configuration","text":"<p>Update bucket name in your LabLink configuration:</p> <p><code>lablink-infrastructure/config/config.yaml</code>: <pre><code>bucket_name: \"tf-state-lablink-allocator-bucket-1234567890\"\n</code></pre></p> <p><code>lablink-infrastructure/backend-test.hcl</code> and <code>backend-prod.hcl</code>: <pre><code>bucket = \"tf-state-lablink-allocator-bucket-1234567890\"\nkey    = \"lablink-allocator-&lt;env&gt;/terraform.tfstate\"\nregion = \"us-west-2\"\n</code></pre></p>"},{"location":"aws-setup/#step-3-elastic-ip-allocation","title":"Step 3: Elastic IP Allocation","text":"<p>Allocate static IPs for test and production environments.</p>"},{"location":"aws-setup/#allocate-elastic-ips","title":"Allocate Elastic IPs","text":"<pre><code># Test environment\naws ec2 allocate-address --region us-west-2 --tag-specifications \\\n  'ResourceType=elastic-ip,Tags=[{Key=Environment,Value=test},{Key=Project,Value=lablink}]'\n\n# Production environment\naws ec2 allocate-address --region us-west-2 --tag-specifications \\\n  'ResourceType=elastic-ip,Tags=[{Key=Environment,Value=prod},{Key=Project,Value=lablink}]'\n</code></pre>"},{"location":"aws-setup/#record-allocation-ids","title":"Record Allocation IDs","text":"<p>Save the <code>AllocationId</code> from each command output (format: <code>eipalloc-xxxxx</code>).</p>"},{"location":"aws-setup/#tag-elastic-ips","title":"Tag Elastic IPs","text":"<pre><code>aws ec2 create-tags \\\n  --resources eipalloc-test-xxxxx \\\n  --tags Key=Name,Value=lablink-test-eip\n\naws ec2 create-tags \\\n  --resources eipalloc-prod-xxxxx \\\n  --tags Key=Name,Value=lablink-prod-eip\n</code></pre>"},{"location":"aws-setup/#update-terraform-configuration","title":"Update Terraform Configuration","text":"<p><code>lablink-infrastructure/main.tf</code>: <pre><code>variable \"allocated_eip\" {\n  description = \"Pre-allocated Elastic IP for production/test\"\n  type        = string\n  default     = \"\"\n}\n\nresource \"aws_eip_association\" \"lablink\" {\n  count         = var.allocated_eip != \"\" ? 1 : 0\n  instance_id   = aws_instance.lablink_allocator.id\n  allocation_id = var.allocated_eip\n}\n</code></pre></p> <p>Use when deploying: <pre><code>terraform apply \\\n  -var=\"resource_suffix=test\" \\\n  -var=\"allocated_eip=eipalloc-test-xxxxx\"\n</code></pre></p>"},{"location":"aws-setup/#step-4-github-actions-oidc-configuration","title":"Step 4: GitHub Actions OIDC Configuration","text":"<p>Set up OpenID Connect (OIDC) for GitHub Actions to authenticate to AWS without storing long-term credentials. This is the recommended and most secure method for CI/CD authentication.</p>"},{"location":"aws-setup/#41-check-for-existing-oidc-provider","title":"4.1: Check for Existing OIDC Provider","text":"<p>Before creating a new OIDC provider, check if one already exists:</p>"},{"location":"aws-setup/#aws-cli","title":"AWS CLI","text":"<pre><code># Check your current AWS account\naws sts get-caller-identity\n\n# List OIDC providers\naws iam list-open-id-connect-providers\n</code></pre> <p>Look for a provider with URL <code>token.actions.githubusercontent.com</code>.</p>"},{"location":"aws-setup/#aws-console","title":"AWS Console","text":"<ol> <li>Go to IAM \u2192 Identity providers</li> <li>Look for provider with URL <code>token.actions.githubusercontent.com</code></li> <li>If it exists, note the ARN and skip to Step 4.2</li> </ol>"},{"location":"aws-setup/#42-create-oidc-provider-if-needed","title":"4.2: Create OIDC Provider (If Needed)","text":""},{"location":"aws-setup/#aws-cli_1","title":"AWS CLI","text":"<pre><code>aws iam create-open-id-connect-provider \\\n  --url https://token.actions.githubusercontent.com \\\n  --client-id-list sts.amazonaws.com \\\n  --thumbprint-list 6938fd4d98bab03faadb97b34396831e3780aea1\n</code></pre> <p>Note: If you get <code>EntityAlreadyExists</code> error, the provider already exists. You can proceed to create the IAM role.</p>"},{"location":"aws-setup/#aws-console_1","title":"AWS Console","text":"<ol> <li>Go to IAM \u2192 Identity providers</li> <li>Click Add provider</li> <li>Select OpenID Connect</li> <li>Provider URL: <code>https://token.actions.githubusercontent.com</code></li> <li>Click Get thumbprint (should show <code>6938fd4d98bab03faadb97b34396831e3780aea1</code>)</li> <li>Audience: <code>sts.amazonaws.com</code></li> <li>Click Add provider</li> </ol>"},{"location":"aws-setup/#43-create-iam-role-for-github-actions","title":"4.3: Create IAM Role for GitHub Actions","text":""},{"location":"aws-setup/#option-a-aws-cli-recommended-for-multiple-repositories","title":"Option A: AWS CLI (Recommended for Multiple Repositories)","text":"<p>Step 1: Get your AWS account ID:</p> <pre><code>aws sts get-caller-identity --query \"Account\" --output text\n</code></pre> <p>Step 2: Create trust policy file <code>github-trust-policy.json</code>:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Federated\": \"arn:aws:iam::YOUR_ACCOUNT_ID:oidc-provider/token.actions.githubusercontent.com\"\n      },\n      \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"token.actions.githubusercontent.com:aud\": \"sts.amazonaws.com\"\n        },\n        \"StringLike\": {\n          \"token.actions.githubusercontent.com:sub\": [\n            \"repo:YOUR_ORG/lablink:*\",\n            \"repo:YOUR_ORG/lablink-template:*\",\n            \"repo:YOUR_ORG/sleap-lablink:*\"\n          ]\n        }\n      }\n    }\n  ]\n}\n</code></pre> <p>Important: Replace: - <code>YOUR_ACCOUNT_ID</code> with your AWS account ID (from Step 1) - <code>YOUR_ORG</code> with your GitHub organization/username (e.g., <code>talmolab</code>)</p> <p>Step 3: Create the IAM role:</p> <pre><code>aws iam create-role \\\n  --role-name GitHubActionsLabLinkRole \\\n  --assume-role-policy-document file://github-trust-policy.json \\\n  --description \"Role for GitHub Actions to deploy LabLink infrastructure\"\n</code></pre> <p>Step 4: Note the role ARN from the output (format: <code>arn:aws:iam::ACCOUNT_ID:role/GitHubActionsLabLinkRole</code>)</p>"},{"location":"aws-setup/#option-b-aws-console","title":"Option B: AWS Console","text":"<p>Step 1: Create the role</p> <ol> <li>Go to IAM \u2192 Roles</li> <li>Click Create role</li> <li>Select Web identity as trusted entity type</li> <li>Choose:</li> <li>Identity provider: <code>token.actions.githubusercontent.com</code></li> <li>Audience: <code>sts.amazonaws.com</code></li> <li>Click Next</li> </ol> <p>Step 2: Skip permissions for now (we'll add them in Step 4.4)</p> <ol> <li>Click Next</li> <li>Role name: <code>GitHubActionsLabLinkRole</code></li> <li>Description: <code>Role for GitHub Actions to deploy LabLink infrastructure</code></li> <li>Click Create role</li> </ol> <p>Step 3: Edit trust policy for multiple repositories</p> <ol> <li>Click on the newly created role</li> <li>Go to Trust relationships tab</li> <li>Click Edit trust policy</li> <li>Replace the trust policy with:</li> </ol> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Federated\": \"arn:aws:iam::YOUR_ACCOUNT_ID:oidc-provider/token.actions.githubusercontent.com\"\n      },\n      \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"token.actions.githubusercontent.com:aud\": \"sts.amazonaws.com\"\n        },\n        \"StringLike\": {\n          \"token.actions.githubusercontent.com:sub\": [\n            \"repo:YOUR_ORG/lablink:*\",\n            \"repo:YOUR_ORG/lablink-template:*\",\n            \"repo:YOUR_ORG/sleap-lablink:*\"\n          ]\n        }\n      }\n    }\n  ]\n}\n</code></pre> <ol> <li>Replace <code>YOUR_ACCOUNT_ID</code> and <code>YOUR_ORG</code> with your values</li> <li>Click Update policy</li> </ol>"},{"location":"aws-setup/#44-attach-permissions-to-role","title":"4.4: Attach Permissions to Role","text":"<p>The role needs permissions to manage EC2, S3, Route53, and IAM resources for infrastructure deployment.</p>"},{"location":"aws-setup/#option-a-aws-cli-use-poweruseraccess-recommended","title":"Option A: AWS CLI - Use PowerUserAccess (Recommended)","text":"<p>This provides broad permissions suitable for infrastructure management:</p> <pre><code>aws iam attach-role-policy \\\n  --role-name GitHubActionsLabLinkRole \\\n  --policy-arn arn:aws:iam::aws:policy/PowerUserAccess\n</code></pre> <p>Note: PowerUserAccess allows all AWS services except IAM user/group management, which is appropriate for infrastructure deployment.</p>"},{"location":"aws-setup/#option-b-aws-cli-create-custom-policy","title":"Option B: AWS CLI - Create Custom Policy","text":"<p>For more restrictive permissions, create a custom policy:</p> <p>Create <code>lablink-terraform-policy.json</code>:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"TerraformStateManagement\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n        \"s3:DeleteObject\",\n        \"s3:ListBucket\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::lablink-terraform-state-*\",\n        \"arn:aws:s3:::lablink-terraform-state-*/*\",\n        \"arn:aws:s3:::tf-state-lablink-*\",\n        \"arn:aws:s3:::tf-state-lablink-*/*\"\n      ]\n    },\n    {\n      \"Sid\": \"EC2FullAccess\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:*\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Sid\": \"IAMInstanceProfile\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"iam:CreateRole\",\n        \"iam:DeleteRole\",\n        \"iam:GetRole\",\n        \"iam:PassRole\",\n        \"iam:AttachRolePolicy\",\n        \"iam:DetachRolePolicy\",\n        \"iam:PutRolePolicy\",\n        \"iam:DeleteRolePolicy\",\n        \"iam:GetRolePolicy\",\n        \"iam:CreateInstanceProfile\",\n        \"iam:DeleteInstanceProfile\",\n        \"iam:GetInstanceProfile\",\n        \"iam:AddRoleToInstanceProfile\",\n        \"iam:RemoveRoleFromInstanceProfile\",\n        \"iam:ListInstanceProfilesForRole\",\n        \"iam:ListAttachedRolePolicies\",\n        \"iam:ListRolePolicies\"\n      ],\n      \"Resource\": [\n        \"arn:aws:iam::*:role/lablink-*\",\n        \"arn:aws:iam::*:instance-profile/lablink-*\"\n      ]\n    },\n    {\n      \"Sid\": \"Route53DNS\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"route53:ListHostedZones\",\n        \"route53:GetHostedZone\",\n        \"route53:ListResourceRecordSets\",\n        \"route53:ChangeResourceRecordSets\",\n        \"route53:GetChange\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n</code></pre> <p>Attach the custom policy:</p> <pre><code>aws iam put-role-policy \\\n  --role-name GitHubActionsLabLinkRole \\\n  --policy-name LabLinkTerraformPolicy \\\n  --policy-document file://lablink-terraform-policy.json\n</code></pre>"},{"location":"aws-setup/#option-c-aws-console","title":"Option C: AWS Console","text":"<ol> <li>Go to IAM \u2192 Roles</li> <li>Click on <code>GitHubActionsLabLinkRole</code></li> <li>Click Add permissions \u2192 Attach policies</li> <li>Search for <code>PowerUserAccess</code></li> <li>Select the checkbox</li> <li>Click Add permissions</li> </ol>"},{"location":"aws-setup/#45-verify-role-configuration","title":"4.5: Verify Role Configuration","text":""},{"location":"aws-setup/#aws-cli_2","title":"AWS CLI","text":"<p>Check the role exists and has correct trust policy:</p> <pre><code># Get role details\naws iam get-role --role-name GitHubActionsLabLinkRole\n\n# Check trust policy\naws iam get-role --role-name GitHubActionsLabLinkRole \\\n  --query \"Role.AssumeRolePolicyDocument\" --output json\n\n# List attached policies\naws iam list-attached-role-policies --role-name GitHubActionsLabLinkRole\n\n# List inline policies\naws iam list-role-policies --role-name GitHubActionsLabLinkRole\n</code></pre> <p>Verify the trust policy includes all your deployment repositories.</p>"},{"location":"aws-setup/#aws-console_2","title":"AWS Console","text":"<ol> <li>Go to IAM \u2192 Roles \u2192 <code>GitHubActionsLabLinkRole</code></li> <li>Trust relationships tab: Verify repositories are listed</li> <li>Permissions tab: Verify <code>PowerUserAccess</code> or custom policy is attached</li> <li>Copy the ARN (e.g., <code>arn:aws:iam::711387140753:role/GitHubActionsLabLinkRole</code>)</li> </ol>"},{"location":"aws-setup/#46-add-github-secrets","title":"4.6: Add GitHub Secrets","text":"<p>Four secrets are required for GitHub Actions workflows to deploy infrastructure securely.</p>"},{"location":"aws-setup/#for-template-repository-lablink-template","title":"For Template Repository (<code>lablink-template</code>)","text":"<ol> <li> <p>Go to repository Settings \u2192 Secrets and variables \u2192 Actions</p> </li> <li> <p>Add AWS_ROLE_ARN secret:</p> </li> <li>Click New repository secret</li> <li>Name: <code>AWS_ROLE_ARN</code></li> <li>Value: <code>arn:aws:iam::YOUR_ACCOUNT_ID:role/GitHubActionsLabLinkRole</code></li> <li> <p>Click Add secret</p> </li> <li> <p>Add AWS_REGION secret:</p> </li> <li>Click New repository secret</li> <li>Name: <code>AWS_REGION</code></li> <li>Value: Your chosen region (e.g., <code>us-west-2</code>, <code>eu-west-1</code>, <code>ap-northeast-1</code>)</li> <li> <p>Click Add secret</p> </li> <li> <p>Add ADMIN_PASSWORD secret:</p> </li> <li>Click New repository secret</li> <li>Name: <code>ADMIN_PASSWORD</code></li> <li>Value: Your secure admin password (use a password manager to generate)</li> <li> <p>Click Add secret</p> </li> <li> <p>Add DB_PASSWORD secret:</p> </li> <li>Click New repository secret</li> <li>Name: <code>DB_PASSWORD</code></li> <li>Value: Your secure database password (use a password manager to generate)</li> <li>Click Add secret</li> </ol> <p>Note: The template repository can safely include these secrets because: - Repository permissions control who can trigger workflows - Secrets are NOT copied when creating repos from the template - External users must configure their own AWS credentials, region, and passwords</p> <p>Security: The workflow automatically injects <code>ADMIN_PASSWORD</code> and <code>DB_PASSWORD</code> into configuration files before Terraform runs, replacing <code>PLACEHOLDER_ADMIN_PASSWORD</code> and <code>PLACEHOLDER_DB_PASSWORD</code>. This prevents passwords from appearing in Terraform logs.</p>"},{"location":"aws-setup/#for-deployment-repositories-eg-sleap-lablink","title":"For Deployment Repositories (e.g., <code>sleap-lablink</code>)","text":"<p>After creating a repository from the template:</p> <ol> <li> <p>Go to the new repository Settings \u2192 Secrets and variables \u2192 Actions</p> </li> <li> <p>Add all four secrets (same process as above):</p> </li> <li><code>AWS_ROLE_ARN</code>: Same ARN as template repository</li> <li><code>AWS_REGION</code>: Your chosen region for this deployment</li> <li><code>ADMIN_PASSWORD</code>: Your secure admin password</li> <li><code>DB_PASSWORD</code>: Your secure database password</li> </ol> <p>Important: - Each deployment repository needs these secrets added manually after creation - Different deployments can use different regions if needed - Region in secret must match region in <code>config/config.yaml</code> - Use strong, unique passwords for each deployment</p>"},{"location":"aws-setup/#47-update-trust-policy-for-new-repositories","title":"4.7: Update Trust Policy for New Repositories","text":"<p>When you create new deployment repositories, update the trust policy to include them:</p>"},{"location":"aws-setup/#aws-cli_3","title":"AWS CLI","text":"<pre><code># Edit trust-policy.json to add new repository:\n# \"repo:YOUR_ORG/new-deployment:*\"\n\n# Update the role\naws iam update-assume-role-policy \\\n  --role-name GitHubActionsLabLinkRole \\\n  --policy-document file://trust-policy.json\n\n# Verify update\naws iam get-role --role-name GitHubActionsLabLinkRole \\\n  --query \"Role.AssumeRolePolicyDocument.Statement[0].Condition.StringLike\"\n</code></pre>"},{"location":"aws-setup/#aws-console_3","title":"AWS Console","text":"<ol> <li>Go to IAM \u2192 Roles \u2192 <code>GitHubActionsLabLinkRole</code></li> <li>Trust relationships tab</li> <li>Click Edit trust policy</li> <li>Add new repository to the <code>token.actions.githubusercontent.com:sub</code> array:    <pre><code>\"token.actions.githubusercontent.com:sub\": [\n  \"repo:YOUR_ORG/lablink:*\",\n  \"repo:YOUR_ORG/lablink-template:*\",\n  \"repo:YOUR_ORG/sleap-lablink:*\",\n  \"repo:YOUR_ORG/new-deployment:*\"\n]\n</code></pre></li> <li>Click Update policy</li> </ol>"},{"location":"aws-setup/#48-verify-github-actions-can-assume-role","title":"4.8: Verify GitHub Actions Can Assume Role","text":"<p>The workflows already include the OIDC authentication step:</p> <pre><code>- name: Configure AWS credentials via OIDC\n  uses: aws-actions/configure-aws-credentials@v3\n  with:\n    role-to-assume: ${{ secrets.AWS_ROLE_ARN }}\n    aws-region: us-west-2\n</code></pre> <p>Test by triggering a workflow:</p> <ol> <li>Go to Actions tab in GitHub</li> <li>Select a workflow (e.g., Terraform Deploy)</li> <li>Click Run workflow</li> <li>Check the logs for successful AWS authentication</li> </ol>"},{"location":"aws-setup/#troubleshooting-oidc-setup","title":"Troubleshooting OIDC Setup","text":""},{"location":"aws-setup/#error-not-authorized-to-perform-stsassumerolewithwebidentity","title":"Error: \"Not authorized to perform sts:AssumeRoleWithWebIdentity\"","text":"<p>Cause: Repository not in trust policy</p> <p>Solution: Add repository to trust policy (Step 4.7)</p>"},{"location":"aws-setup/#error-no-openid-connect-provider-found","title":"Error: \"No OpenID Connect provider found\"","text":"<p>Cause: OIDC provider doesn't exist</p> <p>Solution: Create OIDC provider (Step 4.2)</p>"},{"location":"aws-setup/#error-access-denied-during-deployment","title":"Error: \"Access Denied\" during deployment","text":"<p>Cause: Role lacks required permissions</p> <p>Solution: Attach PowerUserAccess or verify custom policy (Step 4.4)</p>"},{"location":"aws-setup/#verify-trust-policy-includes-repository","title":"Verify Trust Policy Includes Repository","text":"<pre><code># CLI: Check which repos can use the role\naws iam get-role --role-name GitHubActionsLabLinkRole \\\n  --query \"Role.AssumeRolePolicyDocument.Statement[0].Condition.StringLike\" \\\n  --output json\n</code></pre> <p>Console: IAM \u2192 Roles \u2192 GitHubActionsLabLinkRole \u2192 Trust relationships</p>"},{"location":"aws-setup/#step-5-find-ami-ids-for-your-region","title":"Step 5: Find AMI IDs for Your Region","text":"<p>AMI IDs are region-specific. You'll need to find the correct Ubuntu 24.04 AMI IDs for your chosen region.</p>"},{"location":"aws-setup/#find-ubuntu-2404-amis","title":"Find Ubuntu 24.04 AMIs","text":""},{"location":"aws-setup/#aws-cli-method-recommended","title":"AWS CLI Method (Recommended)","text":"<p>For Allocator (Ubuntu 24.04 with Docker): <pre><code>aws ec2 describe-images \\\n  --region YOUR_REGION \\\n  --owners 099720109477 \\\n  --filters \"Name=name,Values=ubuntu/images/hvm-ssd-gp3/ubuntu-noble-24.04-amd64-server-*\" \\\n            \"Name=state,Values=available\" \\\n  --query 'sort_by(Images, &amp;CreationDate)[-1].[ImageId,Name,CreationDate]' \\\n  --output table\n</code></pre></p> <p>For Client VMs (Ubuntu 24.04 with Docker + NVIDIA): <pre><code># First, find latest Ubuntu 24.04\naws ec2 describe-images \\\n  --region YOUR_REGION \\\n  --owners 099720109477 \\\n  --filters \"Name=name,Values=ubuntu/images/hvm-ssd-gp3/ubuntu-noble-24.04-amd64-server-*\" \\\n            \"Name=state,Values=available\" \\\n  --query 'sort_by(Images, &amp;CreationDate)[-1].[ImageId,Name]' \\\n  --output table\n\n# Note: For GPU instances, you may need to use NVIDIA's Deep Learning AMI\n# or install NVIDIA drivers via user_data\n</code></pre></p>"},{"location":"aws-setup/#aws-console-method","title":"AWS Console Method","text":"<ol> <li>Go to EC2 \u2192 AMI Catalog in your chosen region</li> <li>Search for \"ubuntu 24.04\"</li> <li>Select \"AWS Marketplace AMIs\" or \"Community AMIs\"</li> <li>Filter by:</li> <li>Owner: Canonical (099720109477)</li> <li>Architecture: 64-bit (x86)</li> <li>Root device type: EBS</li> <li>Choose the most recent \"ubuntu-noble-24.04\" AMI</li> <li>Copy the AMI ID (e.g., <code>ami-0bd08c9d4aa9f0bc6</code>)</li> </ol>"},{"location":"aws-setup/#update-configuration-with-ami-ids","title":"Update Configuration with AMI IDs","text":"<p>Once you have the AMI IDs for your region, update <code>config/config.yaml</code>:</p> <pre><code># lablink-infrastructure/config/config.yaml\nmachine:\n  ami_id: \"ami-XXXXXXXXX\"  # Client VM AMI for your region\n  # ...\n\nallocator_instance:\n  ami_id: \"ami-YYYYYYYYY\"  # Allocator AMI for your region\n</code></pre>"},{"location":"aws-setup/#lablink-custom-amis-us-west-2-only","title":"LabLink Custom AMIs (us-west-2 only)","text":"<p>LabLink maintains custom AMIs with Docker and NVIDIA drivers pre-installed, only available in us-west-2:</p> <p>Client VM AMI (Ubuntu 24.04 + Docker + NVIDIA): - AMI ID: <code>ami-0601752c11b394251</code> - Description: Custom Ubuntu image with Docker and Nvidia GPU Driver pre-installed - Architecture: x86_64 - Source: Ubuntu Server 24.04 LTS (HVM), SSD Volume Type</p> <p>Allocator VM AMI (Ubuntu 24.04 + Docker): - AMI ID: <code>ami-0bd08c9d4aa9f0bc6</code> - Description: Custom Ubuntu image with Docker pre-installed - Architecture: x86_64 - Source: Ubuntu Server 24.04 LTS (HVM), SSD Volume Type</p>"},{"location":"aws-setup/#using-lablink-in-other-regions","title":"Using LabLink in Other Regions","text":"<p>If you're deploying to a region other than <code>us-west-2</code>, you have two options:</p> <p>Option 1: Copy Custom AMIs to Your Region (Recommended)</p> <p>Copy the LabLink custom AMIs to your preferred region:</p> <pre><code># Copy Client AMI from us-west-2\naws ec2 copy-image \\\n  --source-region us-west-2 \\\n  --source-image-id ami-0601752c11b394251 \\\n  --name \"lablink-client-ubuntu-24.04-docker-nvidia\" \\\n  --description \"Custom Ubuntu image with Docker and Nvidia GPU Driver\" \\\n  --region YOUR_REGION\n\n# Copy Allocator AMI from us-west-2\naws ec2 copy-image \\\n  --source-region us-west-2 \\\n  --source-image-id ami-0bd08c9d4aa9f0bc6 \\\n  --name \"lablink-allocator-ubuntu-24.04-docker\" \\\n  --description \"Custom Ubuntu image with Docker\" \\\n  --region YOUR_REGION\n</code></pre> <p>The copy process takes 10-30 minutes. Note the new AMI IDs from the output and update your <code>config/config.yaml</code>.</p> <p>Option 2: Use Standard Ubuntu 24.04 AMIs</p> <p>Use standard Ubuntu AMIs (Docker and NVIDIA drivers will be installed via user_data):</p> <pre><code># Find latest Ubuntu 24.04 in your region\naws ec2 describe-images \\\n  --region YOUR_REGION \\\n  --owners 099720109477 \\\n  --filters \"Name=name,Values=ubuntu/images/hvm-ssd-gp3/ubuntu-noble-24.04-amd64-server-*\" \\\n            \"Name=state,Values=available\" \\\n  --query 'sort_by(Images, &amp;CreationDate)[-1].[ImageId,Name]' \\\n  --output table\n</code></pre> <p>Note: Standard AMIs require modifying user_data scripts to install Docker and NVIDIA drivers on first boot, increasing deployment time by ~5-10 minutes.</p>"},{"location":"aws-setup/#step-6-security-groups-optional-pre-creation","title":"Step 6: Security Groups (Optional Pre-Creation)","text":"<p>Terraform creates security groups automatically, but you can pre-create them for more control.</p>"},{"location":"aws-setup/#allocator-security-group","title":"Allocator Security Group","text":"<pre><code># Create security group\nALLOCATOR_SG=$(aws ec2 create-security-group \\\n  --group-name lablink-allocator-sg \\\n  --description \"LabLink Allocator Security Group\" \\\n  --vpc-id vpc-xxxxx \\\n  --output text --query 'GroupId')\n\n# Allow HTTP (port 80)\naws ec2 authorize-security-group-ingress \\\n  --group-id $ALLOCATOR_SG \\\n  --protocol tcp \\\n  --port 80 \\\n  --cidr 0.0.0.0/0\n\n# Allow SSH (port 22)\naws ec2 authorize-security-group-ingress \\\n  --group-id $ALLOCATOR_SG \\\n  --protocol tcp \\\n  --port 22 \\\n  --cidr 0.0.0.0/0\n\n# Allow PostgreSQL from VPC (port 5432)\naws ec2 authorize-security-group-ingress \\\n  --group-id $ALLOCATOR_SG \\\n  --protocol tcp \\\n  --port 5432 \\\n  --source-group $ALLOCATOR_SG\n</code></pre>"},{"location":"aws-setup/#step-6-route-53-dns-optional","title":"Step 6: Route 53 DNS (Optional)","text":"<p>Set up custom domains for your allocators.</p>"},{"location":"aws-setup/#create-hosted-zone","title":"Create Hosted Zone","text":"<pre><code>aws route53 create-hosted-zone \\\n  --name lablink.yourdomain.com \\\n  --caller-reference $(date +%s)\n</code></pre> <p>Note the hosted zone ID from the output.</p>"},{"location":"aws-setup/#create-dns-records","title":"Create DNS Records","text":"<p>After deploying allocator, create A record:</p> <pre><code># Get allocator IP\nALLOCATOR_IP=$(terraform output -raw ec2_public_ip)\n\n# Create/update DNS record\naws route53 change-resource-record-sets \\\n  --hosted-zone-id Z1234567890ABC \\\n  --change-batch '{\n    \"Changes\": [{\n      \"Action\": \"UPSERT\",\n      \"ResourceRecordSet\": {\n        \"Name\": \"lablink-test.yourdomain.com\",\n        \"Type\": \"A\",\n        \"TTL\": 300,\n        \"ResourceRecords\": [{\"Value\": \"'$ALLOCATOR_IP'\"}]\n      }\n    }]\n  }'\n</code></pre>"},{"location":"aws-setup/#update-name-servers","title":"Update Name Servers","text":"<p>Update your domain registrar with the Route 53 name servers (from hosted zone output).</p>"},{"location":"aws-setup/#step-7-secrets-manager-optional","title":"Step 7: Secrets Manager (Optional)","text":"<p>Store sensitive configuration in AWS Secrets Manager instead of config files.</p>"},{"location":"aws-setup/#create-secrets","title":"Create Secrets","text":"<pre><code># Database password\naws secretsmanager create-secret \\\n  --name lablink/db-password \\\n  --secret-string \"your-secure-db-password\" \\\n  --region us-west-2\n\n# Admin password\naws secretsmanager create-secret \\\n  --name lablink/admin-password \\\n  --secret-string \"your-secure-admin-password\" \\\n  --region us-west-2\n</code></pre>"},{"location":"aws-setup/#retrieve-in-application","title":"Retrieve in Application","text":"<p>Modify your application code to fetch secrets:</p> <pre><code>import boto3\nfrom botocore.exceptions import ClientError\n\ndef get_secret(secret_name, region_name=\"us-west-2\"):\n    \"\"\"Retrieve secret from AWS Secrets Manager.\"\"\"\n    session = boto3.session.Session()\n    client = session.client(\n        service_name='secretsmanager',\n        region_name=region_name\n    )\n\n    try:\n        response = client.get_secret_value(SecretId=secret_name)\n        return response['SecretString']\n    except ClientError as e:\n        raise e\n\n# Usage\ndb_password = get_secret(\"lablink/db-password\")\nadmin_password = get_secret(\"lablink/admin-password\")\n</code></pre>"},{"location":"aws-setup/#step-8-cloudwatch-monitoring-optional","title":"Step 8: CloudWatch Monitoring (Optional)","text":"<p>Set up monitoring and alerts for your infrastructure.</p>"},{"location":"aws-setup/#enable-cloudwatch-logs","title":"Enable CloudWatch Logs","text":"<p>Update user data script to send logs to CloudWatch:</p> <pre><code>#!/bin/bash\n\n# Install CloudWatch agent\nwget https://s3.amazonaws.com/amazoncloudwatch-agent/ubuntu/amd64/latest/amazon-cloudwatch-agent.deb\ndpkg -i amazon-cloudwatch-agent.deb\n\n# Configure CloudWatch agent\ncat &gt; /opt/aws/amazon-cloudwatch-agent/etc/config.json &lt;&lt;EOF\n{\n  \"logs\": {\n    \"logs_collected\": {\n      \"files\": {\n        \"collect_list\": [\n          {\n            \"file_path\": \"/var/log/syslog\",\n            \"log_group_name\": \"/aws/ec2/lablink-allocator\",\n            \"log_stream_name\": \"{instance_id}/syslog\"\n          }\n        ]\n      }\n    }\n  }\n}\nEOF\n\n# Start agent\n/opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl \\\n  -a fetch-config -m ec2 -s -c file:/opt/aws/amazon-cloudwatch-agent/etc/config.json\n</code></pre>"},{"location":"aws-setup/#create-cloudwatch-alarms","title":"Create CloudWatch Alarms","text":"<pre><code># CPU utilization alarm\naws cloudwatch put-metric-alarm \\\n  --alarm-name lablink-allocator-cpu-high \\\n  --alarm-description \"Alert when CPU exceeds 80%\" \\\n  --metric-name CPUUtilization \\\n  --namespace AWS/EC2 \\\n  --statistic Average \\\n  --period 300 \\\n  --threshold 80 \\\n  --comparison-operator GreaterThanThreshold \\\n  --evaluation-periods 2 \\\n  --dimensions Name=InstanceId,Value=i-xxxxx\n</code></pre>"},{"location":"aws-setup/#step-9-billing-alerts","title":"Step 9: Billing Alerts","text":"<p>Set up cost monitoring to avoid unexpected charges.</p>"},{"location":"aws-setup/#enable-billing-alerts","title":"Enable Billing Alerts","text":"<pre><code>aws ce put-cost-anomaly-monitor \\\n  --anomaly-monitor file://billing-monitor.json\n</code></pre> <p><code>billing-monitor.json</code>: <pre><code>{\n  \"MonitorName\": \"LabLink Cost Monitor\",\n  \"MonitorType\": \"DIMENSIONAL\",\n  \"MonitorDimension\": \"SERVICE\"\n}\n</code></pre></p>"},{"location":"aws-setup/#create-budget","title":"Create Budget","text":"<pre><code>aws budgets create-budget \\\n  --account-id YOUR_ACCOUNT_ID \\\n  --budget file://budget.json \\\n  --notifications-with-subscribers file://subscribers.json\n</code></pre> <p><code>budget.json</code>: <pre><code>{\n  \"BudgetName\": \"LabLink Monthly Budget\",\n  \"BudgetLimit\": {\n    \"Amount\": \"100\",\n    \"Unit\": \"USD\"\n  },\n  \"TimeUnit\": \"MONTHLY\",\n  \"BudgetType\": \"COST\"\n}\n</code></pre></p> <p><code>subscribers.json</code>: <pre><code>[\n  {\n    \"Notification\": {\n      \"NotificationType\": \"ACTUAL\",\n      \"ComparisonOperator\": \"GREATER_THAN\",\n      \"Threshold\": 80\n    },\n    \"Subscribers\": [\n      {\n        \"SubscriptionType\": \"EMAIL\",\n        \"Address\": \"your-email@example.com\"\n      }\n    ]\n  }\n]\n</code></pre></p>"},{"location":"aws-setup/#verification-checklist","title":"Verification Checklist","text":"<p>After completing setup, verify:</p> <ul> <li>[ ] S3 bucket created with versioning and encryption</li> <li>[ ] Elastic IPs allocated for test and prod</li> <li>[ ] OIDC provider created</li> <li>[ ] IAM role for GitHub Actions configured</li> <li>[ ] GitHub workflow has correct role ARN</li> <li>[ ] (Optional) Route 53 hosted zone created</li> <li>[ ] (Optional) Secrets Manager secrets created</li> <li>[ ] (Optional) CloudWatch monitoring configured</li> <li>[ ] (Optional) Billing alerts set up</li> </ul>"},{"location":"aws-setup/#testing-your-setup","title":"Testing Your Setup","text":""},{"location":"aws-setup/#test-aws-cli-access","title":"Test AWS CLI Access","text":"<pre><code>aws sts get-caller-identity\naws s3 ls\naws ec2 describe-regions\n</code></pre>"},{"location":"aws-setup/#test-terraform","title":"Test Terraform","text":"<pre><code>cd lablink-allocator\nterraform init\nterraform validate\n</code></pre>"},{"location":"aws-setup/#test-github-actions","title":"Test GitHub Actions","text":"<p>Push a commit to trigger workflows:</p> <pre><code>git commit --allow-empty -m \"Test GitHub Actions\"\ngit push origin main\n</code></pre> <p>Check Actions tab in GitHub.</p>"},{"location":"aws-setup/#common-issues","title":"Common Issues","text":""},{"location":"aws-setup/#oidc-provider-already-exists","title":"OIDC Provider Already Exists","text":"<p>Error: <code>EntityAlreadyExists: Provider with URL ... already exists</code></p> <p>Solution: Use existing provider, just create new role</p>"},{"location":"aws-setup/#s3-bucket-name-taken","title":"S3 Bucket Name Taken","text":"<p>Error: <code>BucketAlreadyExists</code></p> <p>Solution: Choose a different bucket name (must be globally unique)</p>"},{"location":"aws-setup/#iam-permission-denied","title":"IAM Permission Denied","text":"<p>Error: <code>AccessDenied: User ... is not authorized to perform</code></p> <p>Solution: Ensure IAM user/role has required permissions</p>"},{"location":"aws-setup/#cost-estimation","title":"Cost Estimation","text":"<p>Estimated monthly costs for AWS resources:</p> Resource Usage Estimated Cost S3 Bucket &lt;1 GB, versioning $0.05/month Elastic IPs 2 IPs (test, prod) $0.00 (while associated) Route 53 Hosted Zone 1 zone $0.50/month Secrets Manager 2 secrets $0.80/month <p>Total AWS Setup Cost: ~$1.35/month</p> <p>Running EC2 instances cost extra. See Cost Estimation for details.</p>"},{"location":"aws-setup/#next-steps","title":"Next Steps","text":"<p>With AWS resources configured:</p> <ol> <li>Deployment: Deploy LabLink</li> <li>Security: Review security best practices</li> <li>Workflows: Understand CI/CD pipelines</li> </ol>"},{"location":"aws-setup/#cleanup","title":"Cleanup","text":"<p>To remove all AWS resources:</p> <pre><code># Delete S3 bucket (remove objects first)\naws s3 rm s3://$BUCKET_NAME --recursive\naws s3api delete-bucket --bucket $BUCKET_NAME\n\n# Release Elastic IPs\naws ec2 release-address --allocation-id eipalloc-test-xxxxx\naws ec2 release-address --allocation-id eipalloc-prod-xxxxx\n\n# Delete IAM role\naws iam delete-role-policy --role-name github-lablink-deploy --policy-name lablink-deploy-permissions\naws iam delete-role --role-name github-lablink-deploy\n\n# Delete OIDC provider\naws iam delete-open-id-connect-provider \\\n  --open-id-connect-provider-arn arn:aws:iam::YOUR_ACCOUNT_ID:oidc-provider/token.actions.githubusercontent.com\n</code></pre>"},{"location":"changelog-allocator/","title":"lablink-allocator-service Changelog","text":"<p>All notable changes to lablink-allocator-service will be documented here.</p> <p>The format is based on Keep a Changelog.</p>"},{"location":"changelog-allocator/#unreleased","title":"Unreleased","text":""},{"location":"changelog-allocator/#features","title":"Features","text":"<ul> <li>feat: Add config validation CLI for allocator service (#204)</li> </ul>"},{"location":"changelog-allocator/#other-changes","title":"Other Changes","text":"<ul> <li>Remove remaining infrastructure code migrated to lablink-template (#208)</li> </ul>"},{"location":"changelog-allocator/#003a2","title":"0.0.3a2","text":""},{"location":"changelog-allocator/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>fix: Sanitize URLs to remove prepended dots in allocator and client services (#199)</li> </ul>"},{"location":"changelog-allocator/#003a1","title":"0.0.3a1","text":""},{"location":"changelog-allocator/#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>fix: Add safety check for leading dots in hostname and bump version (#197)</li> <li>fix: Handle empty subdomains in get_allocator_url (#196)</li> <li>fix: update module path in Dockerfile to lablink_allocator_service (#194)</li> </ul>"},{"location":"changelog-allocator/#003a0","title":"0.0.3a0","text":""},{"location":"changelog-allocator/#other-changes_1","title":"Other Changes","text":"<ul> <li>Fix: Skip Terraform tests in publish workflow (#193)</li> <li>Restructure packages and create infrastructure template (#189)</li> </ul>"},{"location":"changelog-allocator/#002a0","title":"0.0.2a0","text":"<p>Initial release.</p> <p>For more details, see the GitHub Releases page.</p>"},{"location":"changelog-client/","title":"lablink-client-service Changelog","text":"<p>All notable changes to lablink-client-service will be documented here.</p> <p>The format is based on Keep a Changelog.</p>"},{"location":"changelog-client/#008a3","title":"0.0.8a3","text":""},{"location":"changelog-client/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>fix: Sanitize URLs to remove prepended dots in allocator and client services (#199)</li> </ul>"},{"location":"changelog-client/#008a2","title":"0.0.8a2","text":""},{"location":"changelog-client/#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>fix: Sanitize URLs to handle prepended dots in subscribe function (#198)</li> </ul>"},{"location":"changelog-client/#008a1","title":"0.0.8a1","text":""},{"location":"changelog-client/#chores","title":"Chores","text":"<ul> <li>chore: Update client package Python requirement and version (#195)</li> </ul>"},{"location":"changelog-client/#008a0","title":"0.0.8a0","text":""},{"location":"changelog-client/#other-changes","title":"Other Changes","text":"<ul> <li>Fix: Skip Terraform tests in publish workflow (#193)</li> <li>Restructure packages and create infrastructure template (#189)</li> </ul>"},{"location":"changelog-client/#007a0","title":"0.0.7a0","text":"<p>Initial release.</p> <p>For more details, see the GitHub Releases page.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>LabLink consists of two independently versioned packages:</p>"},{"location":"changelog/#package-changelogs","title":"Package Changelogs","text":"<ul> <li>lablink-allocator-service - VM Allocator Service</li> <li>lablink-client-service - Client Service</li> </ul> <p>For release notes and downloads, see the GitHub Releases page.</p>"},{"location":"configuration/","title":"Configuration","text":"<p>LabLink uses structured configuration files to customize behavior. This guide covers all configuration options and how to modify them.</p> <p>Infrastructure Repository</p> <p>Configuration files are located in the lablink-template repository under <code>lablink-infrastructure/config/config.yaml</code>. Clone the template repository to deploy LabLink infrastructure.</p>"},{"location":"configuration/#first-steps-change-default-passwords","title":"First Steps: Change Default Passwords","text":"<p>Critical Security Step</p> <p>Before deploying LabLink or creating any VMs, you MUST configure secure passwords!</p> <p>Configuration files use placeholder values that must be replaced with secure passwords: - Admin password placeholder: <code>PLACEHOLDER_ADMIN_PASSWORD</code> - Database password placeholder: <code>PLACEHOLDER_DB_PASSWORD</code></p>"},{"location":"configuration/#how-to-configure-passwords","title":"How to Configure Passwords","text":"<p>Method 1: GitHub Secrets (Recommended for CI/CD)</p> <p>For GitHub Actions deployments, add secrets to your repository:</p> <ol> <li>Go to repository Settings \u2192 Secrets and variables \u2192 Actions</li> <li>Click New repository secret</li> <li>Add <code>ADMIN_PASSWORD</code> with your secure admin password</li> <li>Add <code>DB_PASSWORD</code> with your secure database password</li> </ol> <p>The deployment workflow automatically replaces placeholders with these secret values before Terraform runs, preventing passwords from appearing in logs.</p> <p>Method 2: Manual Configuration</p> <p>For local deployments, edit the configuration file:</p> <pre><code># Edit allocator configuration\nvi lablink-infrastructure/config/config.yaml\n</code></pre> <p>Update these values: <pre><code>db:\n  password: \"YOUR_SECURE_DB_PASSWORD_HERE\"  # Replace PLACEHOLDER_DB_PASSWORD\n\napp:\n  admin_password: \"YOUR_SECURE_PASSWORD_HERE\"  # Replace PLACEHOLDER_ADMIN_PASSWORD\n</code></pre></p> <p>Method 3: Environment Variables</p> <pre><code>export ADMIN_PASSWORD=\"your_secure_password\"\nexport DB_PASSWORD=\"your_secure_db_password\"\n</code></pre> <p>Password requirements: - Minimum 12 characters - Mix of uppercase, lowercase, numbers, symbols - Not a dictionary word - Use a password manager to generate and store</p> <p>See Security \u2192 Change Default Passwords for detailed security guidance.</p>"},{"location":"configuration/#configuration-system","title":"Configuration System","text":"<p>LabLink uses Hydra for configuration management, which provides:</p> <ul> <li>Structured configs: Type-safe dataclass-based configuration</li> <li>Hierarchical composition: Override specific values</li> <li>Environment variables: Override via <code>ENV_VAR</code> syntax</li> <li>Command-line overrides: Pass config values as arguments</li> </ul>"},{"location":"configuration/#configuration-files","title":"Configuration Files","text":""},{"location":"configuration/#allocator-configuration","title":"Allocator Configuration","text":"<p>Location: <code>lablink-infrastructure/config/config.yaml</code></p> <pre><code>db:\n  dbname: \"lablink_db\"\n  user: \"lablink\"\n  password: \"PLACEHOLDER_DB_PASSWORD\"  # Injected from GitHub secret at deploy time\n  host: \"localhost\"\n  port: 5432\n  table_name: \"vms\"\n  message_channel: \"vm_updates\"\n\nmachine:\n  machine_type: \"g4dn.xlarge\"\n  image: \"ghcr.io/talmolab/lablink-client-base-image:linux-amd64-test\"\n  ami_id: \"ami-067cc81f948e50e06\"\n  repository: \"https://github.com/talmolab/sleap-tutorial-data.git\"\n  software: \"sleap\"\n\napp:\n  admin_user: \"admin\"\n  admin_password: \"PLACEHOLDER_ADMIN_PASSWORD\"  # Injected from GitHub secret at deploy time\n  region: \"us-west-2\"\n\nbucket_name: \"tf-state-lablink-allocator-bucket\"\n</code></pre>"},{"location":"configuration/#client-configuration","title":"Client Configuration","text":"<p>Location: <code>packages/client/src/lablink_client/conf/config.yaml</code></p> <pre><code>allocator:\n  host: \"localhost\"\n  port: 80\n\nclient:\n  software: \"sleap\"\n</code></pre>"},{"location":"configuration/#configuration-reference","title":"Configuration Reference","text":""},{"location":"configuration/#database-options-db","title":"Database Options (<code>db</code>)","text":"<p>Configuration for the PostgreSQL database.</p> Option Type Default Description <code>dbname</code> string <code>lablink_db</code> Database name <code>user</code> string <code>lablink</code> Database username <code>password</code> string <code>PLACEHOLDER_DB_PASSWORD</code> Database password (injected from GitHub secret) <code>host</code> string <code>localhost</code> Database host <code>port</code> int <code>5432</code> PostgreSQL port <code>table_name</code> string <code>vms</code> VM table name <code>message_channel</code> string <code>vm_updates</code> PostgreSQL NOTIFY channel <p>Production Security</p> <p>Configure <code>DB_PASSWORD</code> secret for GitHub Actions deployments, or manually replace the placeholder. See Security.</p>"},{"location":"configuration/#machine-options-machine","title":"Machine Options (<code>machine</code>)","text":"<p>Configuration for client VM specifications. These are the key options for adapting LabLink to your research software.</p> Option Type Default Description <code>machine_type</code> string <code>g4dn.xlarge</code> AWS EC2 instance type <code>image</code> string <code>ghcr.io/talmolab/lablink-client-base-image:latest</code> Docker image for client container <code>ami_id</code> string <code>ami-067cc81f948e50e06</code> Amazon Machine Image (Ubuntu 20.04 + Docker) <code>repository</code> string (optional) <code>https://github.com/talmolab/sleap-tutorial-data.git</code> Git repository to clone on VM <code>software</code> string <code>sleap</code> Software identifier (used by client)"},{"location":"configuration/#machine-type-options","title":"Machine Type Options","text":"<p>Common GPU instance types:</p> Instance Type GPU vCPUs Memory GPU Memory Use Case <code>g4dn.xlarge</code> NVIDIA T4 4 16 GB 16 GB Light workloads, testing <code>g4dn.2xlarge</code> NVIDIA T4 8 32 GB 16 GB Medium workloads <code>g5.xlarge</code> NVIDIA A10G 4 16 GB 24 GB Training, inference <code>g5.2xlarge</code> NVIDIA A10G 8 32 GB 24 GB Large models <code>p3.2xlarge</code> NVIDIA V100 8 61 GB 16 GB Deep learning training <p>See AWS Instance Types for complete list.</p>"},{"location":"configuration/#docker-image","title":"Docker Image","text":"<p>Default: <code>ghcr.io/talmolab/lablink-client-base-image:latest</code></p> <p>The Docker image determines what software runs on your VMs. Options:</p> <ol> <li>Use default SLEAP image (for SLEAP workflows)</li> <li>Build custom image (for your research software) - see Adapting LabLink</li> <li>Use different tag:</li> <li><code>:latest</code> - latest stable release</li> <li><code>:linux-amd64-test</code> - development version</li> <li><code>:v1.0.0</code> - specific version</li> </ol>"},{"location":"configuration/#ami-id","title":"AMI ID","text":"<p>Default: <code>ami-067cc81f948e50e06</code> (Ubuntu 20.04 + Docker in us-west-2)</p> <p>The Amazon Machine Image determines the OS and pre-installed software. You may need different AMIs for:</p> <ul> <li>Different AWS regions (AMI IDs are region-specific)</li> <li>Different OS versions</li> <li>Custom pre-configured images</li> </ul> <p>Find AMIs: <pre><code>aws ec2 describe-images \\\n  --owners 099720109477 \\\n  --filters \"Name=name,Values=ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*\" \\\n  --query 'Images[*].[ImageId,Name,CreationDate]' \\\n  --output table\n</code></pre></p>"},{"location":"configuration/#repository","title":"Repository","text":"<p>Default: <code>https://github.com/talmolab/sleap-tutorial-data.git</code></p> <p>Git repository to clone onto the client VM. Use this for:</p> <ul> <li>Custom analysis scripts</li> <li>Training data</li> <li>Configuration files</li> <li>Research code</li> </ul> <p>Set to empty string or omit if no repository needed: <pre><code>repository: \"\"\n</code></pre></p>"},{"location":"configuration/#software-identifier","title":"Software Identifier","text":"<p>Default: <code>sleap</code></p> <p>String identifier for the research software. Used by client service for software-specific logic.</p>"},{"location":"configuration/#application-options-app","title":"Application Options (<code>app</code>)","text":"<p>General application settings.</p> Option Type Default Description <code>admin_user</code> string <code>admin</code> Admin username for web UI <code>admin_password</code> string <code>PLACEHOLDER_ADMIN_PASSWORD</code> Admin password (injected from GitHub secret) <code>region</code> string <code>us-west-2</code> AWS region for deployments <p>Configure Passwords</p> <p>Configure <code>ADMIN_PASSWORD</code> secret for GitHub Actions deployments, or manually replace the placeholder. See Security.</p>"},{"location":"configuration/#allocator-options-allocator","title":"Allocator Options (<code>allocator</code>)","text":"<p>Client configuration for connecting to allocator.</p> Option Type Default Description <code>host</code> string <code>localhost</code> Allocator hostname or IP <code>port</code> int <code>80</code> Allocator port"},{"location":"configuration/#dns-options-dns","title":"DNS Options (<code>dns</code>)","text":"<p>Controls DNS configuration for allocator hostname.</p> Option Type Default Description <code>enabled</code> boolean <code>false</code> Enable DNS-based URLs <code>terraform_managed</code> boolean <code>false</code> Let Terraform manage Route 53 records <code>domain</code> string <code>\"\"</code> Your Route 53 hosted zone domain <code>zone_id</code> string <code>\"\"</code> Route 53 zone ID (optional, skips lookup if provided) <code>app_name</code> string <code>\"\"</code> Application name for auto pattern <code>pattern</code> string <code>\"auto\"</code> DNS pattern: <code>auto</code> or <code>custom</code> <code>custom_subdomain</code> string <code>\"\"</code> Custom subdomain for custom pattern <code>create_zone</code> boolean <code>false</code> Create new Route 53 zone <p>See DNS Configuration for detailed setup instructions.</p>"},{"location":"configuration/#eip-options-eip","title":"EIP Options (<code>eip</code>)","text":"<p>Controls Elastic IP allocation strategy.</p> Option Type Default Description <code>strategy</code> string <code>\"dynamic\"</code> <code>persistent</code> = reuse tagged EIP, <code>dynamic</code> = create new <code>tag_name</code> string <code>\"lablink-eip\"</code> Tag name for persistent EIP lookup"},{"location":"configuration/#ssltls-options-ssl","title":"SSL/TLS Options (<code>ssl</code>)","text":"<p>Controls HTTPS/SSL certificate management.</p> Option Type Default Description <code>provider</code> string <code>\"letsencrypt\"</code> SSL provider: <code>letsencrypt</code>, <code>cloudflare</code>, or <code>none</code> <code>email</code> string <code>\"\"</code> Email for Let's Encrypt notifications <code>staging</code> boolean <code>false</code> HTTP-only mode for testing (unlimited deployments)"},{"location":"configuration/#ssl-providers","title":"SSL Providers","text":"<p><code>letsencrypt</code> - Automatic SSL via Caddy + Let's Encrypt</p> <ul> <li>Staging mode (<code>staging: true</code>): HTTP only, unlimited deployments</li> <li>Production mode (<code>staging: false</code>): HTTPS with trusted certificates</li> </ul> <p><code>cloudflare</code> - CloudFlare proxy handles SSL</p> <ul> <li>Requires CloudFlare DNS configuration</li> <li>Not affected by <code>staging</code> setting</li> </ul> <p><code>none</code> - No SSL, HTTP only</p> <ul> <li>Similar to staging mode but explicit</li> </ul>"},{"location":"configuration/#staging-vs-production-mode","title":"Staging vs Production Mode","text":"<p>Staging Mode (<code>staging: true</code>)</p> <p>Use for testing and development:</p> <ul> <li>Serves HTTP only on port 80 (port 443 closed)</li> <li>Unlimited deployments per day</li> <li>No SSL certificate issuance delays</li> <li>No encryption - all traffic is plaintext</li> <li>Browser shows \"Not Secure\" warning</li> <li>May require clearing browser HSTS cache (see Troubleshooting)</li> </ul> <p>Configuration example: <pre><code>ssl:\n  provider: \"letsencrypt\"\n  email: \"admin@example.com\"\n  staging: true\n</code></pre></p> <p>Production Mode (<code>staging: false</code>)</p> <p>Use for production deployments:</p> <ul> <li>HTTPS with trusted Let's Encrypt certificates</li> <li>Browser shows secure padlock</li> <li>Full TLS 1.3 encryption</li> <li>Automatic HTTP \u2192 HTTPS redirects</li> <li>Rate limited (5 duplicate certificates per week)</li> <li>Certificate issuance takes 30-60 seconds</li> </ul> <p>Configuration example: <pre><code>ssl:\n  provider: \"letsencrypt\"\n  email: \"admin@example.com\"  # Receives cert expiry notifications\n  staging: false\n</code></pre></p>"},{"location":"configuration/#browser-access","title":"Browser Access","text":"<p>With staging mode:</p> <ol> <li>Type <code>http://</code> explicitly in address bar (e.g., <code>http://test.lablink.sleap.ai</code>)</li> <li>Clear HSTS cache if you previously accessed via HTTPS</li> <li>Expect \"Not Secure\" warning (this is normal)</li> </ol> <p>Alternatives: - Use incognito/private browsing - Access via IP: <code>http://&lt;allocator-ip&gt;</code> - Use curl: <code>curl http://test.lablink.sleap.ai</code></p> <p>With production mode:</p> <p>Access via <code>https://your-domain.com</code> - browser shows secure padlock.</p>"},{"location":"configuration/#lets-encrypt-rate-limits","title":"Let's Encrypt Rate Limits","text":"<p>Production mode is subject to Let's Encrypt limits:</p> <ul> <li>50 certificates per domain per week</li> <li>5 duplicate certificates per week (same hostnames)</li> <li>300 pending authorizations per account</li> </ul> <p>Use staging mode for frequent testing to avoid these limits.</p> <p>Warning: Staging mode serves unencrypted HTTP. Never use for production or sensitive data. See Security.</p>"},{"location":"configuration/#bucket-name","title":"Bucket Name","text":"<p>Option: <code>bucket_name</code> Default: <code>tf-state-lablink-allocator-bucket</code></p> <p>S3 bucket for Terraform state storage. Must be globally unique.</p>"},{"location":"configuration/#overriding-configuration","title":"Overriding Configuration","text":""},{"location":"configuration/#method-1-edit-yaml-files","title":"Method 1: Edit YAML Files","text":"<p>Directly modify the configuration files:</p> <pre><code>nano lablink-infrastructure/config/config.yaml\n</code></pre>"},{"location":"configuration/#method-2-environment-variables","title":"Method 2: Environment Variables","text":"<p>Override specific values without modifying files:</p> <pre><code>export DB_PASSWORD=my_secure_password\nexport ADMIN_PASSWORD=my_admin_password\nexport AWS_REGION=us-east-1\n</code></pre>"},{"location":"configuration/#method-3-hydra-command-line-overrides","title":"Method 3: Hydra Command-Line Overrides","text":"<p>When running Python directly:</p> <pre><code>python main.py db.password=my_password app.region=us-east-1\n</code></pre>"},{"location":"configuration/#method-4-docker-environment-variables","title":"Method 4: Docker Environment Variables","text":"<p>Pass environment variables to Docker containers:</p> <pre><code>docker run -d \\\n  -e DB_PASSWORD=secure_password \\\n  -e ADMIN_PASSWORD=admin_password \\\n  -e AWS_REGION=us-east-1 \\\n  -p 5000:5000 \\\n  ghcr.io/talmolab/lablink-allocator-image:latest\n</code></pre> <p>Config Validation and Custom Filenames</p> <p>The allocator supports <code>CONFIG_NAME</code> environment variable to override the config filename. However, the validation CLI (<code>lablink-validate-config</code>) requires the filename to be <code>config.yaml</code> to enable strict schema checking. If you override <code>CONFIG_NAME</code> to use a different filename, validation will not perform strict schema checks and unknown keys may not be caught until runtime.</p>"},{"location":"configuration/#method-5-terraform-variables","title":"Method 5: Terraform Variables","text":"<p>Override during infrastructure deployment:</p> <pre><code>terraform apply \\\n  -var=\"allocator_image_tag=v1.0.0\" \\\n  -var=\"resource_suffix=prod\"\n</code></pre>"},{"location":"configuration/#configuration-for-different-environments","title":"Configuration for Different Environments","text":""},{"location":"configuration/#development","title":"Development","text":"<p>Use Case: Local testing, experimentation</p> <pre><code>db:\n  password: \"simple_dev_password\"\n\nmachine:\n  machine_type: \"t2.micro\"  # Cheaper for testing\n  image: \"ghcr.io/talmolab/lablink-client-base-image:linux-amd64-test\"\n\napp:\n  region: \"us-west-2\"\n</code></pre>"},{"location":"configuration/#teststaging","title":"Test/Staging","text":"<p>Use Case: Pre-production validation</p> <pre><code>db:\n  password: \"${DB_PASSWORD}\"  # From environment variable\n\nmachine:\n  machine_type: \"g4dn.xlarge\"\n  image: \"ghcr.io/talmolab/lablink-client-base-image:linux-amd64-test\"\n\napp:\n  admin_password: \"${ADMIN_PASSWORD}\"\n  region: \"us-west-2\"\n</code></pre>"},{"location":"configuration/#production","title":"Production","text":"<p>Use Case: Production workloads</p> <pre><code>db:\n  password: \"${DB_PASSWORD}\"  # From Secrets Manager\n  host: \"lablink-db.xxxxx.us-west-2.rds.amazonaws.com\"  # RDS instance\n\nmachine:\n  machine_type: \"g5.2xlarge\"\n  image: \"ghcr.io/talmolab/lablink-client-base-image:v1.0.0\"  # Pinned version\n\napp:\n  admin_password: \"${ADMIN_PASSWORD}\"  # From Secrets Manager\n  region: \"us-west-2\"\n\nbucket_name: \"tf-state-lablink-prod\"\n</code></pre>"},{"location":"configuration/#validating-configuration","title":"Validating Configuration","text":"<p>After modifying configuration, validate it:</p>"},{"location":"configuration/#schema-validation-recommended","title":"Schema Validation (Recommended)","text":"<p>Use the built-in validation CLI to check your config against the schema:</p> <pre><code># Validate config file\nlablink-validate-config lablink-infrastructure/config/config.yaml\n\n# Output on success:\n# \u2713 Config validation passed\n\n# Output on error:\n# \u2717 Config validation failed: Error merging config with schema\n#   Unknown keys found: ['unknown_section']\n</code></pre> <p>The validator checks:</p> <ul> <li>File exists and is named <code>config.yaml</code></li> <li>All keys match the structured config schema</li> <li>Required fields are present</li> <li>Type mismatches (strings vs integers, etc.)</li> <li>Unknown configuration sections</li> </ul> <p>Important: The validator requires the filename to be <code>config.yaml</code> to enable Hydra's strict schema matching. Using a different filename will bypass schema validation.</p> <p>Usage in CI/CD:</p> <pre><code># Validate before deployment\nlablink-validate-config config/config.yaml &amp;&amp; terraform apply || exit 1\n</code></pre>"},{"location":"configuration/#check-syntax","title":"Check Syntax","text":"<pre><code># YAML syntax check\npython -c \"import yaml; yaml.safe_load(open('lablink-infrastructure/config/config.yaml'))\"\n</code></pre>"},{"location":"configuration/#test-locally","title":"Test Locally","text":"<pre><code># Run allocator with custom config\ncd packages/allocator\npython src/lablink_allocator_service/main.py\n</code></pre>"},{"location":"configuration/#terraform-validation","title":"Terraform Validation","text":"<pre><code>cd lablink-infrastructure\nterraform validate\nterraform plan  # Preview changes\n</code></pre>"},{"location":"configuration/#common-configuration-patterns","title":"Common Configuration Patterns","text":""},{"location":"configuration/#use-your-own-research-software","title":"Use Your Own Research Software","text":"<pre><code>machine:\n  machine_type: \"g4dn.2xlarge\"\n  image: \"ghcr.io/yourorg/your-research-image:latest\"\n  repository: \"https://github.com/yourorg/your-research-code.git\"\n  software: \"your-software-name\"\n</code></pre> <p>See Adapting LabLink for complete guide.</p>"},{"location":"configuration/#multiple-gpu-types","title":"Multiple GPU Types","text":"<p>Create environment-specific configs:</p> <p><code>config-cpu.yaml</code> (for testing): <pre><code>machine:\n  machine_type: \"t2.medium\"\n  ami_id: \"ami-0c55b159cbfafe1f0\"\n</code></pre></p> <p><code>config-gpu.yaml</code> (for production): <pre><code>machine:\n  machine_type: \"g5.xlarge\"\n  ami_id: \"ami-067cc81f948e50e06\"\n</code></pre></p> <p>Use with Hydra: <pre><code>python main.py --config-name=config-gpu\n</code></pre></p>"},{"location":"configuration/#custom-database","title":"Custom Database","text":"<p>Use external PostgreSQL (RDS):</p> <pre><code>db:\n  dbname: \"lablink_production\"\n  user: \"lablink_admin\"\n  password: \"${DB_PASSWORD}\"\n  host: \"lablink-db.cluster-xxxxx.us-west-2.rds.amazonaws.com\"\n  port: 5432\n</code></pre>"},{"location":"configuration/#configuration-best-practices","title":"Configuration Best Practices","text":"<ol> <li>Never commit secrets: Use environment variables or AWS Secrets Manager</li> <li>Pin versions in production: Use specific image tags, not <code>:latest</code></li> <li>Document custom values: Add comments explaining non-standard configurations</li> <li>Test configuration changes: Validate with <code>terraform plan</code> before applying</li> <li>Use separate configs per environment: Don't reuse dev configs in production</li> </ol>"},{"location":"configuration/#troubleshooting-configuration","title":"Troubleshooting Configuration","text":""},{"location":"configuration/#config-not-loading","title":"Config Not Loading","text":"<p>Check file location and syntax: <pre><code>python -c \"import yaml; print(yaml.safe_load(open('conf/config.yaml')))\"\n</code></pre></p>"},{"location":"configuration/#environment-variables-not-working","title":"Environment Variables Not Working","text":"<p>Verify export and check case sensitivity: <pre><code>env | grep -i lablink\necho $DB_PASSWORD\n</code></pre></p>"},{"location":"configuration/#terraform-variables-not-applied","title":"Terraform Variables Not Applied","text":"<p>Ensure <code>-var</code> flags are passed: <pre><code>terraform plan -var=\"resource_suffix=prod\"\n</code></pre></p>"},{"location":"configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Adapting LabLink: Customize for your research software</li> <li>Deployment: Deploy with your configuration</li> <li>Security: Secure your configuration values</li> </ul>"},{"location":"contributing-docs/","title":"Contributing to Documentation","text":"<p>This guide covers how to contribute to LabLink documentation.</p>"},{"location":"contributing-docs/#documentation-system","title":"Documentation System","text":"<p>LabLink uses MkDocs with the Material theme for documentation.</p> <p>Key Features: - Markdown-based documentation - Automatic API reference generation from Python docstrings - Automatic changelog generation from git history - Version management (multiple doc versions) - Search functionality - Dark/light mode</p>"},{"location":"contributing-docs/#quick-start","title":"Quick Start","text":""},{"location":"contributing-docs/#setup","title":"Setup","text":"<p>Option 1: Using uv (Recommended)</p> <pre><code># Clone repository\ngit clone https://github.com/talmolab/lablink.git\ncd lablink\n\n# Quick test (creates temporary environment automatically)\nuv run --extra docs mkdocs serve\n\n# Or create persistent virtual environment\nuv venv .venv-docs\n# Windows\n.venv-docs\\Scripts\\activate\n# macOS/Linux\nsource .venv-docs/bin/activate\n\n# Install dependencies\nuv sync --extra docs\n</code></pre> <p>Option 2: Using pip</p> <pre><code># Clone repository\ngit clone https://github.com/talmolab/lablink.git\ncd lablink\n\n# Create virtual environment\npython -m venv .venv-docs\n# Windows\n.venv-docs\\Scripts\\activate\n# macOS/Linux\nsource .venv-docs/bin/activate\n\n# Install documentation dependencies (from pyproject.toml)\npip install -e \".[docs]\"\n</code></pre>"},{"location":"contributing-docs/#build-and-preview","title":"Build and Preview","text":"<pre><code># Serve documentation locally\nmkdocs serve\n\n# Open http://localhost:8000 in browser\n</code></pre> <p>Changes to <code>.md</code> files will auto-reload in the browser.</p>"},{"location":"contributing-docs/#build-static-site","title":"Build Static Site","text":"<pre><code># Build documentation\nmkdocs build\n\n# Output in site/ directory\n</code></pre>"},{"location":"contributing-docs/#documentation-structure","title":"Documentation Structure","text":"<pre><code>lablink/\n\u251c\u2500\u2500 mkdocs.yml              # MkDocs configuration\n\u251c\u2500\u2500 pyproject.toml          # Python dependencies (docs extra)\n\u251c\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 index.md           # Homepage\n\u2502   \u251c\u2500\u2500 prerequisites.md   # Getting Started section\n\u2502   \u251c\u2500\u2500 quickstart.md\n\u2502   \u251c\u2500\u2500 installation.md\n\u2502   \u251c\u2500\u2500 architecture.md    # User Guides section\n\u2502   \u251c\u2500\u2500 configuration.md\n\u2502   \u251c\u2500\u2500 adapting.md\n\u2502   \u251c\u2500\u2500 deployment.md\n\u2502   \u251c\u2500\u2500 workflows.md\n\u2502   \u251c\u2500\u2500 ssh-access.md\n\u2502   \u251c\u2500\u2500 database.md\n\u2502   \u251c\u2500\u2500 testing.md\n\u2502   \u251c\u2500\u2500 aws-setup.md       # AWS Setup section\n\u2502   \u251c\u2500\u2500 security.md\n\u2502   \u251c\u2500\u2500 cost-estimation.md\n\u2502   \u251c\u2500\u2500 troubleshooting.md # Reference section\n\u2502   \u251c\u2500\u2500 faq.md\n\u2502   \u251c\u2500\u2500 contributing-docs.md\n\u2502   \u251c\u2500\u2500 scripts/\n\u2502   \u2502   \u251c\u2500\u2500 gen_ref_pages.py    # Auto-generates API docs\n\u2502   \u2502   \u2514\u2500\u2500 gen_changelog.py    # Auto-generates changelog\n\u2502   \u2514\u2500\u2500 assets/            # Images, diagrams, etc.\n\u2514\u2500\u2500 .github/workflows/\n    \u2514\u2500\u2500 docs.yml           # Documentation CI/CD\n</code></pre>"},{"location":"contributing-docs/#writing-documentation","title":"Writing Documentation","text":""},{"location":"contributing-docs/#markdown-basics","title":"Markdown Basics","text":"<pre><code># Page Title (H1)\n\n## Section (H2)\n\n### Subsection (H3)\n\n**Bold text**\n*Italic text*\n`inline code`\n\n[Link text](https://example.com)\n[Internal link](other-page.md)\n\n- Bullet list\n- Item 2\n\n1. Numbered list\n2. Item 2\n</code></pre>"},{"location":"contributing-docs/#code-blocks","title":"Code Blocks","text":"<p>Use fenced code blocks with language specification:</p> <pre><code>```python\ndef hello_world():\n    print(\"Hello, World!\")\n```\n\n```bash\nterraform apply -var=\"resource_suffix=dev\"\n```\n\n```yaml\ndb:\n  host: localhost\n  port: 5432\n```\n</code></pre>"},{"location":"contributing-docs/#admonitions","title":"Admonitions","text":"<p>Use admonitions for notes, warnings, tips:</p> <pre><code>!!! note\n    This is a note.\n\n!!! warning\n    This is a warning.\n\n!!! tip\n    This is a helpful tip.\n\n!!! danger\n    This is critical information.\n</code></pre> <p>Renders as:</p> <p>Note</p> <p>This is a note.</p> <p>Warning</p> <p>This is a warning.</p>"},{"location":"contributing-docs/#tabs","title":"Tabs","text":"<p>For multi-option content:</p> <pre><code>=== \"macOS\"\n    ```bash\n    brew install terraform\n    ```\n\n=== \"Linux\"\n    ```bash\n    wget https://releases.hashicorp.com/terraform/...\n    ```\n\n=== \"Windows\"\n    Download from terraform.io\n</code></pre> <p>Renders as:</p> macOSLinuxWindows <pre><code>brew install terraform\n</code></pre> <pre><code>wget https://releases.hashicorp.com/terraform/...\n</code></pre> <p>Download from terraform.io</p>"},{"location":"contributing-docs/#tables","title":"Tables","text":"<pre><code>| Column 1 | Column 2 | Column 3 |\n|----------|----------|----------|\n| Value 1  | Value 2  | Value 3  |\n| Value 4  | Value 5  | Value 6  |\n</code></pre>"},{"location":"contributing-docs/#internal-links","title":"Internal Links","text":"<pre><code>See [Configuration](configuration.md) for details.\n\nLink to specific section: [Configuration \u2192 Database](configuration.md#database-options-db)\n</code></pre>"},{"location":"contributing-docs/#images","title":"Images","text":"<pre><code>![Alt text](assets/diagram.png)\n\n# With caption\n&lt;figure markdown&gt;\n  ![Alt text](assets/diagram.png)\n  &lt;figcaption&gt;Caption text&lt;/figcaption&gt;\n&lt;/figure&gt;\n</code></pre>"},{"location":"contributing-docs/#documentation-guidelines","title":"Documentation Guidelines","text":""},{"location":"contributing-docs/#style-guide","title":"Style Guide","text":"<ol> <li>Be concise: Short sentences, clear language</li> <li>Use active voice: \"Run the command\" not \"The command should be run\"</li> <li>Include examples: Show don't just tell</li> <li>Test commands: Verify all bash commands work</li> <li>Update dates: Use current years in examples</li> <li>Cross-reference: Link to related pages</li> <li>Use consistent terminology: \"allocator\" not \"allocator server\" or \"allocation service\"</li> </ol>"},{"location":"contributing-docs/#page-structure","title":"Page Structure","text":"<p>Every documentation page should have:</p> <ol> <li>Title (H1): Page name</li> <li>Introduction: Brief overview (1-2 sentences)</li> <li>Main content: Organized with H2/H3 sections</li> <li>Examples: Code samples and use cases</li> <li>Related links: \"Next Steps\" or \"See Also\" section</li> </ol> <p>Example Template:</p> <pre><code># Page Title\n\nBrief introduction explaining what this page covers.\n\n## Main Section\n\nContent here.\n\n### Subsection\n\nMore detailed content.\n\n## Examples\n\nPractical examples.\n\n## Troubleshooting\n\nCommon issues.\n\n## Next Steps\n\n- [Related Page 1](page1.md)\n- [Related Page 2](page2.md)\n</code></pre>"},{"location":"contributing-docs/#code-examples","title":"Code Examples","text":"<p>Good: <pre><code># Comment explaining what this does\nterraform apply -var=\"resource_suffix=dev\"\n</code></pre></p> <p>Bad: <pre><code>terraform apply\n</code></pre></p> <p>Always: - Include comments - Show complete commands - Provide context - Test before documenting</p>"},{"location":"contributing-docs/#command-documentation","title":"Command Documentation","text":"<p>When documenting commands:</p> <ol> <li>Show the command</li> <li>Explain what it does</li> <li>Show expected output (if helpful)</li> <li>Mention common errors</li> </ol> <p>Example:</p> <pre><code>### Connect to Allocator\n\nSSH into the allocator instance:\n\n\\`\\`\\`bash\nssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\n\\`\\`\\`\n\n**Expected output**:\n\\`\\`\\`\nWelcome to Ubuntu 20.04.6 LTS\n...\nubuntu@ip-xxx-xx-xx-xx:~$\n\\`\\`\\`\n\n**Common errors**: See [Troubleshooting \u2192 SSH Issues](troubleshooting.md#ssh-access-issues)\n</code></pre>"},{"location":"contributing-docs/#api-documentation","title":"API Documentation","text":""},{"location":"contributing-docs/#python-docstrings","title":"Python Docstrings","text":"<p>API documentation is auto-generated from docstrings. Use Google-style docstrings:</p> <pre><code>def request_vm(email: str, crd_command: str) -&gt; dict:\n    \"\"\"Request a VM from the allocator.\n\n    Args:\n        email: User email address\n        crd_command: Command to execute on the VM\n\n    Returns:\n        Dictionary containing VM assignment details:\n        - hostname: VM hostname\n        - status: VM status\n        - assigned_at: Assignment timestamp\n\n    Raises:\n        ValueError: If email is invalid\n        RuntimeError: If no VMs available\n\n    Example:\n        &gt;&gt;&gt; result = request_vm(\"user@example.com\", \"python train.py\")\n        &gt;&gt;&gt; print(result['hostname'])\n        i-0abc123def456\n    \"\"\"\n    # Implementation\n</code></pre>"},{"location":"contributing-docs/#documenting-new-modules","title":"Documenting New Modules","text":"<p>When adding new Python modules:</p> <ol> <li>Add docstrings to all public functions/classes</li> <li>Run docs build to see generated API docs:    <pre><code>mkdocs serve\n# Navigate to Reference \u2192 API Reference\n</code></pre></li> <li>Verify documentation is clear and complete</li> </ol>"},{"location":"contributing-docs/#configuration-reference","title":"Configuration Reference","text":"<p>When documenting configuration options:</p> <ol> <li>Use tables for option lists</li> <li>Include:</li> <li>Option name</li> <li>Type</li> <li>Default value</li> <li>Description</li> <li>Provide examples</li> </ol> <p>Example:</p> <pre><code>### Database Options (`db`)\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `dbname` | string | `lablink_db` | Database name |\n| `user` | string | `lablink` | Database username |\n| `password` | string | `lablink` | Database password |\n\n**Example**:\n\\`\\`\\`yaml\ndb:\n  dbname: \"lablink_db\"\n  user: \"lablink\"\n  password: \"secure_password\"\n\\`\\`\\`\n</code></pre>"},{"location":"contributing-docs/#updating-navigation","title":"Updating Navigation","text":"<p>Navigation is defined in <code>mkdocs.yml</code>:</p> <pre><code>nav:\n  - Home: index.md\n  - Getting Started:\n      - Prerequisites: prerequisites.md\n      - Installation: installation.md\n  - User Guides:\n      - Configuration: configuration.md\n      - Deployment: deployment.md\n</code></pre> <p>When adding a new page:</p> <ol> <li>Create the <code>.md</code> file in <code>docs/</code></li> <li>Add entry to <code>nav</code> in <code>mkdocs.yml</code></li> <li>Build docs to verify</li> </ol>"},{"location":"contributing-docs/#adding-assets","title":"Adding Assets","text":""},{"location":"contributing-docs/#images_1","title":"Images","text":"<ol> <li>Place images in <code>docs/assets/</code></li> <li>Use descriptive names: <code>architecture-diagram.png</code></li> <li>Reference in markdown:    <pre><code>![Architecture Diagram](assets/architecture-diagram.png)\n</code></pre></li> </ol>"},{"location":"contributing-docs/#diagrams","title":"Diagrams","text":"<p>Use ASCII art for simple diagrams (portable):</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Allocator  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Client VM  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>For complex diagrams, use tools like: - draw.io - Mermaid (supported by MkDocs Material) - PlantUML</p> <p>Mermaid Example:</p> <pre><code>\\`\\`\\`mermaid\ngraph LR\n    A[User] --&gt; B[Allocator]\n    B --&gt; C[Client VM 1]\n    B --&gt; D[Client VM 2]\n\\`\\`\\`\n</code></pre>"},{"location":"contributing-docs/#versioning-documentation","title":"Versioning Documentation","text":"<p>Documentation is versioned using <code>mike</code>:</p>"},{"location":"contributing-docs/#version-names","title":"Version Names","text":"<ul> <li><code>latest</code>: Latest stable release</li> <li><code>v1.0.0</code>, <code>v1.1.0</code>, etc.: Specific versions</li> <li><code>dev</code>: Development/unreleased changes</li> </ul>"},{"location":"contributing-docs/#deploy-new-version","title":"Deploy New Version","text":"<pre><code># Deploy version 1.0.0 as latest\nmike deploy 1.0.0 latest --update-aliases\nmike set-default latest\n\n# Deploy dev version\nmike deploy dev\n\n# List versions\nmike list\n\n# Delete version\nmike delete v0.9.0\n</code></pre>"},{"location":"contributing-docs/#version-workflow","title":"Version Workflow","text":"<ol> <li>On main branch push: Deploy as <code>dev</code></li> <li>On release: Deploy as version number + <code>latest</code></li> </ol>"},{"location":"contributing-docs/#cicd-workflow","title":"CI/CD Workflow","text":"<p>Documentation is built and deployed via GitHub Actions (<code>.github/workflows/docs.yml</code>).</p> <p>Triggers: - Push to <code>main</code> \u2192 Deploy <code>dev</code> docs - Release published \u2192 Deploy versioned docs - Pull request \u2192 Build only (no deploy)</p> <p>Process: 1. Checkout code 2. Setup Python 3. Install dependencies 4. Run <code>mike deploy</code> (or <code>mkdocs build</code>) 5. Push to <code>gh-pages</code> branch</p>"},{"location":"contributing-docs/#testing-documentation","title":"Testing Documentation","text":""},{"location":"contributing-docs/#before-committing","title":"Before Committing","text":"<ol> <li> <p>Build locally:    <pre><code>mkdocs build --strict\n</code></pre> <code>--strict</code> treats warnings as errors</p> </li> <li> <p>Serve locally:    <pre><code>mkdocs serve\n</code></pre>    Review changes in browser</p> </li> <li> <p>Check links:</p> </li> <li>Click through all internal links</li> <li> <p>Verify external links work</p> </li> <li> <p>Test code examples:</p> </li> <li>Copy-paste commands and verify they work</li> <li>Test on clean environment if possible</li> </ol>"},{"location":"contributing-docs/#validation-checklist","title":"Validation Checklist","text":"<ul> <li>[ ] All links work (internal and external)</li> <li>[ ] Code blocks have language specified</li> <li>[ ] Commands tested and work</li> <li>[ ] Images display correctly</li> <li>[ ] Tables render properly</li> <li>[ ] No typos or grammar errors</li> <li>[ ] Follows style guide</li> <li>[ ] Cross-references added where relevant</li> </ul>"},{"location":"contributing-docs/#common-issues","title":"Common Issues","text":""},{"location":"contributing-docs/#link-not-working","title":"Link Not Working","text":"<p>Problem: Link shows 404</p> <p>Solution: - Use relative paths: <code>[Text](other-page.md)</code> not <code>[Text](/other-page.md)</code> - For sections: <code>[Text](page.md#section-heading)</code> - Check file exists in <code>docs/</code> directory</p>"},{"location":"contributing-docs/#code-block-not-highlighting","title":"Code Block Not Highlighting","text":"<p>Problem: Code block appears as plain text</p> <p>Solution: - Specify language: <code>```python</code> not just <code>```</code> - Check language name is correct: <code>bash</code> not <code>shell</code>, <code>yaml</code> not <code>yml</code></p>"},{"location":"contributing-docs/#admonition-not-rendering","title":"Admonition Not Rendering","text":"<p>Problem: Admonition shows as plain text</p> <p>Solution: <pre><code># Correct\n!!! note\n    Content indented with 4 spaces\n\n# Wrong\n!!! note\nContent not indented\n</code></pre></p>"},{"location":"contributing-docs/#table-not-aligning","title":"Table Not Aligning","text":"<p>Problem: Table cells misaligned</p> <p>Solution: - Ensure same number of columns in header and rows - Align pipes vertically (not required but helps) - Use markdown table formatter</p>"},{"location":"contributing-docs/#contributing-workflow","title":"Contributing Workflow","text":"<ol> <li> <p>Fork repository (if not a maintainer)</p> </li> <li> <p>Create branch:    <pre><code>git checkout -b docs/improve-configuration-page\n</code></pre></p> </li> <li> <p>Make changes:</p> </li> <li>Edit markdown files</li> <li>Add/update examples</li> <li> <p>Test locally with <code>mkdocs serve</code></p> </li> <li> <p>Commit:    <pre><code>git add docs/\ngit commit -m \"docs: improve configuration examples\"\n</code></pre></p> </li> <li> <p>Push:    <pre><code>git push origin docs/improve-configuration-page\n</code></pre></p> </li> <li> <p>Open Pull Request:</p> </li> <li>Clear title describing changes</li> <li>Description explaining what and why</li> <li> <p>Screenshots if visual changes</p> </li> <li> <p>Address feedback:</p> </li> <li>Respond to review comments</li> <li>Make requested changes</li> <li> <p>Push updates</p> </li> <li> <p>Merge:</p> </li> <li>Once approved, PR will be merged</li> <li>Documentation will auto-deploy</li> </ol>"},{"location":"contributing-docs/#getting-help","title":"Getting Help","text":"<ul> <li>Questions: Open GitHub issue with <code>documentation</code> label</li> <li>Suggestions: Open GitHub discussion</li> <li>Bugs in docs: Open GitHub issue</li> </ul>"},{"location":"contributing-docs/#resources","title":"Resources","text":"<ul> <li>MkDocs Documentation</li> <li>Material for MkDocs</li> <li>Python Markdown</li> <li>mkdocstrings</li> </ul>"},{"location":"contributing-docs/#quick-reference","title":"Quick Reference","text":"<pre><code># Install dependencies (uv)\nuv sync --extra docs\n\n# Install dependencies (pip)\npip install -e \".[docs]\"\n\n# Serve locally\nmkdocs serve\n\n# Build documentation\nmkdocs build --strict\n\n# Deploy version\nmike deploy 1.0.0 latest\n\n# View deployed versions\nmike list\n</code></pre>"},{"location":"contributing/","title":"Contributing to LabLink","text":"<p>Thank you for your interest in contributing to LabLink! This guide will help you get started.</p>"},{"location":"contributing/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Code of Conduct</li> <li>Getting Started</li> <li>Development Setup</li> <li>How to Contribute</li> <li>Contribution Workflow</li> <li>Coding Standards</li> <li>Testing</li> <li>Documentation</li> <li>Pull Request Process</li> <li>Getting Help</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>This project adheres to a code of conduct that we expect all contributors to follow. Please be respectful and constructive in all interactions.</p> <p>Expected Behavior: - Be respectful and inclusive - Welcome newcomers and help them learn - Focus on what is best for the community - Show empathy towards other community members</p> <p>Unacceptable Behavior: - Harassment, discrimination, or offensive comments - Trolling or insulting/derogatory comments - Public or private harassment - Publishing others' private information without permission</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":""},{"location":"contributing/#ways-to-contribute","title":"Ways to Contribute","text":"<ul> <li>\ud83d\udc1b Report bugs: Open an issue describing the problem</li> <li>\u2728 Suggest features: Open an issue with enhancement label</li> <li>\ud83d\udcdd Improve documentation: Fix typos, add examples, clarify instructions</li> <li>\ud83d\udd27 Fix bugs: Submit pull requests for open issues</li> <li>\ud83d\ude80 Add features: Implement new functionality</li> <li>\ud83e\uddea Write tests: Improve test coverage</li> <li>\ud83d\udcac Help others: Answer questions in issues and discussions</li> </ul>"},{"location":"contributing/#before-you-start","title":"Before You Start","text":"<ol> <li>Check existing issues: Someone may already be working on it</li> <li>Open an issue first: For major changes, discuss before implementing</li> <li>Read the docs: Familiarize yourself with LabLink architecture</li> <li>Review CLAUDE.md: Developer-focused project overview</li> </ol>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9 or higher</li> <li>Docker and Docker Desktop running</li> <li>AWS CLI (for testing infrastructure)</li> <li>Terraform 1.6.6+ (for testing infrastructure)</li> <li>Git</li> </ul>"},{"location":"contributing/#local-setup","title":"Local Setup","text":"<pre><code># Fork and clone the repository\ngit clone https://github.com/YOUR_USERNAME/lablink.git\ncd lablink\n\n# Install uv (recommended Python package manager)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Setup allocator service\ncd packages/allocator\nuv sync --extra dev\n\n# Setup client service\ncd ../client\nuv sync --extra dev\n\n# Return to root\ncd ../..\n</code></pre>"},{"location":"contributing/#verify-setup","title":"Verify Setup","text":"<pre><code># Run allocator tests\ncd packages/allocator\nPYTHONPATH=. pytest\n\n# Run client tests\ncd ../client\nPYTHONPATH=. pytest\n\n# Run linting\nruff check .\n\n# Build Docker images (development)\ndocker build -t lablink-allocator:dev -f packages/allocator/Dockerfile.dev .\ndocker build -t lablink-client:dev -f packages/client/Dockerfile.dev .\n</code></pre>"},{"location":"contributing/#how-to-contribute","title":"How to Contribute","text":""},{"location":"contributing/#reporting-bugs","title":"Reporting Bugs","text":"<p>Before reporting: 1. Check if the bug has already been reported 2. Try to reproduce on the latest version 3. Check the Troubleshooting Guide</p> <p>When reporting, include: - Clear, descriptive title - Steps to reproduce the issue - Expected behavior vs actual behavior - Error messages and logs - Environment details (OS, Python version, Docker version) - Screenshots (if applicable)</p> <p>Example bug report:</p> <p><pre><code>**Title**: PostgreSQL connection fails after deployment\n\n**Description**:\nAfter deploying the allocator to AWS, cannot connect to PostgreSQL database.\n\n**Steps to Reproduce**:\n1. Deploy allocator with `terraform apply`\n2. SSH into instance\n3. Try to access database: `psql -U lablink -d lablink_db`\n\n**Expected**: Successfully connect to database\n\n**Actual**: Connection refused error\n\n**Environment**:\n- OS: Ubuntu 20.04\n- Terraform: 1.6.6\n- Image tag: linux-amd64-latest\n\n**Logs**:\n</code></pre> [error logs here] <pre><code>\n</code></pre></p>"},{"location":"contributing/#suggesting-features","title":"Suggesting Features","text":"<p>Before suggesting: 1. Check if it's already suggested or implemented 2. Consider if it fits the project scope 3. Think about backwards compatibility</p> <p>When suggesting, include: - Clear, descriptive title - Use case and motivation - Proposed solution - Alternative solutions considered - Impact on existing functionality</p> <p>Example feature request:</p> <pre><code>**Title**: Add support for Azure cloud provider\n\n**Use Case**:\nSome research institutions use Azure instead of AWS and would benefit from LabLink.\n\n**Proposed Solution**:\n- Add Azure provider to Terraform configurations\n- Support Azure VMs alongside EC2\n- Document Azure-specific setup\n\n**Alternatives**:\n- Create separate fork for Azure\n- Use abstraction layer for multi-cloud support\n\n**Impact**:\n- Requires significant changes to infrastructure code\n- Need Azure-specific configuration options\n- May need separate documentation\n</code></pre>"},{"location":"contributing/#contribution-workflow","title":"Contribution Workflow","text":""},{"location":"contributing/#1-fork-the-repository","title":"1. Fork the Repository","text":"<p>Click \"Fork\" button on GitHub to create your copy.</p>"},{"location":"contributing/#2-create-a-branch","title":"2. Create a Branch","text":"<p>Use descriptive branch names:</p> <pre><code>git checkout -b feature/add-spot-instance-support\ngit checkout -b fix/postgresql-connection-issue\ngit checkout -b docs/improve-configuration-guide\n</code></pre> <p>Branch naming conventions: - <code>feature/</code> - New features - <code>fix/</code> - Bug fixes - <code>docs/</code> - Documentation changes - <code>refactor/</code> - Code refactoring - <code>test/</code> - Test additions or modifications</p>"},{"location":"contributing/#3-make-your-changes","title":"3. Make Your Changes","text":"<p>Follow the Coding Standards below.</p>"},{"location":"contributing/#4-test-your-changes","title":"4. Test Your Changes","text":"<pre><code># Run tests\nPYTHONPATH=. pytest\n\n# Run linting\nruff check .\n\n# Auto-fix linting issues\nruff check --fix .\n\n# Format code\nruff format .\n\n# Run type checking (if applicable)\nmypy .\n</code></pre>"},{"location":"contributing/#5-commit-your-changes","title":"5. Commit Your Changes","text":"<p>Use clear, descriptive commit messages following Conventional Commits:</p> <pre><code># Format: &lt;type&gt;(&lt;scope&gt;): &lt;description&gt;\n\ngit commit -m \"feat(allocator): add support for Spot Instances\"\ngit commit -m \"fix(database): resolve PostgreSQL connection timeout\"\ngit commit -m \"docs(security): add section on OIDC setup\"\ngit commit -m \"test(api): add tests for VM request endpoint\"\n</code></pre> <p>Commit types: - <code>feat</code> - New feature - <code>fix</code> - Bug fix - <code>docs</code> - Documentation changes - <code>style</code> - Code style changes (formatting, etc.) - <code>refactor</code> - Code refactoring - <code>test</code> - Adding or updating tests - <code>chore</code> - Maintenance tasks</p>"},{"location":"contributing/#6-push-to-your-fork","title":"6. Push to Your Fork","text":"<pre><code>git push origin feature/add-spot-instance-support\n</code></pre>"},{"location":"contributing/#7-open-a-pull-request","title":"7. Open a Pull Request","text":"<ol> <li>Go to the original repository</li> <li>Click \"New Pull Request\"</li> <li>Select your fork and branch</li> <li>Fill in the PR template (see below)</li> </ol>"},{"location":"contributing/#coding-standards","title":"Coding Standards","text":""},{"location":"contributing/#python-style","title":"Python Style","text":"<ul> <li>Follow PEP 8 style guide</li> <li>Use <code>ruff</code> for linting and formatting</li> <li>Maximum line length: 88 characters (Black default)</li> <li>Use type hints for function parameters and return values</li> <li>Write docstrings for all public functions and classes</li> </ul> <p>Example:</p> <pre><code>def request_vm(email: str, crd_command: str) -&gt; dict[str, str]:\n    \"\"\"Request a VM from the allocator.\n\n    Args:\n        email: User email address for VM assignment.\n        crd_command: Command to execute on the VM.\n\n    Returns:\n        Dictionary containing VM details:\n        - hostname: VM hostname\n        - status: Current VM status\n        - assigned_at: Assignment timestamp\n\n    Raises:\n        ValueError: If email format is invalid.\n        RuntimeError: If no VMs are available.\n\n    Example:\n        &gt;&gt;&gt; result = request_vm(\"user@example.com\", \"python train.py\")\n        &gt;&gt;&gt; print(result['hostname'])\n        i-0abc123def456\n    \"\"\"\n    if not validate_email(email):\n        raise ValueError(f\"Invalid email format: {email}\")\n\n    vm = get_available_vm()\n    if not vm:\n        raise RuntimeError(\"No VMs available\")\n\n    return assign_vm(vm, email, crd_command)\n</code></pre>"},{"location":"contributing/#terraform-style","title":"Terraform Style","text":"<ul> <li>Use descriptive resource names</li> <li>Add comments for complex logic</li> <li>Tag all resources with <code>Name</code>, <code>Project</code>, <code>Environment</code></li> <li>Use variables for configurable values</li> <li>Include outputs for important values</li> </ul> <p>Example:</p> <pre><code>resource \"aws_instance\" \"lablink_allocator\" {\n  ami           = var.ami_id\n  instance_type = var.instance_type\n\n  tags = {\n    Name        = \"lablink-allocator-${var.environment}\"\n    Project     = \"LabLink\"\n    Environment = var.environment\n    ManagedBy   = \"Terraform\"\n  }\n\n  # Security group allowing HTTP and SSH\n  vpc_security_group_ids = [aws_security_group.lablink.id]\n}\n</code></pre>"},{"location":"contributing/#documentation-style","title":"Documentation Style","text":"<ul> <li>Use clear, concise language</li> <li>Include code examples</li> <li>Test all commands before documenting</li> <li>Use consistent terminology</li> <li>Link to related documentation</li> </ul> <p>See Contributing to Documentation for detailed guidelines.</p>"},{"location":"contributing/#testing","title":"Testing","text":""},{"location":"contributing/#writing-tests","title":"Writing Tests","text":"<ul> <li>Write tests for all new functionality</li> <li>Maintain or improve code coverage</li> <li>Use descriptive test names</li> <li>Test both success and failure cases</li> <li>Mock external dependencies (AWS, database)</li> </ul> <p>Example test:</p> <pre><code>import pytest\nfrom unittest.mock import MagicMock, patch\n\ndef test_request_vm_success():\n    \"\"\"Test successful VM request.\"\"\"\n    mock_db = MagicMock()\n    mock_db.get_available_vm.return_value = {\n        'hostname': 'i-12345',\n        'status': 'available'\n    }\n\n    result = request_vm(\"user@example.com\", \"echo test\", db=mock_db)\n\n    assert result['hostname'] == 'i-12345'\n    assert result['status'] == 'in-use'\n    mock_db.update_vm_status.assert_called_once()\n\ndef test_request_vm_no_vms_available():\n    \"\"\"Test VM request when no VMs available.\"\"\"\n    mock_db = MagicMock()\n    mock_db.get_available_vm.return_value = None\n\n    with pytest.raises(RuntimeError, match=\"No VMs available\"):\n        request_vm(\"user@example.com\", \"echo test\", db=mock_db)\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\nPYTHONPATH=. pytest\n\n# Run specific test file\nPYTHONPATH=. pytest tests/test_api_calls.py\n\n# Run specific test\nPYTHONPATH=. pytest tests/test_api_calls.py::test_request_vm_success\n\n# Run with coverage\nPYTHONPATH=. pytest --cov=lablink_allocator --cov-report=html\n\n# View coverage report\nopen htmlcov/index.html  # macOS\nxdg-open htmlcov/index.html  # Linux\nstart htmlcov/index.html  # Windows\n</code></pre>"},{"location":"contributing/#test-requirements","title":"Test Requirements","text":"<ul> <li>All new features must have tests</li> <li>Bug fixes should include regression tests</li> <li>Tests must pass in CI before merging</li> <li>Aim for &gt;80% code coverage</li> </ul>"},{"location":"contributing/#documentation","title":"Documentation","text":""},{"location":"contributing/#updating-documentation","title":"Updating Documentation","text":"<p>When making changes that affect users:</p> <ol> <li>Update relevant docs in <code>docs/</code> directory</li> <li>Update docstrings for API changes</li> <li>Update README.md if adding major features</li> <li>Update CLAUDE.md if changing architecture</li> <li>Test documentation locally with <code>mkdocs serve</code></li> </ol>"},{"location":"contributing/#documentation-checklist","title":"Documentation Checklist","text":"<ul> <li>[ ] Docstrings updated for changed functions</li> <li>[ ] Relevant documentation pages updated</li> <li>[ ] Examples provided for new features</li> <li>[ ] Links to related documentation added</li> <li>[ ] Tested all commands/examples</li> <li>[ ] Screenshots added (if UI changes)</li> </ul>"},{"location":"contributing/#building-documentation-locally","title":"Building Documentation Locally","text":"<p>Using uv (Recommended):</p> <pre><code># Quick test (creates temporary environment automatically)\nuv run --extra docs mkdocs serve\n\n# Or create persistent virtual environment\nuv venv .venv-docs\n# Windows\n.venv-docs\\Scripts\\activate\n# macOS/Linux\nsource .venv-docs/bin/activate\n\n# Install dependencies\nuv sync --extra docs\n\n# Serve documentation\nmkdocs serve\n\n# Open http://localhost:8000\n\n# Build static site\nmkdocs build\n</code></pre> <p>Using pip:</p> <pre><code># Install docs dependencies\npip install -e \".[docs]\"\n\n# Serve documentation\nmkdocs serve\n\n# Open http://localhost:8000\n\n# Build static site\nmkdocs build\n</code></pre>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":""},{"location":"contributing/#pr-template","title":"PR Template","text":"<p>When opening a PR, include:</p> <pre><code>## Description\nBrief description of changes\n\n## Type of Change\n- [ ] Bug fix (non-breaking change fixing an issue)\n- [ ] New feature (non-breaking change adding functionality)\n- [ ] Breaking change (fix or feature causing existing functionality to change)\n- [ ] Documentation update\n\n## Related Issue\nFixes #(issue number)\n\n## Changes Made\n- Bullet list of changes\n- Be specific\n\n## Testing\n- [ ] Tests added/updated\n- [ ] All tests pass locally\n- [ ] Documentation updated\n\n## Screenshots (if applicable)\nAdd screenshots for UI changes\n\n## Checklist\n- [ ] Code follows project style guidelines\n- [ ] Self-review completed\n- [ ] Comments added for complex code\n- [ ] Documentation updated\n- [ ] No new warnings generated\n- [ ] Tests added and passing\n- [ ] Branch is up to date with main\n</code></pre>"},{"location":"contributing/#pr-review-process","title":"PR Review Process","text":"<ol> <li>Automated checks run (CI tests, linting)</li> <li>Maintainer review for code quality and design</li> <li>Address feedback by pushing new commits</li> <li>Approval from at least one maintainer</li> <li>Merge by maintainer (squash merge preferred)</li> </ol>"},{"location":"contributing/#pr-guidelines","title":"PR Guidelines","text":"<p>Do: - Keep PRs focused on a single concern - Write clear PR descriptions - Respond to feedback promptly - Keep PRs up to date with main branch - Be respectful of reviewers' time</p> <p>Don't: - Mix multiple unrelated changes - Submit huge PRs (&gt;500 lines if possible) - Make breaking changes without discussion - Merge without approval - Force push after review starts</p>"},{"location":"contributing/#release-process","title":"Release Process","text":"<p>LabLink uses independent versioning for its two packages. Maintainers follow this process for releases:</p>"},{"location":"contributing/#package-versions","title":"Package Versions","text":"<ul> <li>lablink-allocator-service: VM Allocator Service</li> <li>lablink-client-service: Client Service</li> </ul> <p>Each package is versioned and released independently following Semantic Versioning.</p>"},{"location":"contributing/#release-workflow","title":"Release Workflow","text":""},{"location":"contributing/#1-prepare-the-release","title":"1. Prepare the Release","text":"<pre><code># Update version in pyproject.toml\ncd packages/allocator  # or packages/client\n# Edit pyproject.toml: version = \"0.3.0\"\n\n# Commit the version bump\ngit add pyproject.toml\ngit commit -m \"chore: bump lablink-allocator-service to 0.3.0\"\ngit push origin main\n</code></pre>"},{"location":"contributing/#2-test-with-dry-run-recommended","title":"2. Test with Dry Run (Recommended)","text":"<p>Before creating a release, test the build process:</p> <pre><code># For allocator\ngh workflow run \"Publish Python Packages\" \\\n  -f package=lablink-allocator-service \\\n  -f dry_run=true\n\n# For client\ngh workflow run \"Publish Python Packages\" \\\n  -f package=lablink-client-service \\\n  -f dry_run=true\n\n# Monitor the workflow\ngh run watch\n</code></pre> <p>The dry run will verify: - \u2705 Package metadata is correct - \u2705 Linting passes - \u2705 Tests pass - \u2705 Package builds successfully</p>"},{"location":"contributing/#3-create-github-release","title":"3. Create GitHub Release","text":"<p>Once the dry run passes, create the release:</p> <pre><code># For lablink-allocator-service\ngh release create lablink-allocator-service_v0.3.0 \\\n  --title \"lablink-allocator-service v0.3.0\" \\\n  --notes \"## Changes\n\n### Features\n- New feature X (#123)\n- Enhancement Y (#124)\n\n### Bug Fixes\n- Fixed issue Z (#125)\n\n### Documentation\n- Updated configuration docs\n\n## Installation\n\\`\\`\\`bash\npip install lablink-allocator-service==0.3.0\n\\`\\`\\`\n\"\n\n# For lablink-client-service\ngh release create lablink-client-service_v0.1.5 \\\n  --title \"lablink-client-service v0.1.5\" \\\n  --notes \"## Changes\n\n### Features\n- New feature A (#130)\n\n### Bug Fixes\n- Fixed bug B (#131)\n\n## Installation\n\\`\\`\\`bash\npip install lablink-client-service==0.1.5\n\\`\\`\\`\n\"\n</code></pre>"},{"location":"contributing/#4-automated-publishing","title":"4. Automated Publishing","text":"<p>When you create the GitHub Release, the <code>Publish Python Packages</code> workflow automatically:</p> <ol> <li>Verifies the release is from the <code>main</code> branch</li> <li>Checks the tag version matches <code>pyproject.toml</code></li> <li>Validates package metadata</li> <li>Runs linting checks</li> <li>Executes test suite</li> <li>Builds the package</li> <li>Publishes to PyPI using OIDC (no API token needed)</li> <li>Displays Docker build command for creating production images</li> </ol>"},{"location":"contributing/#5-build-production-docker-images","title":"5. Build Production Docker Images","text":"<p>After publishing to PyPI, manually trigger Docker image builds to create production images with version tags.</p> <p>Using GitHub CLI: <pre><code># Build both images with their respective versions\ngh workflow run lablink-images.yml \\\n  -f environment=prod \\\n  -f allocator_version=0.3.0 \\\n  -f client_version=0.1.5\n\n# Monitor the build\ngh run watch\n</code></pre></p> <p>Using GitHub UI: 1. Go to Actions \u2192 Build and Push Docker Images 2. Click \"Run workflow\" 3. Fill in:    - Branch: <code>main</code>    - Environment: <code>prod</code>    - Allocator version: <code>0.3.0</code>    - Client version: <code>0.1.5</code> 4. Click \"Run workflow\"</p> <p>This creates Docker images tagged with: - <code>ghcr.io/talmolab/lablink-allocator-image:0.3.0</code> (version tag) - <code>ghcr.io/talmolab/lablink-client-base-image:0.1.5</code> (version tag) - <code>latest</code> tags for both images - Plus platform-specific and metadata tags</p> <p>See Image Tagging Strategy for complete tag details.</p>"},{"location":"contributing/#release-guardrails","title":"Release Guardrails","text":"<p>The workflow includes several safety checks:</p> Check Purpose Failure Action Branch verification Ensures releases only from <code>main</code> Blocks publish Version match Tag must equal <code>pyproject.toml</code> Blocks publish Metadata validation All required fields present Blocks publish Linting Code quality standards Blocks publish Test suite Functionality verification Blocks publish Build verification Package builds successfully Blocks publish"},{"location":"contributing/#tag-naming-convention","title":"Tag Naming Convention","text":"<p>Format: <code>&lt;package-name&gt;_v&lt;version&gt;</code></p> <p>Examples: - <code>lablink-allocator-service_v0.3.0</code> - <code>lablink-allocator-service_v1.0.0-rc1</code> - <code>lablink-client-service_v0.1.5</code> - <code>lablink-client-service_v0.2.0-beta1</code></p>"},{"location":"contributing/#versioning-guidelines","title":"Versioning Guidelines","text":"<p>Follow Semantic Versioning:</p> <ul> <li>MAJOR (1.0.0): Breaking changes</li> <li>MINOR (0.1.0): New features, backwards compatible</li> <li>PATCH (0.0.1): Bug fixes, backwards compatible</li> </ul> <p>Pre-release identifiers: - <code>0.3.0-alpha1</code>: Alpha release - <code>0.3.0-beta1</code>: Beta release - <code>0.3.0-rc1</code>: Release candidate</p>"},{"location":"contributing/#post-release","title":"Post-Release","text":"<p>After publishing and building Docker images:</p> <ol> <li> <p>Verify on PyPI: Check package appears on PyPI <pre><code>pip install lablink-allocator-service==0.3.0\n</code></pre></p> </li> <li> <p>Verify Docker images: Check images on GHCR    <pre><code>docker pull ghcr.io/talmolab/lablink-allocator-image:0.3.0\ndocker pull ghcr.io/talmolab/lablink-client-base-image:0.1.5\n</code></pre></p> </li> <li> <p>Test installation: Install and verify the package works    <pre><code>pip install lablink-allocator-service==0.3.0\npython -c \"from lablink_allocator.main import main; print('OK')\"\n</code></pre></p> </li> <li> <p>Check documentation: Verify docs at https://talmolab.github.io/lablink/</p> </li> <li> <p>Announce: Post release announcement (if major version)</p> </li> </ol>"},{"location":"contributing/#troubleshooting-releases","title":"Troubleshooting Releases","text":"<p>Version mismatch error: <pre><code># Ensure pyproject.toml version matches tag\ngrep '^version = ' packages/allocator/pyproject.toml\n# Should output: version = \"0.3.0\"\n</code></pre></p> <p>Tests failing: <pre><code># Run tests locally first\ncd packages/allocator\nuv sync --extra dev\nsource .venv/bin/activate  # or .venv\\Scripts\\activate on Windows\nuv run pytest tests\n</code></pre></p> <p>Build failing: <pre><code># Test build locally\ncd packages/allocator\nuv build\nls -lh dist/\n</code></pre></p>"},{"location":"contributing/#rolling-back-a-release","title":"Rolling Back a Release","text":"<p>If a release has issues:</p> <ol> <li>Delete the GitHub Release (does not delete the tag)</li> <li>Delete the tag: <code>gh release delete lablink-allocator-service_v0.3.0 --yes</code></li> <li>Delete from PyPI: Contact PyPI support (cannot delete via API)</li> <li>Fix the issue and release as a new patch version (e.g., 0.3.1)</li> </ol> <p>Note: PyPI does not allow re-uploading the same version. Always increment the version number.</p>"},{"location":"contributing/#getting-help","title":"Getting Help","text":""},{"location":"contributing/#resources","title":"Resources","text":"<ul> <li>\ud83d\udcd6 Documentation: https://talmolab.github.io/lablink/</li> <li>\ud83d\udc1b Issues: https://github.com/talmolab/lablink/issues</li> <li>\ud83d\udcac Discussions: https://github.com/talmolab/lablink/discussions</li> <li>\ud83d\udce7 Developer Guide: CLAUDE.md</li> </ul>"},{"location":"contributing/#questions","title":"Questions?","text":"<ul> <li>Check FAQ</li> <li>Search existing issues</li> <li>Open a new discussion</li> <li>Open an issue if bug/feature</li> </ul>"},{"location":"contributing/#communication","title":"Communication","text":"<ul> <li>Be patient: Maintainers are volunteers</li> <li>Be clear: Provide context and details</li> <li>Be respectful: Follow code of conduct</li> <li>Be helpful: Help others when you can</li> </ul>"},{"location":"contributing/#recognition","title":"Recognition","text":"<p>Contributors are recognized in: - Git commit history - GitHub contributors page - Release notes (for significant contributions)</p>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the same BSD-3-Clause License that covers the project.</p>"},{"location":"contributing/#thank-you","title":"Thank You!","text":"<p>Your contributions make LabLink better for the research community. We appreciate your time and effort! \ud83c\udf89</p> <p>Questions about contributing? Open a discussion or reach out in an issue.</p>"},{"location":"cost-estimation/","title":"Cost Estimation","text":"<p>This guide helps you understand and estimate AWS costs for running LabLink.</p>"},{"location":"cost-estimation/#cost-overview","title":"Cost Overview","text":"<p>LabLink costs consist of:</p> <ol> <li>Infrastructure costs (one-time or monthly)</li> <li>Compute costs (per hour, based on usage)</li> <li>Storage costs (monthly)</li> <li>Data transfer costs (per GB)</li> </ol>"},{"location":"cost-estimation/#aws-pricing-calculator","title":"AWS Pricing Calculator","text":"<p>For exact pricing, use the AWS Pricing Calculator.</p> <p>Note</p> <p>Prices shown are for us-west-2 region as of January 2025. Check current AWS pricing for your region.</p>"},{"location":"cost-estimation/#infrastructure-costs-minimal","title":"Infrastructure Costs (Minimal)","text":""},{"location":"cost-estimation/#s3-bucket-terraform-state","title":"S3 Bucket (Terraform State)","text":"<p>Purpose: Store Terraform state files</p> Item Usage Monthly Cost Storage &lt; 1 GB $0.02 Requests ~ 100/month $0.01 Versioning Enabled Included <p>Estimated Monthly Cost: $0.05</p>"},{"location":"cost-estimation/#elastic-ips","title":"Elastic IPs","text":"<p>Purpose: Static IP addresses for allocators</p> Item Quantity Monthly Cost Elastic IP (associated) 2 (test, prod) $0.00 Elastic IP (unassociated) 0 $0.00 <p>Warning</p> <p>Unassociated Elastic IPs cost $0.005/hour ($3.60/month). Always associate or release unused IPs.</p> <p>Estimated Monthly Cost: $0.00 (when associated)</p>"},{"location":"cost-estimation/#route-53-optional","title":"Route 53 (Optional)","text":"<p>Purpose: DNS management for custom domains</p> Item Quantity Monthly Cost Hosted Zone 1 $0.50 Queries 1M $0.40 <p>Estimated Monthly Cost: $0.90</p>"},{"location":"cost-estimation/#total-infrastructure-cost","title":"Total Infrastructure Cost","text":"<p>Without Route 53: ~$0.05/month With Route 53: ~$0.95/month</p>"},{"location":"cost-estimation/#compute-costs-variable","title":"Compute Costs (Variable)","text":""},{"location":"cost-estimation/#allocator-instance","title":"Allocator Instance","text":"<p>Costs for running the allocator EC2 instance.</p>"},{"location":"cost-estimation/#instance-type-options","title":"Instance Type Options","text":"Instance Type vCPUs RAM Price (On-Demand) Monthly (24/7) t2.micro 1 1 GB $0.0116/hour $8.50 t2.small 1 2 GB $0.023/hour $16.79 t2.medium 2 4 GB $0.0464/hour $33.87 t3.micro 2 1 GB $0.0104/hour $7.59 t3.small 2 2 GB $0.0208/hour $15.18 <p>Recommended: t2.micro for dev/test, t2.small for production</p> <p>Estimated Monthly Cost: $8.50 - $17 (if running 24/7)</p>"},{"location":"cost-estimation/#cost-optimization","title":"Cost Optimization","text":"<p>Option 1: Terminate When Not Needed - Stop allocator during off-hours - Cost: Only when running - Example: 8 hours/day \u00d7 20 days = 160 hours = $1.86/month (t2.micro)</p> <p>Option 2: Reserved Instances (1-year commitment) - Up to 75% savings - t2.micro: $5.03/month (vs $8.50)</p> <p>Option 3: Savings Plans - Flexible commitment - Similar savings to Reserved Instances</p>"},{"location":"cost-estimation/#client-vm-instances","title":"Client VM Instances","text":"<p>Costs for running research workload VMs.</p>"},{"location":"cost-estimation/#gpu-instance-types","title":"GPU Instance Types","text":"Instance Type GPU vCPUs RAM GPU Memory Price/Hour Monthly (24/7) g4dn.xlarge T4 4 16 GB 16 GB $0.526 $384 g4dn.2xlarge T4 8 32 GB 16 GB $0.752 $549 g4dn.4xlarge T4 16 64 GB 16 GB $1.204 $879 g5.xlarge A10G 4 16 GB 24 GB $1.006 $735 g5.2xlarge A10G 8 32 GB 24 GB $1.212 $885 p3.2xlarge V100 8 61 GB 16 GB $3.06 $2,234 <p>Most Common: g4dn.xlarge (good balance of performance and cost)</p>"},{"location":"cost-estimation/#cost-optimization-strategies","title":"Cost Optimization Strategies","text":"<p>Option 1: Spot Instances (Up to 90% savings)</p> <pre><code># terraform/main.tf\nresource \"aws_instance\" \"client\" {\n  instance_market_options {\n    market_type = \"spot\"\n    spot_options {\n      max_price = \"0.20\"  # Max price you're willing to pay\n    }\n  }\n}\n</code></pre> <ul> <li>g4dn.xlarge Spot: ~$0.158/hour (vs $0.526 on-demand)</li> <li>Savings: 70%</li> <li>Risk: Can be terminated if capacity needed</li> </ul> <p>Option 2: Terminate After Use - Only run VMs when actively working - Cost: Per-hour usage only</p> <p>Example: 10 VMs \u00d7 8 hours = 80 hours \u00d7 $0.526 = $42.08</p> <p>Option 3: Right-Size Instance Types - Use smallest instance that meets requirements - Test on smaller instances first</p>"},{"location":"cost-estimation/#cpu-only-instance-types-non-gpu","title":"CPU-Only Instance Types (Non-GPU)","text":"<p>For non-GPU workloads:</p> Instance Type vCPUs RAM Price/Hour Monthly (24/7) c5.xlarge 4 8 GB $0.17 $124 c5.2xlarge 8 16 GB $0.34 $248 c6i.xlarge 4 8 GB $0.17 $124 <p>Use Case: Data processing without GPU requirements</p>"},{"location":"cost-estimation/#storage-costs","title":"Storage Costs","text":""},{"location":"cost-estimation/#ebs-volumes-ec2-storage","title":"EBS Volumes (EC2 Storage)","text":"Volume Type Allocator Client VM Price/GB-Month gp3 30 GB 100 GB $0.08 <p>Allocator: 30 GB \u00d7 $0.08 = $2.40/month Client VM: 100 GB \u00d7 $0.08 = $8.00/month per VM</p> <p>Note: EBS charges apply even for stopped instances. Terminate to avoid charges.</p>"},{"location":"cost-estimation/#s3-storage-backups","title":"S3 Storage (Backups)","text":"Item Usage Monthly Cost Standard Storage 10 GB $0.23 Glacier (Archive) 100 GB $0.40 <p>Use Case: Database backups, logs, artifacts</p>"},{"location":"cost-estimation/#data-transfer-costs","title":"Data Transfer Costs","text":""},{"location":"cost-estimation/#inbound-free","title":"Inbound (Free)","text":"<ul> <li>Data transfer into AWS is free</li> <li>Pulling Docker images: Free</li> <li>SSH/API calls into instances: Free</li> </ul>"},{"location":"cost-estimation/#outbound","title":"Outbound","text":"Destination Price per GB First 100 GB/month Free Next 10 TB/month $0.09 Internet (general) $0.09 <p>Typical Usage: &lt; 100 GB/month (covered by free tier)</p>"},{"location":"cost-estimation/#inter-region-transfer","title":"Inter-Region Transfer","text":"<p>If using resources across regions:</p> Transfer Type Price per GB Cross-region $0.02 <p>Avoid: Keep all resources in same region</p>"},{"location":"cost-estimation/#example-cost-scenarios","title":"Example Cost Scenarios","text":""},{"location":"cost-estimation/#scenario-1-developmenttesting","title":"Scenario 1: Development/Testing","text":"<p>Setup: - 1 allocator (t2.micro) - 2 client VMs (g4dn.xlarge) - Running 40 hours/month</p> <p>Costs: - Infrastructure: $0.05/month - Allocator: 40 hours \u00d7 $0.0116 = $0.46 - Client VMs: 2 \u00d7 40 hours \u00d7 $0.526 = $42.08 - Storage: $2.40 (allocator) + $16.00 (2 clients) = $18.40</p> <p>Total: ~$61/month</p>"},{"location":"cost-estimation/#scenario-2-light-production-use","title":"Scenario 2: Light Production Use","text":"<p>Setup: - 1 allocator (t2.small, 24/7) - 5 client VMs (g4dn.xlarge) - VMs running 160 hours/month each</p> <p>Costs: - Infrastructure: $0.95/month (with Route 53) - Allocator: $16.79/month - Client VMs: 5 \u00d7 160 hours \u00d7 $0.526 = $420.80 - Storage: $2.40 + (5 \u00d7 $8.00) = $42.40</p> <p>Total: ~$481/month</p>"},{"location":"cost-estimation/#scenario-3-heavy-production-use","title":"Scenario 3: Heavy Production Use","text":"<p>Setup: - 1 allocator (t2.small, 24/7, Reserved Instance) - 20 client VMs (g4dn.xlarge, Spot Instances) - VMs running 320 hours/month each</p> <p>Costs: - Infrastructure: $0.95/month - Allocator: $5.03/month (Reserved Instance) - Client VMs: 20 \u00d7 320 hours \u00d7 $0.158 (Spot) = $1,011.20 - Storage: $2.40 + (20 \u00d7 $8.00) = $162.40</p> <p>Total: ~$1,182/month</p> <p>Savings vs On-Demand: ~$3,200/month (70% reduction)</p>"},{"location":"cost-estimation/#scenario-4-minimal-cost-conscious","title":"Scenario 4: Minimal (Cost-Conscious)","text":"<p>Setup: - 1 allocator (t2.micro) - 3 client VMs (g4dn.xlarge, Spot) - Only running when actively working (40 hours/month)</p> <p>Costs: - Infrastructure: $0.05/month - Allocator: 40 hours \u00d7 $0.0116 = $0.46 - Client VMs: 3 \u00d7 40 hours \u00d7 $0.158 (Spot) = $18.96 - Storage: Minimal (terminate when done) = $0.50</p> <p>Total: ~$20/month</p>"},{"location":"cost-estimation/#cost-monitoring","title":"Cost Monitoring","text":""},{"location":"cost-estimation/#set-up-billing-alerts","title":"Set Up Billing Alerts","text":"<p>Step 1: Create SNS Topic <pre><code>aws sns create-topic --name lablink-billing-alerts\n</code></pre></p> <p>Step 2: Subscribe Email <pre><code>aws sns subscribe \\\n  --topic-arn arn:aws:sns:us-west-2:ACCOUNT_ID:lablink-billing-alerts \\\n  --protocol email \\\n  --notification-endpoint your-email@example.com\n</code></pre></p> <p>Step 3: Create Budget <pre><code>aws budgets create-budget --account-id ACCOUNT_ID --budget file://budget.json\n</code></pre></p> <p><code>budget.json</code>: <pre><code>{\n  \"BudgetName\": \"LabLink Monthly Budget\",\n  \"BudgetLimit\": {\n    \"Amount\": \"100\",\n    \"Unit\": \"USD\"\n  },\n  \"TimeUnit\": \"MONTHLY\",\n  \"BudgetType\": \"COST\"\n}\n</code></pre></p>"},{"location":"cost-estimation/#view-current-costs","title":"View Current Costs","text":"<p>AWS Console: 1. Navigate to Billing Dashboard 2. View Cost Explorer 3. Filter by tag <code>Project: LabLink</code></p> <p>AWS CLI: <pre><code>aws ce get-cost-and-usage \\\n  --time-period Start=2025-01-01,End=2025-01-31 \\\n  --granularity MONTHLY \\\n  --metrics UnblendedCost \\\n  --filter file://filter.json\n</code></pre></p> <p><code>filter.json</code>: <pre><code>{\n  \"Tags\": {\n    \"Key\": \"Project\",\n    \"Values\": [\"LabLink\"]\n  }\n}\n</code></pre></p>"},{"location":"cost-estimation/#tag-resources","title":"Tag Resources","text":"<p>Tag all resources for cost tracking:</p> <pre><code># terraform/main.tf\nresource \"aws_instance\" \"lablink_allocator\" {\n  # ... other config\n\n  tags = {\n    Name    = \"lablink-allocator-${var.environment}\"\n    Project = \"LabLink\"\n    Environment = var.environment\n    ManagedBy = \"Terraform\"\n  }\n}\n</code></pre> <p>View costs by tag in Cost Explorer.</p>"},{"location":"cost-estimation/#cost-optimization-checklist","title":"Cost Optimization Checklist","text":"<ul> <li>[ ] Use Spot Instances for client VMs (70-90% savings)</li> <li>[ ] Terminate VMs when not in use</li> <li>[ ] Use Reserved Instances for always-on allocators (75% savings)</li> <li>[ ] Right-size instance types (don't over-provision)</li> <li>[ ] Use gp3 volumes instead of gp2 (20% cheaper)</li> <li>[ ] Set up billing alerts</li> <li>[ ] Monitor costs weekly in Cost Explorer</li> <li>[ ] Tag all resources for cost attribution</li> <li>[ ] Use Lifecycle Policies to delete old S3 backups</li> <li>[ ] Terminate (not stop) unused instances</li> <li>[ ] Release unused Elastic IPs</li> <li>[ ] Clean up old EBS snapshots</li> </ul>"},{"location":"cost-estimation/#free-tier","title":"Free Tier","text":"<p>New AWS accounts get 12 months of free tier:</p> Service Free Tier (Monthly) EC2 (t2.micro) 750 hours EBS (gp2/gp3) 30 GB S3 5 GB storage Data Transfer 100 GB out <p>Note: GPU instances (g4dn, g5, p3) are not included in free tier.</p>"},{"location":"cost-estimation/#hidden-costs-to-watch","title":"Hidden Costs to Watch","text":"<ol> <li>Unassociated Elastic IPs: $3.60/month each</li> <li>Stopped instances with EBS: Storage charges still apply</li> <li>Old EBS snapshots: Accumulate over time</li> <li>Unused load balancers: $16-18/month</li> <li>NAT Gateways: $32/month + data transfer</li> <li>Idle RDS instances: $15-200/month</li> </ol> <p>Solution: Regular cleanup and monitoring</p>"},{"location":"cost-estimation/#cost-comparison","title":"Cost Comparison","text":""},{"location":"cost-estimation/#lablink-vs-self-managed","title":"LabLink vs Self-Managed","text":"Aspect LabLink Self-Managed Infrastructure setup $0-1/month $0 Allocator runtime $8-17/month $0 (your time) Client VMs Same Same Management time Minimal Significant <p>LabLink advantage: Time savings outweigh small infrastructure costs</p>"},{"location":"cost-estimation/#lablink-vs-managed-services","title":"LabLink vs Managed Services","text":"Service Monthly Cost (5 VMs) Setup Complexity LabLink ~$481 Moderate AWS Batch ~$500+ High SageMaker ~$600+ Moderate Cloud GPUs (vast.ai) ~$200-400 Low <p>LabLink advantage: Balance of cost, features, and control</p>"},{"location":"cost-estimation/#budget-recommendations","title":"Budget Recommendations","text":""},{"location":"cost-estimation/#by-use-case","title":"By Use Case","text":"Use Case Recommended Monthly Budget Individual researcher (occasional) $50-100 Individual researcher (regular) $200-500 Small research group $500-1,500 Large research group $1,500-5,000+"},{"location":"cost-estimation/#next-steps","title":"Next Steps","text":"<ul> <li>AWS Setup: Set up billing alerts</li> <li>Configuration: Choose cost-effective instance types</li> <li>Deployment: Deploy with cost optimization</li> </ul>"},{"location":"cost-estimation/#questions-about-costs","title":"Questions About Costs?","text":"<ul> <li>Check AWS Pricing</li> <li>Use AWS Pricing Calculator</li> <li>Contact AWS Support for enterprise pricing</li> </ul>"},{"location":"database/","title":"Database Management","text":"<p>This guide covers the PostgreSQL database used by LabLink, including schema, management tasks, and troubleshooting.</p>"},{"location":"database/#database-overview","title":"Database Overview","text":"<p>LabLink uses PostgreSQL for:</p> <ul> <li>Tracking VM states (available, in-use, failed)</li> <li>Storing user assignments</li> <li>Real-time notifications (LISTEN/NOTIFY)</li> <li>Audit logging</li> </ul> <p>Version: PostgreSQL 13+ Location: Runs in allocator Docker container Access: Port 5432 (internal)</p>"},{"location":"database/#database-schema","title":"Database Schema","text":""},{"location":"database/#tables","title":"Tables","text":""},{"location":"database/#vms-table","title":"<code>vms</code> Table","text":"<p>Primary table tracking all VM instances.</p> Column Type Constraints Description <code>id</code> SERIAL PRIMARY KEY Unique VM identifier <code>hostname</code> VARCHAR(255) NOT NULL, UNIQUE VM hostname/instance ID <code>email</code> VARCHAR(255) User email address <code>status</code> VARCHAR(50) NOT NULL VM status (available/in-use/failed) <code>crd_command</code> TEXT Command to execute on VM <code>created_at</code> TIMESTAMP DEFAULT NOW() Creation timestamp <code>updated_at</code> TIMESTAMP DEFAULT NOW() Last update timestamp <p>Status Values: - <code>available</code>: VM ready for assignment - <code>in-use</code>: VM currently assigned to user - <code>failed</code>: VM encountered error</p> <p>Example Row: <pre><code>id  | hostname          | email            | status    | crd_command      | created_at          | updated_at\n----+-------------------+------------------+-----------+------------------+---------------------+---------------------\n1   | i-0abc123def456   | user@example.com | in-use    | python train.py  | 2025-01-15 10:30:00 | 2025-01-15 10:35:00\n</code></pre></p>"},{"location":"database/#triggers","title":"Triggers","text":""},{"location":"database/#notify_vm_update","title":"<code>notify_vm_update</code>","text":"<p>Sends PostgreSQL NOTIFY when VM table changes.</p> <p>Purpose: Real-time updates to client VMs</p> <p>Definition: <pre><code>CREATE OR REPLACE FUNCTION notify_vm_changes()\nRETURNS trigger AS $$\nBEGIN\n  PERFORM pg_notify('vm_updates', row_to_json(NEW)::text);\n  RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER vm_update_trigger\nAFTER INSERT OR UPDATE ON vms\nFOR EACH ROW\nEXECUTE FUNCTION notify_vm_changes();\n</code></pre></p> <p>How it works: 1. Row inserted/updated in <code>vms</code> table 2. Trigger fires 3. JSON payload sent to <code>vm_updates</code> channel 4. Listening clients receive notification 5. Clients query for their specific assignment</p>"},{"location":"database/#accessing-the-database","title":"Accessing the Database","text":""},{"location":"database/#via-ssh-and-psql","title":"Via SSH and psql","text":"<pre><code># SSH into allocator\nssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\n\n# Get container ID\nCONTAINER_ID=$(sudo docker ps --filter \"ancestor=ghcr.io/talmolab/lablink-allocator-image\" --format \"{{.ID}}\")\n\n# Access PostgreSQL\nsudo docker exec -it $CONTAINER_ID psql -U lablink -d lablink_db\n</code></pre>"},{"location":"database/#connection-parameters","title":"Connection Parameters","text":"<p>From config (<code>lablink-infrastructure/config/config.yaml</code>):</p> <pre><code>db:\n  dbname: \"lablink_db\"\n  user: \"lablink\"\n  password: \"lablink\"  # Change in production!\n  host: \"localhost\"\n  port: 5432\n</code></pre>"},{"location":"database/#from-python-inside-container","title":"From Python (Inside Container)","text":"<pre><code>import psycopg2\n\nconn = psycopg2.connect(\n    dbname=\"lablink_db\",\n    user=\"lablink\",\n    password=\"lablink\",\n    host=\"localhost\",\n    port=5432\n)\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT * FROM vms;\")\nrows = cursor.fetchall()\n\nfor row in rows:\n    print(row)\n\nconn.close()\n</code></pre>"},{"location":"database/#common-database-operations","title":"Common Database Operations","text":""},{"location":"database/#view-all-vms","title":"View All VMs","text":"<pre><code>SELECT * FROM vms;\n</code></pre>"},{"location":"database/#view-available-vms","title":"View Available VMs","text":"<pre><code>SELECT hostname, status, created_at\nFROM vms\nWHERE status = 'available'\nORDER BY created_at;\n</code></pre>"},{"location":"database/#view-in-use-vms","title":"View In-Use VMs","text":"<pre><code>SELECT hostname, email, status, crd_command, updated_at\nFROM vms\nWHERE status = 'in-use'\nORDER BY updated_at DESC;\n</code></pre>"},{"location":"database/#count-vms-by-status","title":"Count VMs by Status","text":"<pre><code>SELECT status, COUNT(*) as count\nFROM vms\nGROUP BY status;\n</code></pre> <p>Expected output: <pre><code> status    | count\n-----------+-------\n available |     5\n in-use    |     3\n failed    |     1\n</code></pre></p>"},{"location":"database/#find-vm-by-email","title":"Find VM by Email","text":"<pre><code>SELECT hostname, status, crd_command\nFROM vms\nWHERE email = 'user@example.com';\n</code></pre>"},{"location":"database/#update-vm-status","title":"Update VM Status","text":"<pre><code>-- Mark VM as available\nUPDATE vms\nSET status = 'available', email = NULL, crd_command = NULL, updated_at = NOW()\nWHERE hostname = 'i-0abc123def456';\n\n-- Mark VM as failed\nUPDATE vms\nSET status = 'failed', updated_at = NOW()\nWHERE hostname = 'i-0abc123def456';\n</code></pre>"},{"location":"database/#delete-vm-record","title":"Delete VM Record","text":"<pre><code>DELETE FROM vms WHERE hostname = 'i-0abc123def456';\n</code></pre> <p>Warning</p> <p>Only delete after VM instance is terminated in AWS.</p>"},{"location":"database/#clear-all-vms","title":"Clear All VMs","text":"<pre><code>-- Use with caution!\nTRUNCATE TABLE vms;\n</code></pre>"},{"location":"database/#monitoring-listennotify","title":"Monitoring LISTEN/NOTIFY","text":""},{"location":"database/#listen-for-vm-updates","title":"Listen for VM Updates","text":"<pre><code>-- In psql session\nLISTEN vm_updates;\n\n-- In another session, make a change:\nUPDATE vms SET status = 'in-use' WHERE id = 1;\n\n-- First session receives:\nAsynchronous notification \"vm_updates\" received from server process with PID 12345.\n</code></pre>"},{"location":"database/#listen-from-python","title":"Listen from Python","text":"<pre><code>import psycopg2\nimport select\n\nconn = psycopg2.connect(\n    dbname=\"lablink_db\",\n    user=\"lablink\",\n    password=\"lablink\",\n    host=\"localhost\"\n)\n\nconn.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)\ncursor = conn.cursor()\ncursor.execute(\"LISTEN vm_updates;\")\n\nprint(\"Waiting for notifications...\")\n\nwhile True:\n    if select.select([conn], [], [], 5) == ([], [], []):\n        print(\"Timeout\")\n    else:\n        conn.poll()\n        while conn.notifies:\n            notify = conn.notifies.pop(0)\n            print(f\"Notification: {notify.payload}\")\n</code></pre>"},{"location":"database/#database-backup","title":"Database Backup","text":""},{"location":"database/#manual-backup","title":"Manual Backup","text":"<pre><code># SSH into allocator\nssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\n\n# Backup database\nsudo docker exec &lt;container-id&gt; pg_dump -U lablink lablink_db &gt; lablink_backup.sql\n\n# Download backup\nscp -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;:~/lablink_backup.sql ./\n</code></pre>"},{"location":"database/#automated-backup-script","title":"Automated Backup Script","text":"<p><code>backup.sh</code>: <pre><code>#!/bin/bash\n\nCONTAINER_ID=$(sudo docker ps --filter \"ancestor=ghcr.io/talmolab/lablink-allocator-image\" --format \"{{.ID}}\")\nBACKUP_DIR=\"/home/ubuntu/backups\"\nDATE=$(date +%Y%m%d_%H%M%S)\n\nmkdir -p $BACKUP_DIR\n\nsudo docker exec $CONTAINER_ID pg_dump -U lablink lablink_db &gt; $BACKUP_DIR/lablink_$DATE.sql\n\n# Upload to S3\naws s3 cp $BACKUP_DIR/lablink_$DATE.sql s3://lablink-backups/\n\n# Keep only last 7 days locally\nfind $BACKUP_DIR -name \"lablink_*.sql\" -mtime +7 -delete\n\necho \"Backup complete: lablink_$DATE.sql\"\n</code></pre></p> <p>Setup cron job: <pre><code># Edit crontab\ncrontab -e\n\n# Add daily backup at 2 AM\n0 2 * * * /home/ubuntu/backup.sh &gt;&gt; /var/log/lablink-backup.log 2&gt;&amp;1\n</code></pre></p>"},{"location":"database/#restore-from-backup","title":"Restore from Backup","text":"<pre><code># SSH into allocator\nssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\n\n# Copy backup to instance\nscp -i ~/lablink-key.pem lablink_backup.sql ubuntu@&lt;allocator-ip&gt;:~/\n\n# Restore database\nsudo docker exec -i &lt;container-id&gt; psql -U lablink lablink_db &lt; lablink_backup.sql\n</code></pre>"},{"location":"database/#database-maintenance","title":"Database Maintenance","text":""},{"location":"database/#vacuum-database","title":"Vacuum Database","text":"<p>Remove dead tuples and reclaim space:</p> <pre><code>-- Analyze and vacuum\nVACUUM ANALYZE vms;\n\n-- Full vacuum (more aggressive, requires exclusive lock)\nVACUUM FULL vms;\n</code></pre>"},{"location":"database/#reindex","title":"Reindex","text":"<p>Rebuild indexes for performance:</p> <pre><code>REINDEX TABLE vms;\n</code></pre>"},{"location":"database/#check-database-size","title":"Check Database Size","text":"<pre><code>SELECT pg_size_pretty(pg_database_size('lablink_db'));\n</code></pre>"},{"location":"database/#check-table-size","title":"Check Table Size","text":"<pre><code>SELECT\n  schemaname,\n  tablename,\n  pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size\nFROM pg_tables\nWHERE schemaname = 'public'\nORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;\n</code></pre>"},{"location":"database/#view-active-connections","title":"View Active Connections","text":"<pre><code>SELECT\n  pid,\n  usename,\n  application_name,\n  client_addr,\n  state,\n  query\nFROM pg_stat_activity\nWHERE datname = 'lablink_db';\n</code></pre>"},{"location":"database/#kill-idle-connections","title":"Kill Idle Connections","text":"<pre><code>SELECT pg_terminate_backend(pid)\nFROM pg_stat_activity\nWHERE datname = 'lablink_db'\n  AND state = 'idle'\n  AND state_change &lt; NOW() - INTERVAL '10 minutes';\n</code></pre>"},{"location":"database/#migrating-to-rds-production","title":"Migrating to RDS (Production)","text":"<p>For production, consider Amazon RDS for managed PostgreSQL.</p>"},{"location":"database/#benefits","title":"Benefits","text":"<ul> <li>Automated backups</li> <li>Multi-AZ high availability</li> <li>Automatic failover</li> <li>Automated patching</li> <li>Monitoring and metrics</li> <li>Point-in-time recovery</li> </ul>"},{"location":"database/#setup-rds-instance","title":"Setup RDS Instance","text":"<pre><code># terraform/rds.tf\n\nresource \"aws_db_instance\" \"lablink\" {\n  identifier        = \"lablink-db-${var.environment}\"\n  engine            = \"postgres\"\n  engine_version    = \"13.7\"\n  instance_class    = \"db.t3.micro\"\n  allocated_storage = 20\n\n  db_name  = \"lablink_db\"\n  username = \"lablink\"\n  password = var.db_password  # From Secrets Manager\n\n  vpc_security_group_ids = [aws_security_group.rds.id]\n  db_subnet_group_name   = aws_db_subnet_group.lablink.name\n\n  backup_retention_period = 7\n  backup_window          = \"03:00-04:00\"\n  maintenance_window     = \"mon:04:00-mon:05:00\"\n\n  storage_encrypted      = true\n  skip_final_snapshot   = false\n  final_snapshot_identifier = \"lablink-final-${var.environment}\"\n\n  tags = {\n    Name = \"lablink-db-${var.environment}\"\n  }\n}\n\noutput \"rds_endpoint\" {\n  value = aws_db_instance.lablink.endpoint\n}\n</code></pre>"},{"location":"database/#update-application-configuration","title":"Update Application Configuration","text":"<pre><code>db:\n  dbname: \"lablink_db\"\n  user: \"lablink\"\n  password: \"${DB_PASSWORD}\"  # From Secrets Manager\n  host: \"lablink-db-prod.xxxxx.us-west-2.rds.amazonaws.com\"\n  port: 5432\n</code></pre>"},{"location":"database/#migrate-data","title":"Migrate Data","text":"<pre><code># Dump from container database\nsudo docker exec &lt;container-id&gt; pg_dump -U lablink lablink_db &gt; dump.sql\n\n# Restore to RDS\npsql -h lablink-db-prod.xxxxx.us-west-2.rds.amazonaws.com -U lablink -d lablink_db &lt; dump.sql\n</code></pre>"},{"location":"database/#troubleshooting","title":"Troubleshooting","text":""},{"location":"database/#postgresql-wont-start","title":"PostgreSQL Won't Start","text":"<p>Check logs: <pre><code>sudo docker exec &lt;container-id&gt; tail -f /var/log/postgresql/postgresql-13-main.log\n</code></pre></p> <p>Common issues:</p> <ol> <li> <p>Port already in use:    <pre><code>sudo netstat -tulpn | grep 5432\n# Kill process using port\n</code></pre></p> </li> <li> <p>Disk full:    <pre><code>df -h\n# Clean up space\n</code></pre></p> </li> <li> <p>Corrupt data files:    <pre><code># Stop container, remove volume, restart\nsudo docker stop &lt;container-id&gt;\nsudo docker rm &lt;container-id&gt;\n# Redeploy with fresh database\n</code></pre></p> </li> </ol>"},{"location":"database/#cannot-connect-to-database","title":"Cannot Connect to Database","text":"<p>Check connection from allocator: <pre><code>sudo docker exec &lt;container-id&gt; pg_isready -U lablink\n</code></pre></p> <p>Test connection: <pre><code>sudo docker exec &lt;container-id&gt; psql -U lablink -d lablink_db -c \"SELECT 1;\"\n</code></pre></p> <p>Check pg_hba.conf: <pre><code>sudo docker exec &lt;container-id&gt; cat /etc/postgresql/13/main/pg_hba.conf\n</code></pre></p> <p>Should include: <pre><code>host    all             all             0.0.0.0/0            md5\n</code></pre></p>"},{"location":"database/#database-performance-issues","title":"Database Performance Issues","text":"<p>Check slow queries: <pre><code>SELECT\n  pid,\n  now() - pg_stat_activity.query_start AS duration,\n  query\nFROM pg_stat_activity\nWHERE state = 'active'\n  AND now() - pg_stat_activity.query_start &gt; interval '5 seconds'\nORDER BY duration DESC;\n</code></pre></p> <p>Enable query logging: <pre><code># In postgresql.conf\nlog_min_duration_statement = 1000  # Log queries &gt; 1 second\n</code></pre></p> <p>Add indexes: <pre><code>-- Index on email for faster lookups\nCREATE INDEX idx_vms_email ON vms(email);\n\n-- Index on status\nCREATE INDEX idx_vms_status ON vms(status);\n\n-- Composite index\nCREATE INDEX idx_vms_status_email ON vms(status, email);\n</code></pre></p>"},{"location":"database/#restart-postgresql","title":"Restart PostgreSQL","text":"<p>Known issue requiring manual restart after first boot:</p> <pre><code># SSH into allocator\nssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\n\n# Access container\nsudo docker exec -it &lt;container-id&gt; bash\n\n# Inside container\n/etc/init.d/postgresql restart\n\n# Verify\npg_isready -U lablink\n</code></pre>"},{"location":"database/#security-best-practices","title":"Security Best Practices","text":"<ol> <li>Change default password: See Security</li> <li>Use SSL connections: Configure <code>sslmode=require</code></li> <li>Restrict pg_hba.conf: Limit to specific IPs/VPCs</li> <li>Regular backups: Automate daily backups</li> <li>Monitor access logs: Review connection attempts</li> <li>Use RDS for production: Better security and management</li> </ol>"},{"location":"database/#performance-tuning","title":"Performance Tuning","text":""},{"location":"database/#configuration-recommendations","title":"Configuration Recommendations","text":"<p>For allocator with 2GB RAM:</p> <pre><code># postgresql.conf\nshared_buffers = 512MB\neffective_cache_size = 1GB\nmaintenance_work_mem = 128MB\ncheckpoint_completion_target = 0.9\nwal_buffers = 16MB\ndefault_statistics_target = 100\nrandom_page_cost = 1.1\neffective_io_concurrency = 200\nwork_mem = 5MB\nmin_wal_size = 1GB\nmax_wal_size = 4GB\n</code></pre>"},{"location":"database/#connection-pooling","title":"Connection Pooling","text":"<p>For high-concurrency, use pgBouncer:</p> <pre><code># docker-compose.yml\nservices:\n  pgbouncer:\n    image: pgbouncer/pgbouncer\n    environment:\n      DATABASES_HOST: localhost\n      DATABASES_PORT: 5432\n      DATABASES_DBNAME: lablink_db\n    ports:\n      - \"6432:6432\"\n</code></pre>"},{"location":"database/#next-steps","title":"Next Steps","text":"<ul> <li>SSH Access: Connect to database via SSH</li> <li>Troubleshooting: Fix database issues</li> <li>Security: Secure database access</li> <li>Architecture: Understand database role</li> </ul>"},{"location":"database/#quick-reference","title":"Quick Reference","text":"<pre><code>-- View all VMs\nSELECT * FROM vms;\n\n-- Count by status\nSELECT status, COUNT(*) FROM vms GROUP BY status;\n\n-- Find available VMs\nSELECT * FROM vms WHERE status = 'available';\n\n-- Update VM status\nUPDATE vms SET status = 'available' WHERE hostname = 'i-xxxxx';\n\n-- Backup\npg_dump -U lablink lablink_db &gt; backup.sql\n\n-- Restore\npsql -U lablink lablink_db &lt; backup.sql\n\n-- Vacuum\nVACUUM ANALYZE vms;\n</code></pre>"},{"location":"deployment/","title":"Deployment","text":"<p>This guide covers deploying LabLink to AWS using both automated (GitHub Actions) and manual (Terraform CLI) methods.</p>"},{"location":"deployment/#deployment-overview","title":"Deployment Overview","text":"<p>LabLink supports three deployment environments:</p> Environment Purpose Trigger Image Tag dev Local/personal development Manual <code>*-test</code> test Staging, pre-production testing Push to <code>test</code> branch <code>*-test</code> prod Production workloads Manual workflow dispatch Pinned version tags"},{"location":"deployment/#prerequisites","title":"Prerequisites","text":"<p>Before deploying, ensure you have:</p> <ul> <li>[x] AWS account configured (see Prerequisites)</li> <li>[x] Terraform installed (see Prerequisites)</li> <li>[x] S3 bucket for Terraform state (see AWS Setup)</li> <li>[x] Elastic IP allocated for test/prod (see AWS Setup)</li> <li>[x] IAM roles configured for GitHub Actions (see AWS Setup)</li> </ul>"},{"location":"deployment/#method-1-github-actions-recommended","title":"Method 1: GitHub Actions (Recommended)","text":"<p>Automated deployment via CI/CD pipelines.</p>"},{"location":"deployment/#initial-setup","title":"Initial Setup","text":"<ol> <li>Configure GitHub Secrets</li> </ol> <p>Navigate to Settings \u2192 Secrets and variables \u2192 Actions in your GitHub repository.</p> <p>Required secrets:    - <code>AWS_ROLE_ARN</code>: IAM role ARN for GitHub Actions authentication      Example: <code>arn:aws:iam::711387140753:role/GitHubActionsLabLinkRole</code>    - <code>AWS_REGION</code>: AWS region for deployment      Example: <code>us-west-2</code>, <code>eu-west-1</code>, <code>ap-northeast-1</code> Note: Must match region in <code>config/config.yaml</code>    - <code>ADMIN_PASSWORD</code>: Admin password for allocator web interface      Example: Generate a secure password using a password manager    - <code>DB_PASSWORD</code>: Database password for PostgreSQL      Example: Generate a secure password using a password manager</p> <p>Security Note: The workflow automatically replaces <code>PLACEHOLDER_ADMIN_PASSWORD</code> and <code>PLACEHOLDER_DB_PASSWORD</code> in config files with these secret values before Terraform runs, preventing passwords from appearing in logs. If these secrets are not set, the workflow uses temporary <code>CHANGEME_*</code> defaults and displays a warning.</p> <ol> <li>Verify OIDC Configuration</li> </ol> <p>Ensure AWS IAM role exists and trusts your GitHub repository:</p> <ul> <li>OIDC provider exists: <code>token.actions.githubusercontent.com</code></li> <li>IAM role trust policy includes your repository: <code>repo:YOUR_ORG/YOUR_REPO:*</code></li> <li>Role has PowerUserAccess or equivalent permissions</li> </ul> <p>See detailed setup instructions: AWS Setup \u2192 OIDC Configuration</p>"},{"location":"deployment/#deploy-to-test-environment","title":"Deploy to Test Environment","text":"<p>Trigger: Push to <code>test</code> branch</p> <pre><code>git checkout -b test\ngit push origin test\n</code></pre> <p>This automatically: 1. Builds Docker images with <code>-test</code> tags 2. Runs Terraform init with <code>backend-test.hcl</code> 3. Deploys to test environment 4. Outputs allocator URL and SSH key</p> <p>Monitor Progress: - Go to Actions tab in GitHub - Watch <code>Terraform Deploy</code> workflow - Check logs for any errors</p> <p>Access Deployment: - Allocator URL: Available in workflow output - SSH Key: Download from workflow artifacts</p>"},{"location":"deployment/#deploy-to-production","title":"Deploy to Production","text":"<p>Trigger: Manual workflow dispatch</p> <ol> <li> <p>Navigate to Actions tab in GitHub</p> </li> <li> <p>Select \"Terraform Deploy\" workflow</p> </li> <li> <p>Click \"Run workflow\"</p> </li> <li> <p>Fill in parameters:</p> </li> <li>Environment: <code>prod</code></li> <li> <p>Image tag: Specific version (e.g., <code>v1.0.0</code> or commit SHA)</p> </li> <li> <p>Click \"Run workflow\"</p> </li> </ol> <p>Why Manual? Production deployments use pinned image tags for reproducibility and require explicit approval.</p> <p>Production Image Tags</p> <p>Never use <code>:latest</code> or <code>-test</code> tags in production. Always use specific version tags or commit SHAs.</p>"},{"location":"deployment/#deployment-outputs","title":"Deployment Outputs","text":"<p>After successful deployment, the workflow provides:</p> <ul> <li>Allocator FQDN: DNS name for your allocator</li> <li>EC2 Public IP: IP address of the allocator instance</li> <li>EC2 Key Name: Name of the SSH key pair</li> <li>Private Key: Downloaded as artifact (expires in 1 day)</li> </ul>"},{"location":"deployment/#deployment-workflow-details","title":"Deployment Workflow Details","text":"<p>The GitHub Actions workflow (<code>.github/workflows/lablink-allocator-terraform.yml</code>) performs:</p> <ol> <li>Checkout code from repository</li> <li>Configure AWS credentials via OIDC</li> <li>Setup Terraform (version 1.6.6)</li> <li>Determine environment from trigger</li> <li>Inject password secrets - Replace placeholders in config files with GitHub secrets</li> <li>Initialize Terraform with environment-specific backend</li> <li>Validate Terraform configuration</li> <li>Plan infrastructure changes</li> <li>Apply changes to AWS</li> <li>Save SSH key as artifact</li> <li>Output deployment details</li> <li>Destroy on failure (if apply fails)</li> </ol> <p>Password Injection Step: Before Terraform runs, the workflow finds the config file and uses <code>sed</code> to replace <code>PLACEHOLDER_ADMIN_PASSWORD</code> and <code>PLACEHOLDER_DB_PASSWORD</code> with values from GitHub secrets. This ensures passwords never appear in Terraform logs while maintaining secure configuration.</p>"},{"location":"deployment/#method-2-manual-terraform-deployment","title":"Method 2: Manual Terraform Deployment","text":"<p>Deploy directly from your local machine using Terraform CLI.</p>"},{"location":"deployment/#step-1-clone-template-repository","title":"Step 1: Clone Template Repository","text":"<pre><code>git clone https://github.com/talmolab/lablink-template.git\ncd lablink-template/lablink-infrastructure\n</code></pre> <p>All infrastructure configurations are in the <code>lablink-infrastructure/</code> directory within the template repository.</p>"},{"location":"deployment/#step-2-configure-aws-credentials","title":"Step 2: Configure AWS Credentials","text":"<p>Option A: AWS CLI Profiles <pre><code>export AWS_PROFILE=your-profile\naws configure --profile your-profile\n</code></pre></p> <p>Option B: Environment Variables <pre><code>export AWS_ACCESS_KEY_ID=your-access-key\nexport AWS_SECRET_ACCESS_KEY=your-secret-key\nexport AWS_REGION=us-west-2\n</code></pre></p> <p>Option C: AWS SSO <pre><code>aws sso login --profile your-sso-profile\nexport AWS_PROFILE=your-sso-profile\n</code></pre></p>"},{"location":"deployment/#step-3-initialize-terraform","title":"Step 3: Initialize Terraform","text":"<p>For dev environment (local state): <pre><code>terraform init\n</code></pre></p> <p>For test/prod (remote state): <pre><code># Test\nterraform init -backend-config=backend-test.hcl\n\n# Production\nterraform init -backend-config=backend-prod.hcl\n</code></pre></p>"},{"location":"deployment/#step-4-plan-deployment","title":"Step 4: Plan Deployment","text":"<p>Preview infrastructure changes:</p> <pre><code>terraform plan \\\n  -var=\"resource_suffix=dev\" \\\n  -var=\"allocator_image_tag=linux-amd64-latest-test\"\n</code></pre> <p>Review the plan output carefully. Terraform will show: - Resources to be created - Resources to be modified - Resources to be destroyed</p>"},{"location":"deployment/#step-5-apply-deployment","title":"Step 5: Apply Deployment","text":"<p>Deploy the infrastructure:</p> <pre><code>terraform apply \\\n  -var=\"resource_suffix=dev\" \\\n  -var=\"allocator_image_tag=linux-amd64-latest-test\"\n</code></pre> <p>Type <code>yes</code> when prompted to confirm.</p> <p>Deployment time: ~5-10 minutes</p>"},{"location":"deployment/#step-6-get-outputs","title":"Step 6: Get Outputs","text":"<p>After deployment completes:</p> <pre><code># Get allocator URL\nterraform output allocator_fqdn\n\n# Get public IP\nterraform output ec2_public_ip\n\n# Save SSH key\nterraform output -raw private_key_pem &gt; ~/lablink-dev-key.pem\nchmod 600 ~/lablink-dev-key.pem\n</code></pre>"},{"location":"deployment/#step-7-verify-deployment","title":"Step 7: Verify Deployment","text":"<p>Test the allocator:</p> <pre><code># Get the IP\nALLOCATOR_IP=$(terraform output -raw ec2_public_ip)\n\n# Test web interface\ncurl http://$ALLOCATOR_IP:80\n\n# SSH into instance\nssh -i ~/lablink-dev-key.pem ubuntu@$ALLOCATOR_IP\n</code></pre>"},{"location":"deployment/#terraform-variables","title":"Terraform Variables","text":"<p>Key variables for customizing deployment:</p> Variable Description Default Example <code>resource_suffix</code> Environment suffix for resource names <code>dev</code> <code>prod</code>, <code>test</code> <code>allocator_image_tag</code> Docker image tag for allocator (required) <code>v1.0.0</code>, <code>linux-amd64-latest-test</code> <code>instance_type</code> EC2 instance type for allocator <code>t2.micro</code> <code>t2.small</code>, <code>t3.medium</code> <code>allocated_eip</code> Pre-allocated Elastic IP (test/prod) None <code>eipalloc-xxxxx</code> <p>Usage: <pre><code>terraform apply \\\n  -var=\"resource_suffix=prod\" \\\n  -var=\"allocator_image_tag=v1.0.0\" \\\n  -var=\"instance_type=t2.small\" \\\n  -var=\"allocated_eip=eipalloc-xxxxx\"\n</code></pre></p>"},{"location":"deployment/#environment-specific-configurations","title":"Environment-Specific Configurations","text":""},{"location":"deployment/#development","title":"Development","text":"<p>Purpose: Local testing, rapid iteration</p> <p>Configuration: - Terraform state: Local file - Image tag: <code>-test</code> versions - Instance type: <code>t2.micro</code> (cheapest) - No Elastic IP (dynamic)</p> <p>Deploy: <pre><code>terraform init\nterraform apply \\\n  -var=\"resource_suffix=dev\" \\\n  -var=\"allocator_image_tag=linux-amd64-latest-test\"\n</code></pre></p>"},{"location":"deployment/#teststaging","title":"Test/Staging","text":"<p>Purpose: Pre-production validation, integration testing</p> <p>Configuration: - Terraform state: S3 bucket (<code>backend-test.hcl</code>) - Image tag: <code>-test</code> versions - Instance type: Same as production - Elastic IP: Pre-allocated</p> <p>Deploy: <pre><code>terraform init -backend-config=backend-test.hcl\nterraform apply \\\n  -var=\"resource_suffix=test\" \\\n  -var=\"allocator_image_tag=linux-amd64-latest-test\" \\\n  -var=\"allocated_eip=eipalloc-test\"\n</code></pre></p>"},{"location":"deployment/#production","title":"Production","text":"<p>Purpose: Live workloads, stable releases</p> <p>Configuration: - Terraform state: S3 bucket (<code>backend-prod.hcl</code>) - Image tag: Pinned versions (<code>v1.0.0</code>) - Instance type: Appropriately sized - Elastic IP: Pre-allocated - Monitoring and backups enabled</p> <p>Deploy: <pre><code>terraform init -backend-config=backend-prod.hcl\nterraform apply \\\n  -var=\"resource_suffix=prod\" \\\n  -var=\"allocator_image_tag=v1.0.0\" \\\n  -var=\"allocated_eip=eipalloc-prod\"\n</code></pre></p>"},{"location":"deployment/#post-deployment-tasks","title":"Post-Deployment Tasks","text":"<p>After deploying the allocator:</p>"},{"location":"deployment/#1-configure-dns-optional","title":"1. Configure DNS (Optional)","text":"<p>Point a custom domain to your allocator for easier access.</p>"},{"location":"deployment/#using-aws-route-53","title":"Using AWS Route 53","text":"<p>Quick Update: <pre><code># Get IP\nALLOCATOR_IP=$(terraform output -raw ec2_public_ip)\n\n# Update Route 53 A record\naws route53 change-resource-record-sets \\\n  --hosted-zone-id Z1234567890ABC \\\n  --change-batch '{\n    \"Changes\": [{\n      \"Action\": \"UPSERT\",\n      \"ResourceRecordSet\": {\n        \"Name\": \"lablink.yourdomain.com\",\n        \"Type\": \"A\",\n        \"TTL\": 300,\n        \"ResourceRecords\": [{\"Value\": \"'$ALLOCATOR_IP'\"}]\n      }\n    }]\n  }'\n</code></pre></p>"},{"location":"deployment/#example-talmo-lab-dns-configuration","title":"Example: Talmo Lab DNS Configuration","text":"<p>The Talmo Lab LabLink deployment uses the <code>sleap.ai</code> domain with environment-specific subdomains:</p> Environment Subdomain IP Address Purpose Production <code>lablink.sleap.ai</code> <code>44.247.165.126</code> Production allocator Test <code>test.lablink.sleap.ai</code> <code>100.20.149.17</code> Testing environment Dev <code>dev.lablink.sleap.ai</code> <code>34.208.206.60</code> Development environment <p>DNS Configuration: - Type: A Records - TTL: 300 seconds - Managed via: AWS Route 53 - Name Servers:   - <code>ns-158.awsdns-19.com</code>   - <code>ns-697.awsdns-23.net</code>   - <code>ns-1839.awsdns-37.co.uk</code>   - <code>ns-1029.awsdns-00.org</code></p> <p>To replicate this setup:</p> <ol> <li> <p>Create hosted zone in Route 53:    <pre><code>aws route53 create-hosted-zone \\\n  --name sleap.ai \\\n  --caller-reference $(date +%s)\n</code></pre></p> </li> <li> <p>Add A records for each environment:    <pre><code># Production\naws route53 change-resource-record-sets \\\n  --hosted-zone-id YOUR_ZONE_ID \\\n  --change-batch '{\n    \"Changes\": [{\n      \"Action\": \"UPSERT\",\n      \"ResourceRecordSet\": {\n        \"Name\": \"lablink.sleap.ai\",\n        \"Type\": \"A\",\n        \"TTL\": 300,\n        \"ResourceRecords\": [{\"Value\": \"44.247.165.126\"}]\n      }\n    }]\n  }'\n\n# Test environment\naws route53 change-resource-record-sets \\\n  --hosted-zone-id YOUR_ZONE_ID \\\n  --change-batch '{\n    \"Changes\": [{\n      \"Action\": \"UPSERT\",\n      \"ResourceRecordSet\": {\n        \"Name\": \"test.lablink.sleap.ai\",\n        \"Type\": \"A\",\n        \"TTL\": 300,\n        \"ResourceRecords\": [{\"Value\": \"100.20.149.17\"}]\n      }\n    }]\n  }'\n\n# Dev environment\naws route53 change-resource-record-sets \\\n  --hosted-zone-id YOUR_ZONE_ID \\\n  --change-batch '{\n    \"Changes\": [{\n      \"Action\": \"UPSERT\",\n      \"ResourceRecordSet\": {\n        \"Name\": \"dev.lablink.sleap.ai\",\n        \"Type\": \"A\",\n        \"TTL\": 300,\n        \"ResourceRecords\": [{\"Value\": \"34.208.206.60\"}]\n      }\n    }]\n  }'\n</code></pre></p> </li> <li> <p>Verify DNS propagation:    <pre><code># Check if DNS is resolving\nnslookup lablink.sleap.ai\ndig lablink.sleap.ai\n\n# Test all environments\ncurl http://lablink.sleap.ai\ncurl http://test.lablink.sleap.ai\ncurl http://dev.lablink.sleap.ai\n</code></pre></p> </li> </ol> <p>DNS Best Practices</p> <ul> <li>Use environment-specific subdomains (e.g., <code>prod.</code>, <code>test.</code>, <code>dev.</code>)</li> <li>Keep TTL low (300s) for easier updates during initial setup</li> <li>Increase TTL (3600s+) once stable to reduce DNS query costs</li> <li>Use Elastic IPs for production to avoid DNS updates on instance replacement</li> </ul>"},{"location":"deployment/#2-change-default-passwords","title":"2. Change Default Passwords","text":"<p>Security Critical</p> <p>Change default passwords before creating any VMs!</p> <p>SSH into allocator and update configuration: <pre><code>ssh -i ~/lablink-key.pem ubuntu@$ALLOCATOR_IP\nsudo docker exec -it &lt;container&gt; bash\n# Edit config and restart container\n</code></pre></p> <p>See Security \u2192 Change Default Passwords.</p>"},{"location":"deployment/#3-test-vm-creation","title":"3. Test VM Creation","text":"<p>Via web interface: 1. Navigate to <code>http://&lt;allocator-ip&gt;:80</code> 2. Login with admin credentials 3. Go to Admin \u2192 Create Instances 4. Enter number of VMs to create 5. Submit and monitor creation</p> <p>Via API: <pre><code>curl -X POST http://&lt;allocator-ip&gt;:80/request_vm \\\n  -d \"email=test@example.com\" \\\n  -d \"crd_command=test_command\"\n</code></pre></p>"},{"location":"deployment/#4-monitor-logs","title":"4. Monitor Logs","text":"<pre><code># SSH into allocator\nssh -i ~/lablink-key.pem ubuntu@$ALLOCATOR_IP\n\n# Check Docker containers\nsudo docker ps\n\n# View allocator logs\nsudo docker logs &lt;allocator-container-id&gt;\n\n# View PostgreSQL logs\nsudo docker exec -it &lt;allocator-container-id&gt; \\\n  tail -f /var/log/postgresql/postgresql-13-main.log\n</code></pre>"},{"location":"deployment/#updating-a-deployment","title":"Updating a Deployment","text":"<p>To update an existing deployment with new configuration or image:</p> <pre><code># Pull latest code\ngit pull origin main\n\n# Re-initialize if needed\nterraform init -reconfigure\n\n# Plan changes\nterraform plan \\\n  -var=\"resource_suffix=dev\" \\\n  -var=\"allocator_image_tag=new-version\"\n\n# Apply changes\nterraform apply \\\n  -var=\"resource_suffix=dev\" \\\n  -var=\"allocator_image_tag=new-version\"\n</code></pre> <p>Note: Changing the image tag will replace the EC2 instance.</p>"},{"location":"deployment/#destroying-a-deployment","title":"Destroying a Deployment","text":""},{"location":"deployment/#via-github-actions","title":"Via GitHub Actions","text":"<p>Use the destroy workflow:</p> <ol> <li>Go to Actions \u2192 Allocator Master Destroy</li> <li>Click Run workflow</li> <li>Select environment</li> <li>Confirm destruction</li> </ol>"},{"location":"deployment/#via-terraform-cli","title":"Via Terraform CLI","text":"<pre><code>cd lablink-infrastructure\n\nterraform destroy \\\n  -var=\"resource_suffix=dev\" \\\n  -var=\"allocator_image_tag=dummy\"  # Still required\n\n# Type 'yes' to confirm\n</code></pre> <p>Warning: This destroys: - EC2 instance - Security group - SSH key pair - All associated resources</p> <p>Not destroyed: - S3 bucket (Terraform state) - Elastic IPs (must be released manually) - Any client VMs created by the allocator</p>"},{"location":"deployment/#troubleshooting-deployments","title":"Troubleshooting Deployments","text":""},{"location":"deployment/#terraform-init-fails","title":"Terraform Init Fails","text":"<p>Error: <code>Backend configuration changed</code></p> <p>Solution: <pre><code>terraform init -reconfigure\n</code></pre></p>"},{"location":"deployment/#apply-fails-resource-already-exists","title":"Apply Fails: Resource Already Exists","text":"<p>Error: <code>Error creating security group: ... already exists</code></p> <p>Solution: Import existing resource or destroy manually: <pre><code>terraform import aws_security_group.lablink sg-xxxxx\n</code></pre></p>"},{"location":"deployment/#ssh-key-not-working","title":"SSH Key Not Working","text":"<p>Error: <code>Permission denied (publickey)</code></p> <p>Check: <pre><code># Verify key permissions\nls -l ~/lablink-key.pem\n# Should show: -rw------- (600)\n\n# Fix permissions\nchmod 600 ~/lablink-key.pem\n</code></pre></p>"},{"location":"deployment/#instance-not-accessible","title":"Instance Not Accessible","text":"<p>Check: 1. Security group allows port 80 from your IP 2. Instance has public IP 3. Instance is running (<code>aws ec2 describe-instances</code>)</p>"},{"location":"deployment/#terraform-state-locked","title":"Terraform State Locked","text":"<p>Error: <code>Error acquiring the state lock</code></p> <p>Solution: <pre><code># If no other Terraform process is running:\nterraform force-unlock &lt;lock-id&gt;\n</code></pre></p>"},{"location":"deployment/#best-practices","title":"Best Practices","text":"<ol> <li>Use version control: Always commit Terraform configs before applying</li> <li>Review plans: Always run <code>terraform plan</code> before <code>apply</code></li> <li>Pin versions: Use specific image tags in production</li> <li>Separate environments: Never share state between dev/test/prod</li> <li>Backup state: Enable S3 versioning for Terraform state</li> <li>Monitor costs: Set up AWS billing alerts</li> <li>Document changes: Use descriptive commit messages</li> </ol>"},{"location":"deployment/#next-steps","title":"Next Steps","text":"<ul> <li>Workflows: Understand the CI/CD pipeline</li> <li>SSH Access: Connect to your deployed instances</li> <li>Database Management: Manage the allocator database</li> <li>Troubleshooting: Fix common deployment issues</li> </ul>"},{"location":"deployment/#cost-management","title":"Cost Management","text":"<p>See Cost Estimation for expected AWS costs and how to monitor spending.</p>"},{"location":"dns-configuration/","title":"DNS Configuration Guide","text":""},{"location":"dns-configuration/#overview","title":"Overview","text":"<p>LabLink uses AWS Route53 for DNS management with a delegated subdomain structure. This document describes the DNS architecture and configuration for internal reference.</p>"},{"location":"dns-configuration/#dns-architecture","title":"DNS Architecture","text":""},{"location":"dns-configuration/#domain-structure","title":"Domain Structure","text":"<ul> <li>Parent Domain: <code>sleap.ai</code> (managed in Cloudflare)</li> <li>Delegated Subdomain: <code>lablink.sleap.ai</code> (managed in AWS Route53)</li> <li>Example Deployment: <code>test.lablink.sleap.ai</code> \u2192 allocator instance</li> </ul>"},{"location":"dns-configuration/#why-delegated-subdomain","title":"Why Delegated Subdomain?","text":"<p>The main <code>sleap.ai</code> domain is managed in Cloudflare for the primary website. To avoid conflicts and allow AWS-based automation, we use NS delegation to hand off the <code>lablink.sleap.ai</code> subdomain to AWS Route53.</p> <p>Benefits: - Terraform can manage DNS records automatically - No Cloudflare API credentials needed - Separation of concerns (website vs LabLink infrastructure) - Let's Encrypt can validate domain ownership via DNS</p>"},{"location":"dns-configuration/#route53-setup","title":"Route53 Setup","text":""},{"location":"dns-configuration/#hosted-zone-configuration","title":"Hosted Zone Configuration","text":"<p>Zone Name: <code>lablink.sleap.ai</code> Zone ID: <code>Z010760118DSWF5IYKMOM</code> Type: Public hosted zone</p> <p>Nameservers (AWS-assigned): <pre><code>ns-158.awsdns-19.com\nns-697.awsdns-23.net\nns-1839.awsdns-37.co.uk\nns-1029.awsdns-00.org\n</code></pre></p>"},{"location":"dns-configuration/#cloudflare-ns-delegation","title":"Cloudflare NS Delegation","text":"<p>In Cloudflare DNS for <code>sleap.ai</code>, the following NS records delegate the subdomain to AWS:</p> <p>Record Type: NS Name: <code>lablink</code> Content (4 records): <pre><code>ns-158.awsdns-19.com\nns-697.awsdns-23.net\nns-1839.awsdns-37.co.uk\nns-1029.awsdns-00.org\n</code></pre></p> <p>TTL: Auto (or 300 seconds)</p>"},{"location":"dns-configuration/#verification","title":"Verification","text":"<pre><code># Query AWS nameservers directly\ndig @ns-158.awsdns-19.com lablink.sleap.ai\n\n# Check NS delegation\ndig NS lablink.sleap.ai\n\n# Should show AWS nameservers, not Cloudflare\n</code></pre>"},{"location":"dns-configuration/#lablink-dns-configuration","title":"LabLink DNS Configuration","text":""},{"location":"dns-configuration/#configuration-file","title":"Configuration File","text":"<p>Location: <code>lablink-infrastructure/config/config.yaml</code></p> <pre><code>dns:\n  enabled: true\n  domain: \"lablink.sleap.ai\"\n  zone_id: \"Z010760118DSWF5IYKMOM\"  # Hardcoded to avoid zone lookup issues\n  app_name: \"\"                       # Empty when using custom pattern\n  pattern: \"custom\"                  # Use custom subdomain\n  custom_subdomain: \"test\"           # Creates test.lablink.sleap.ai\n  create_zone: false                 # Zone already exists, don't create\n</code></pre>"},{"location":"dns-configuration/#dns-patterns","title":"DNS Patterns","text":"<p>LabLink supports multiple DNS naming patterns:</p>"},{"location":"dns-configuration/#1-custom-pattern-recommended","title":"1. Custom Pattern (Recommended)","text":"<pre><code>pattern: \"custom\"\ncustom_subdomain: \"test\"\n# Result: test.lablink.sleap.ai\n</code></pre>"},{"location":"dns-configuration/#2-auto-pattern-environment-based","title":"2. Auto Pattern (environment-based)","text":"<pre><code>pattern: \"auto\"\n# Result: {environment}.lablink.sleap.ai\n# Examples: dev.lablink.sleap.ai, test.lablink.sleap.ai, prod.lablink.sleap.ai\n</code></pre>"},{"location":"dns-configuration/#3-timestamp-pattern","title":"3. Timestamp Pattern","text":"<pre><code>pattern: \"timestamp\"\n# Result: lablink-20251004-143022.lablink.sleap.ai\n</code></pre>"},{"location":"dns-configuration/#zone-id-configuration","title":"Zone ID Configuration","text":"<p>Why hardcode zone_id?</p> <p>The Terraform data source <code>aws_route53_zone</code> can incorrectly match parent zones when searching by domain name. To ensure the correct zone is always used, we hardcode the zone ID in the config:</p> <pre><code>dns:\n  zone_id: \"Z010760118DSWF5IYKMOM\"  # Forces use of lablink.sleap.ai zone\n</code></pre> <p>How to find your zone ID: <pre><code>aws route53 list-hosted-zones --query \"HostedZones[?Name=='lablink.sleap.ai.'].Id\" --output text\n</code></pre></p>"},{"location":"dns-configuration/#terraform-dns-management","title":"Terraform DNS Management","text":""},{"location":"dns-configuration/#main-configuration","title":"Main Configuration","text":"<p>Location: <code>lablink-infrastructure/main.tf</code></p> <pre><code># DNS configuration from config.yaml\nlocals {\n  dns_enabled          = try(local.config_file.dns.enabled, false)\n  dns_domain           = try(local.config_file.dns.domain, \"\")\n  dns_zone_id          = try(local.config_file.dns.zone_id, \"\")\n  dns_app_name         = try(local.config_file.dns.app_name, \"lablink\")\n  dns_pattern          = try(local.config_file.dns.pattern, \"auto\")\n  dns_custom_subdomain = try(local.config_file.dns.custom_subdomain, \"\")\n  dns_create_zone      = try(local.config_file.dns.create_zone, false)\n}\n\n# Zone selection priority: hardcoded zone_id &gt; create new &gt; lookup existing\nlocals {\n  zone_id = local.dns_enabled ? (\n    local.dns_zone_id != \"\" ? local.dns_zone_id : (\n      local.dns_create_zone ? aws_route53_zone.new[0].zone_id : data.aws_route53_zone.existing[0].zone_id\n    )\n  ) : \"\"\n}\n</code></pre>"},{"location":"dns-configuration/#dns-record-creation","title":"DNS Record Creation","text":"<pre><code>resource \"aws_route53_record\" \"allocator\" {\n  count   = local.dns_enabled ? 1 : 0\n  zone_id = local.zone_id\n  name    = local.fqdn\n  type    = \"A\"\n  ttl     = 300\n  records = [local.allocator_public_ip]\n}\n</code></pre>"},{"location":"dns-configuration/#ssltls-configuration","title":"SSL/TLS Configuration","text":""},{"location":"dns-configuration/#lets-encrypt-integration","title":"Let's Encrypt Integration","text":"<p>LabLink uses Caddy for automatic SSL certificate acquisition from Let's Encrypt.</p> <p>Prerequisites: - Valid DNS record pointing to allocator IP - DNS propagated to public resolvers (Google DNS 8.8.8.8, Cloudflare DNS 1.1.1.1) - Port 80 and 443 accessible</p> <p>Configuration: <pre><code>ssl:\n  provider: \"letsencrypt\"\n  email: \"admin@example.com\"\n</code></pre></p> <p>Caddy Configuration (<code>lablink-infrastructure/user_data.sh</code>): <pre><code>cat &gt; /etc/caddy/Caddyfile &lt;&lt;EOF\n${fqdn} {\n    reverse_proxy localhost:5000\n}\nEOF\n</code></pre></p> <p>Caddy automatically: 1. Requests certificate from Let's Encrypt 2. Validates domain ownership via HTTP-01 challenge 3. Renews certificates before expiration 4. Redirects HTTP to HTTPS</p>"},{"location":"dns-configuration/#ssl-troubleshooting","title":"SSL Troubleshooting","text":"<p>Check Caddy logs: <pre><code>ssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\nsudo journalctl -u caddy -f\n</code></pre></p> <p>Common issues: - DNS not propagated \u2192 Wait 5-10 minutes - Port 80/443 blocked \u2192 Check security group rules - Invalid domain \u2192 Verify DNS record exists</p>"},{"location":"dns-configuration/#dns-troubleshooting","title":"DNS Troubleshooting","text":""},{"location":"dns-configuration/#issue-record-created-in-wrong-zone","title":"Issue: Record Created in Wrong Zone","text":"<p>Symptom: DNS record appears in <code>sleap.ai</code> zone instead of <code>lablink.sleap.ai</code> zone</p> <p>Cause: Terraform data source matched parent zone</p> <p>Solution: Add <code>zone_id</code> to config.yaml: <pre><code>dns:\n  zone_id: \"Z010760118DSWF5IYKMOM\"\n</code></pre></p>"},{"location":"dns-configuration/#issue-dns-not-resolving","title":"Issue: DNS Not Resolving","text":"<p>Check DNS propagation: <pre><code># Check Google DNS\nnslookup test.lablink.sleap.ai 8.8.8.8\n\n# Check Cloudflare DNS\nnslookup test.lablink.sleap.ai 1.1.1.1\n\n# Check authoritative nameservers\nnslookup test.lablink.sleap.ai ns-158.awsdns-19.com\n</code></pre></p> <p>Verify Route53 record: <pre><code>aws route53 list-resource-record-sets \\\n  --hosted-zone-id Z010760118DSWF5IYKMOM \\\n  --query \"ResourceRecordSets[?Name=='test.lablink.sleap.ai.']\"\n</code></pre></p> <p>Check NS delegation: <pre><code>dig NS lablink.sleap.ai\n# Should return AWS nameservers, not Cloudflare\n</code></pre></p>"},{"location":"dns-configuration/#issue-ns-delegation-not-working","title":"Issue: NS Delegation Not Working","text":"<p>Symptom: DNS queries return NXDOMAIN even though record exists in Route53</p> <p>Cause: NS records not properly configured in Cloudflare</p> <p>Solution: 1. Log into Cloudflare 2. Go to DNS settings for <code>sleap.ai</code> 3. Add/verify NS records for <code>lablink</code> pointing to all 4 AWS nameservers 4. Wait 5-15 minutes for propagation</p>"},{"location":"dns-configuration/#issue-multiple-hosted-zones-conflict","title":"Issue: Multiple Hosted Zones Conflict","text":"<p>Symptom: Both <code>sleap.ai</code> and <code>lablink.sleap.ai</code> zones exist in Route53</p> <p>Solution: Delete the parent zone from Route53 if it's managed elsewhere (Cloudflare) <pre><code># List zones\naws route53 list-hosted-zones\n\n# Delete parent zone (if managed in Cloudflare)\naws route53 delete-hosted-zone --id &lt;zone-id&gt;\n</code></pre></p>"},{"location":"dns-configuration/#dns-verification-script","title":"DNS Verification Script","text":"<p>Use the deployment verification script to check DNS:</p> <pre><code>cd lablink-infrastructure\n./verify-deployment.sh test.lablink.sleap.ai 52.40.142.146\n</code></pre> <p>This checks: 1. DNS resolution via Google/Cloudflare DNS 2. HTTP connectivity 3. HTTPS/SSL certificate (if enabled)</p>"},{"location":"dns-configuration/#configuration-templates","title":"Configuration Templates","text":""},{"location":"dns-configuration/#development-environment","title":"Development Environment","text":"<pre><code>dns:\n  enabled: true\n  domain: \"lablink.sleap.ai\"\n  zone_id: \"Z010760118DSWF5IYKMOM\"\n  pattern: \"custom\"\n  custom_subdomain: \"dev\"\n  create_zone: false\n\nssl:\n  provider: \"letsencrypt\"\n  email: \"dev@example.com\"\n</code></pre>"},{"location":"dns-configuration/#test-environment","title":"Test Environment","text":"<pre><code>dns:\n  enabled: true\n  domain: \"lablink.sleap.ai\"\n  zone_id: \"Z010760118DSWF5IYKMOM\"\n  pattern: \"custom\"\n  custom_subdomain: \"test\"\n  create_zone: false\n\nssl:\n  provider: \"letsencrypt\"\n  email: \"test@example.com\"\n</code></pre>"},{"location":"dns-configuration/#production-environment","title":"Production Environment","text":"<pre><code>dns:\n  enabled: true\n  domain: \"lablink.sleap.ai\"\n  zone_id: \"Z010760118DSWF5IYKMOM\"\n  pattern: \"custom\"\n  custom_subdomain: \"app\"  # or just use root: lablink.sleap.ai\n  create_zone: false\n\nssl:\n  provider: \"letsencrypt\"\n  email: \"admin@example.com\"\n</code></pre>"},{"location":"dns-configuration/#ip-only-deployment-no-dns","title":"IP-Only Deployment (No DNS)","text":"<pre><code>dns:\n  enabled: false\n\nssl:\n  provider: \"none\"\n</code></pre>"},{"location":"dns-configuration/#best-practices","title":"Best Practices","text":"<ol> <li>Always hardcode zone_id - Prevents zone lookup issues</li> <li>Use custom pattern - Explicit control over subdomain names</li> <li>Verify NS delegation - Check before first deployment</li> <li>Wait for DNS propagation - Allow 5-15 minutes after changes</li> <li>Test with verification script - Automate DNS/SSL checks</li> <li>Document all changes - Record zone IDs and nameservers</li> </ol>"},{"location":"dns-configuration/#security-considerations","title":"Security Considerations","text":""},{"location":"dns-configuration/#dns-security","title":"DNS Security","text":"<ul> <li>Route53 hosted zones are public by default</li> <li>Use IAM policies to restrict who can modify DNS records</li> <li>Enable CloudTrail logging for DNS changes</li> <li>Consider DNSSEC for additional security (advanced)</li> </ul>"},{"location":"dns-configuration/#ssltls-security","title":"SSL/TLS Security","text":"<ul> <li>Let's Encrypt certificates are valid for 90 days</li> <li>Caddy handles automatic renewal</li> <li>Monitor certificate expiration in Caddy logs</li> <li>Use strong cipher suites (Caddy defaults are secure)</li> </ul>"},{"location":"dns-configuration/#monitoring-and-maintenance","title":"Monitoring and Maintenance","text":""},{"location":"dns-configuration/#regular-checks","title":"Regular Checks","text":"<ul> <li>Verify DNS records exist in correct zone</li> <li>Check SSL certificate expiration</li> <li>Monitor Caddy logs for renewal issues</li> <li>Review Route53 query metrics</li> </ul>"},{"location":"dns-configuration/#backup-information","title":"Backup Information","text":"<p>Keep this information documented: - Zone ID: <code>Z010760118DSWF5IYKMOM</code> - Nameservers: (listed above) - Parent domain registrar: Cloudflare - SSL provider: Let's Encrypt via Caddy</p>"},{"location":"dns-configuration/#references","title":"References","text":"<ul> <li>AWS Route53 Documentation</li> <li>Cloudflare DNS Documentation</li> <li>Let's Encrypt Documentation</li> <li>Caddy Documentation</li> <li>LabLink Configuration Guide</li> <li>LabLink Deployment Guide</li> </ul>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":"<p>Common questions and answers about LabLink.</p>"},{"location":"faq/#general","title":"General","text":""},{"location":"faq/#what-is-lablink","title":"What is LabLink?","text":"<p>LabLink is a dynamic VM allocation and management system designed for computational research. It automates the deployment and management of cloud-based VMs for running research software like SLEAP or custom tools.</p>"},{"location":"faq/#who-is-lablink-for","title":"Who is LabLink for?","text":"<ul> <li>Research labs needing on-demand GPU compute resources</li> <li>Scientists running batch computational workloads</li> <li>Teams training machine learning models on cloud infrastructure</li> <li>Anyone needing automated VM management for containerized software</li> </ul>"},{"location":"faq/#what-cloud-providers-does-lablink-support","title":"What cloud providers does LabLink support?","text":"<p>Currently, LabLink supports AWS (Amazon Web Services) only. The architecture uses AWS-specific services (EC2, S3, IAM).</p>"},{"location":"faq/#is-lablink-free","title":"Is LabLink free?","text":"<p>LabLink itself is open-source and free. However, you'll pay for AWS resources you use (EC2 instances, S3 storage, etc.). See Cost Estimation.</p>"},{"location":"faq/#installation-setup","title":"Installation &amp; Setup","text":""},{"location":"faq/#do-i-need-an-aws-account","title":"Do I need an AWS account?","text":"<p>Yes. LabLink deploys to AWS and requires an AWS account with permissions to create EC2 instances, security groups, and other resources.</p>"},{"location":"faq/#how-long-does-setup-take","title":"How long does setup take?","text":"<ul> <li>Initial AWS setup: 1-2 hours (first time)</li> <li>Local testing: 5-10 minutes</li> <li>First deployment: 10-15 minutes</li> </ul>"},{"location":"faq/#can-i-run-lablink-locally-without-aws","title":"Can I run LabLink locally without AWS?","text":"<p>You can run the allocator locally with Docker for testing, but creating client VMs requires AWS.</p>"},{"location":"faq/#configuration","title":"Configuration","text":""},{"location":"faq/#how-do-i-change-the-gpu-type","title":"How do I change the GPU type?","text":"<p>Edit the allocator configuration:</p> <pre><code># lablink-infrastructure/config/config.yaml\nmachine:\n  machine_type: \"g5.xlarge\"  # Change to desired instance type\n</code></pre> <p>See Configuration \u2192 Machine Type Options for available types.</p>"},{"location":"faq/#how-do-i-use-my-own-research-software","title":"How do I use my own research software?","text":"<ol> <li>Create a Docker image with your software</li> <li>Push to a container registry (e.g., ghcr.io)</li> <li>Update configuration with your image URL</li> <li>Optionally specify your code repository</li> </ol> <p>See Adapting LabLink for detailed guide.</p>"},{"location":"faq/#can-i-use-a-different-aws-region","title":"Can I use a different AWS region?","text":"<p>Yes. Update the region in configuration:</p> <pre><code>app:\n  region: \"us-east-1\"  # Change to your preferred region\n</code></pre> <p>Important: AMI IDs are region-specific. You'll need to find the appropriate AMI for your region.</p>"},{"location":"faq/#how-do-i-change-default-passwords","title":"How do I change default passwords?","text":"<p>Critical for production!</p> <pre><code># In config.yaml\napp:\n  admin_user: \"admin\"\n  admin_password: \"YOUR_SECURE_PASSWORD\"\n\ndb:\n  password: \"YOUR_SECURE_DB_PASSWORD\"\n</code></pre> <p>Or use environment variables. See Security \u2192 Change Default Passwords.</p>"},{"location":"faq/#why-does-my-browser-say-not-secure","title":"Why does my browser say \"Not Secure\"?","text":"<p>You're using staging mode (<code>ssl.staging: true</code>), which serves HTTP only (no encryption). This is expected for testing.</p> <p>To get a secure HTTPS connection with browser padlock, set <code>ssl.staging: false</code> in your configuration and redeploy.</p> <p>See Configuration \u2192 SSL Options.</p>"},{"location":"faq/#why-cant-i-access-the-allocator-in-my-browser","title":"Why can't I access the allocator in my browser?","text":"<p>If your browser cannot connect to <code>http://your-domain.com</code>:</p> <ol> <li>Make sure you explicitly type <code>http://</code> (not <code>https://</code>)</li> <li>Clear your browser's HSTS cache (see Troubleshooting \u2192 Browser HSTS)</li> <li>Try incognito/private browsing mode</li> <li>Try accessing via IP address: <code>http://&lt;allocator-ip&gt;</code></li> </ol>"},{"location":"faq/#should-i-use-staging-or-production-mode","title":"Should I use staging or production mode?","text":"<p>Use staging mode (<code>ssl.staging: true</code>) for:</p> <ul> <li>Initial testing and development</li> <li>Frequent deployments (unlimited)</li> <li>Testing infrastructure changes</li> <li>CI/CD automated tests</li> </ul> <p>Use production mode (<code>ssl.staging: false</code>) for:</p> <ul> <li>Production deployments</li> <li>Internet-accessible allocators</li> <li>Handling sensitive data</li> <li>Long-running deployments</li> </ul> <p>Key difference: Staging = HTTP only (fast, unlimited, no encryption). Production = HTTPS with trusted certificates (secure, rate limited).</p> <p>See Configuration \u2192 Staging vs Production.</p>"},{"location":"faq/#how-many-times-can-i-deploy-with-staging-mode","title":"How many times can I deploy with staging mode?","text":"<p>Unlimited. Staging mode uses HTTP only, so there are no Let's Encrypt rate limits.</p> <p>With production mode, you're limited to 5 duplicate certificates per week.</p>"},{"location":"faq/#can-i-switch-from-staging-to-production-mode","title":"Can I switch from staging to production mode?","text":"<p>Yes. Change the configuration and redeploy:</p> <pre><code>ssl:\n  staging: false\n</code></pre> <p>Then run: <pre><code>terraform apply\n</code></pre></p> <p>The allocator will obtain a trusted Let's Encrypt certificate and start serving HTTPS. You may need to clear your browser's HSTS cache.</p>"},{"location":"faq/#deployment","title":"Deployment","text":""},{"location":"faq/#whats-the-difference-between-dev-test-and-prod-environments","title":"What's the difference between dev, test, and prod environments?","text":"Environment Purpose Image Tags Terraform State dev Local development <code>-test</code> Local file test Staging/pre-prod <code>-test</code> S3 bucket prod Production Pinned versions S3 bucket <p>See Deployment \u2192 Environment-Specific Configurations.</p>"},{"location":"faq/#how-do-i-deploy-to-production","title":"How do I deploy to production?","text":"<ol> <li>Navigate to Actions tab in GitHub</li> <li>Select \"Terraform Deploy\" workflow</li> <li>Click \"Run workflow\"</li> <li>Select <code>prod</code> environment</li> <li>Enter specific image tag (e.g., <code>v1.0.0</code>)</li> <li>Click \"Run workflow\"</li> </ol> <p>Never use <code>:latest</code> in production!</p>"},{"location":"faq/#can-i-deploy-without-github-actions","title":"Can I deploy without GitHub Actions?","text":"<p>Yes, using Terraform CLI:</p> <pre><code>cd lablink-allocator\nterraform init\nterraform apply -var=\"resource_suffix=prod\" -var=\"allocator_image_tag=v1.0.0\"\n</code></pre> <p>See Deployment \u2192 Method 2: Manual Terraform.</p>"},{"location":"faq/#how-do-i-update-an-existing-deployment","title":"How do I update an existing deployment?","text":"<pre><code># Pull latest code\ngit pull\n\n# Re-apply Terraform with new image tag\nterraform apply -var=\"resource_suffix=prod\" -var=\"allocator_image_tag=v1.1.0\"\n</code></pre> <p>This will replace the EC2 instance with the new image.</p>"},{"location":"faq/#operations","title":"Operations","text":""},{"location":"faq/#how-do-i-create-client-vms","title":"How do I create client VMs?","text":"<p>Via Web UI: 1. Navigate to allocator web interface 2. Login with admin credentials 3. Go to Admin \u2192 Create Instances 4. Enter number of VMs 5. Submit</p> <p>Via API: <pre><code>curl -X POST http://&lt;allocator-ip&gt;:80/admin/create \\\n  -u admin:password \\\n  -d \"instance_count=5\"\n</code></pre></p>"},{"location":"faq/#how-do-i-check-vm-status","title":"How do I check VM status?","text":"<p>Via Web UI: - Navigate to Admin \u2192 View Instances</p> <p>Via Database: <pre><code>ssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\nsudo docker exec &lt;container-id&gt; psql -U lablink -d lablink_db -c \"SELECT hostname, status, email FROM vms;\"\n</code></pre></p>"},{"location":"faq/#how-do-i-destroy-a-deployment","title":"How do I destroy a deployment?","text":"<p>Via GitHub Actions: 1. Actions \u2192 \"Allocator Master Destroy\" 2. Run workflow 3. Select environment</p> <p>Via Terraform CLI: <pre><code>terraform destroy -var=\"resource_suffix=dev\"\n</code></pre></p>"},{"location":"faq/#what-happens-if-i-destroy-the-allocator","title":"What happens if I destroy the allocator?","text":"<ul> <li>Allocator EC2 instance terminated</li> <li>Database data lost (unless backed up)</li> <li>Client VMs remain running (must be destroyed separately)</li> </ul> <p>Always backup database before destroying!</p>"},{"location":"faq/#troubleshooting","title":"Troubleshooting","text":""},{"location":"faq/#postgresql-wont-start-after-deployment","title":"PostgreSQL won't start after deployment","text":"<p>Known issue. Solution:</p> <pre><code>ssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\nsudo docker exec -it &lt;container-id&gt; bash\n/etc/init.d/postgresql restart\n</code></pre> <p>See Troubleshooting \u2192 PostgreSQL Issues.</p>"},{"location":"faq/#i-cant-ssh-into-the-instance","title":"I can't SSH into the instance","text":"<p>Check: 1. Key permissions: <code>chmod 600 ~/lablink-key.pem</code> 2. Security group allows port 22 3. Using correct IP address 4. Using correct user (<code>ubuntu</code>)</p> <p>See Troubleshooting \u2192 SSH Access Issues.</p>"},{"location":"faq/#client-vms-arent-being-created","title":"Client VMs aren't being created","text":"<p>Check: 1. AWS credentials configured in allocator 2. Allocator container logs for errors 3. IAM permissions for EC2 operations</p> <p>See Troubleshooting \u2192 VM Spawning Issues.</p>"},{"location":"faq/#im-getting-billed-unexpectedly","title":"I'm getting billed unexpectedly","text":"<ul> <li>Check for running EC2 instances you forgot to terminate</li> <li>Set up billing alerts (see AWS Setup \u2192 Billing Alerts)</li> <li>Review Cost Estimation guide</li> </ul>"},{"location":"faq/#costs","title":"Costs","text":""},{"location":"faq/#how-much-does-lablink-cost-to-run","title":"How much does LabLink cost to run?","text":"<p>AWS infrastructure: - S3 bucket: ~$0.05/month - Elastic IPs: Free while associated</p> <p>Running costs (per hour): - Allocator (t2.micro): $0.0116/hour (~$8.50/month if running 24/7) - Client VM (g4dn.xlarge): $0.526/hour</p> <p>See Cost Estimation for detailed breakdown.</p>"},{"location":"faq/#how-can-i-reduce-costs","title":"How can I reduce costs?","text":"<ol> <li>Terminate VMs when not in use</li> <li>Use Spot Instances for client VMs (up to 90% savings)</li> <li>Use smaller instance types for testing</li> <li>Set up billing alerts to monitor spending</li> <li>Use Reserved Instances for long-running allocators (up to 75% savings)</li> </ol>"},{"location":"faq/#do-i-get-charged-for-stopped-instances","title":"Do I get charged for stopped instances?","text":"<ul> <li>EC2 instances: No compute charges, but EBS storage charges apply</li> <li>Elastic IPs: Free while associated, $0.005/hour if unassociated</li> </ul> <p>Best practice: Terminate (not stop) instances when done.</p>"},{"location":"faq/#advanced","title":"Advanced","text":""},{"location":"faq/#can-i-use-a-custom-ami","title":"Can I use a custom AMI?","text":"<p>Yes. Create an AMI with your software pre-installed:</p> <pre><code># Create AMI from running instance\naws ec2 create-image --instance-id i-xxxxx --name \"my-custom-ami\"\n\n# Use in configuration\nmachine:\n  ami_id: \"ami-your-custom-ami-id\"\n</code></pre> <p>See Adapting LabLink \u2192 Custom AMI.</p>"},{"location":"faq/#can-i-use-rds-instead-of-postgresql-in-docker","title":"Can I use RDS instead of PostgreSQL in Docker?","text":"<p>Yes, for production. See Database \u2192 Migrating to RDS.</p> <p>Benefits: - Automated backups - High availability - Managed updates</p>"},{"location":"faq/#can-i-use-lablink-with-multiple-aws-accounts","title":"Can I use LabLink with multiple AWS accounts?","text":"<p>Yes. Deploy separate instances with different AWS credentials/roles for each account.</p>"},{"location":"faq/#can-i-add-my-own-api-endpoints","title":"Can I add my own API endpoints?","text":"<p>Yes. Edit the allocator service in <code>packages/allocator/src/lablink_allocator/main.py</code>:</p> <pre><code>@app.route('/my-custom-endpoint', methods=['POST'])\ndef my_custom_endpoint():\n    # Your code here\n    return jsonify({'status': 'success'})\n</code></pre> <p>Rebuild the Docker image and redeploy.</p>"},{"location":"faq/#how-do-i-enable-https","title":"How do I enable HTTPS?","text":"<ol> <li>Get SSL certificate (e.g., Let's Encrypt)</li> <li>Configure nginx or use AWS Application Load Balancer</li> <li>Update security groups to allow port 443</li> </ol> <p>See Security \u2192 Encryption in Transit.</p>"},{"location":"faq/#security","title":"Security","text":""},{"location":"faq/#is-it-safe-to-use-default-passwords","title":"Is it safe to use default passwords?","text":"<p>No! Change them immediately for any non-local deployment.</p> <p>See Security \u2192 Change Default Passwords.</p>"},{"location":"faq/#how-are-aws-credentials-stored","title":"How are AWS credentials stored?","text":"<p>For GitHub Actions: OIDC (no stored credentials)</p> <p>For local: AWS credentials file or environment variables</p> <p>Never commit credentials to version control!</p>"},{"location":"faq/#how-are-ssh-keys-managed","title":"How are SSH keys managed?","text":"<ul> <li>Terraform generates unique keys per environment</li> <li>Keys stored in Terraform state</li> <li>GitHub Actions exposes keys as temporary artifacts (1 day expiration)</li> <li>Rotate keys by destroying and recreating infrastructure</li> </ul> <p>See SSH Access \u2192 Key Management.</p>"},{"location":"faq/#contributing","title":"Contributing","text":""},{"location":"faq/#can-i-contribute-to-lablink","title":"Can I contribute to LabLink?","text":"<p>Yes! LabLink is open-source. Contributions welcome:</p> <ol> <li>Fork repository</li> <li>Create feature branch</li> <li>Make changes</li> <li>Add tests</li> <li>Submit pull request</li> </ol>"},{"location":"faq/#how-do-i-report-bugs","title":"How do I report bugs?","text":"<p>Open an issue on GitHub with: - Description of the bug - Steps to reproduce - Expected vs actual behavior - Logs/error messages - Environment details</p>"},{"location":"faq/#where-can-i-ask-questions","title":"Where can I ask questions?","text":"<ul> <li>GitHub Issues: For bugs and feature requests</li> <li>GitHub Discussions: For questions and general discussion</li> </ul>"},{"location":"faq/#comparison","title":"Comparison","text":""},{"location":"faq/#how-is-lablink-different-from-aws-batch","title":"How is LabLink different from AWS Batch?","text":"Feature LabLink AWS Batch Setup complexity Moderate High Custom software Easy (Docker) Easy (Docker) GPU support Yes Yes Cost Pay for VMs Pay for VMs + Batch overhead Web UI Included Requires building VM management Automated Automated Learning curve Moderate Steep <p>LabLink advantage: Simpler setup, included web UI, research-focused</p>"},{"location":"faq/#how-is-lablink-different-from-kubernetes","title":"How is LabLink different from Kubernetes?","text":"<p>LabLink is simpler and more focused:</p> <ul> <li>LabLink: VM allocation for research workloads</li> <li>Kubernetes: General-purpose container orchestration</li> </ul> <p>If you need simple VM management for research, use LabLink. If you need complex microservices orchestration, use Kubernetes.</p>"},{"location":"faq/#still-have-questions","title":"Still Have Questions?","text":"<ul> <li>Check Documentation</li> <li>Search GitHub Issues</li> <li>Open new issue with your question</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker 20.10+</li> <li>Python 3.9+ (for development)</li> <li>8GB RAM, 10GB disk space</li> </ul>"},{"location":"installation/#install-via-docker","title":"Install via Docker","text":"AllocatorClient <pre><code>docker pull ghcr.io/talmolab/lablink-allocator-image:latest\ndocker run -d -p 5000:5000 --name lablink-allocator \\\n  ghcr.io/talmolab/lablink-allocator-image:latest\n</code></pre> <p>Access at http://localhost:5000</p> <pre><code>docker pull ghcr.io/talmolab/lablink-client-base-image:latest\ndocker run -d --name lablink-client \\\n  -e ALLOCATOR_HOST=&lt;allocator_ip&gt; \\\n  -e ALLOCATOR_PORT=80 \\\n  ghcr.io/talmolab/lablink-client-base-image:latest\n</code></pre>"},{"location":"installation/#install-from-source","title":"Install from source","text":"CloneBuild allocatorBuild client <pre><code>git clone https://github.com/talmolab/lablink.git\ncd lablink\n</code></pre> <pre><code>docker build -t lablink-allocator \\\n  -f packages/allocator/Dockerfile .\ndocker run -d -p 5000:5000 lablink-allocator\n</code></pre> <pre><code>docker build -t lablink-client \\\n  -f packages/client/Dockerfile .\n</code></pre>"},{"location":"installation/#python-development","title":"Python development","text":"Install uvAllocatorClient <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <pre><code>cd packages/allocator\nuv sync\nuv run lablink-allocator\n</code></pre> <pre><code>cd packages/client\nuv sync\nuv run subscribe\n</code></pre>"},{"location":"installation/#configuration","title":"Configuration","text":"<p>See Configuration for environment variables and config files.</p>"},{"location":"installation/#next-steps","title":"Next steps","text":"<ul> <li>Quickstart - Deploy to AWS</li> <li>Configuration - Customize settings</li> <li>Troubleshooting - Common issues</li> </ul>"},{"location":"prerequisites/","title":"Prerequisites","text":"<p>Before deploying LabLink, you'll need to set up several tools and accounts. This guide covers everything you need to get started.</p>"},{"location":"prerequisites/#required-tools","title":"Required Tools","text":""},{"location":"prerequisites/#1-aws-account","title":"1. AWS Account","text":"<p>You'll need an AWS account with appropriate permissions to create:</p> <ul> <li>EC2 instances</li> <li>Security groups</li> <li>Elastic IPs</li> <li>S3 buckets (for Terraform state)</li> <li>IAM roles and policies</li> </ul> <p>Cost Considerations: See the Cost Estimation guide for expected AWS costs.</p>"},{"location":"prerequisites/#2-aws-cli","title":"2. AWS CLI","text":"<p>Install the AWS Command Line Interface:</p> macOSLinuxWindows <pre><code>brew install awscli\n</code></pre> <pre><code>curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install\n</code></pre> <p>Download and run the AWS CLI MSI installer</p> <p>Verify installation: <pre><code>aws --version\n</code></pre></p>"},{"location":"prerequisites/#configure-aws-credentials","title":"Configure AWS Credentials","text":"<p>You have two options:</p> <p>Option 1: AWS Access Keys (Local Development) <pre><code>aws configure\n</code></pre></p> <p>Enter your: - AWS Access Key ID - AWS Secret Access Key - Default region (e.g., <code>us-west-2</code>) - Default output format (<code>json</code>)</p> <p>Option 2: OIDC (GitHub Actions)</p> <p>For automated deployments, you'll configure OpenID Connect (OIDC) to allow GitHub Actions to assume an IAM role without storing credentials. See AWS Setup from Scratch for details.</p>"},{"location":"prerequisites/#3-terraform","title":"3. Terraform","text":"<p>Install Terraform for infrastructure provisioning:</p> macOSLinuxWindows <pre><code>brew tap hashicorp/tap\nbrew install hashicorp/tap/terraform\n</code></pre> <pre><code>wget https://releases.hashicorp.com/terraform/1.6.6/terraform_1.6.6_linux_amd64.zip\nunzip terraform_1.6.6_linux_amd64.zip\nsudo mv terraform /usr/local/bin/\n</code></pre> <p>Download from Terraform Downloads and add to PATH</p> <p>Verify installation: <pre><code>terraform version\n</code></pre></p> <p>Version Requirement: LabLink uses Terraform 1.6.6 (as specified in the CI workflow).</p>"},{"location":"prerequisites/#4-docker","title":"4. Docker","text":"<p>Install Docker for local testing and development:</p> macOSLinuxWindows <p>Download Docker Desktop for Mac</p> <pre><code>curl -fsSL https://get.docker.com -o get-docker.sh\nsudo sh get-docker.sh\n</code></pre> <p>Download Docker Desktop for Windows</p> <p>Verify installation: <pre><code>docker --version\ndocker ps\n</code></pre></p> <p>Docker Permissions (Linux)</p> <p>If you encounter permission errors: <pre><code>sudo usermod -aG docker $USER\nnewgrp docker\n</code></pre></p>"},{"location":"prerequisites/#5-git","title":"5. Git","text":"<p>Git should already be installed on most systems. Verify: <pre><code>git --version\n</code></pre></p> <p>If not installed:</p> macOSLinuxWindows <pre><code>brew install git\n</code></pre> <pre><code>sudo apt-get install git  # Debian/Ubuntu\nsudo yum install git      # RHEL/CentOS\n</code></pre> <p>Download from git-scm.com</p>"},{"location":"prerequisites/#optional-tools","title":"Optional Tools","text":""},{"location":"prerequisites/#github-cli-gh","title":"GitHub CLI (gh)","text":"<p>Useful for managing releases and workflows:</p> <pre><code># macOS\nbrew install gh\n\n# Linux\nsudo apt install gh\n\n# Windows\nwinget install GitHub.cli\n</code></pre>"},{"location":"prerequisites/#python-and-uv","title":"Python and uv","text":"<p>If you want to run the services locally or contribute to development:</p> <pre><code># Install uv (recommended Python package manager)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Install Python 3.9+\nuv python install 3.11\n</code></pre>"},{"location":"prerequisites/#aws-resource-requirements","title":"AWS Resource Requirements","text":"<p>Before deploying, ensure you have or will create:</p> <ol> <li>S3 Bucket: For Terraform state storage</li> <li>Naming: <code>tf-state-lablink-allocator-bucket</code> (configurable)</li> <li> <p>Versioning enabled recommended</p> </li> <li> <p>Elastic IPs: Pre-allocated for each environment</p> </li> <li>1 for dev</li> <li>1 for test</li> <li> <p>1 for prod</p> </li> <li> <p>IAM Roles: For OIDC authentication (GitHub Actions)</p> </li> <li>Trust relationship with GitHub</li> <li> <p>Permissions for EC2, S3, Route53</p> </li> <li> <p>Route 53 Hosted Zone (Optional): For custom DNS</p> </li> <li>Example: <code>lablink.yourdomain.com</code></li> </ol> <p>See the AWS Setup from Scratch guide for detailed setup instructions.</p>"},{"location":"prerequisites/#ssh-key-pair","title":"SSH Key Pair","text":"<p>LabLink automatically generates SSH key pairs via Terraform, but you should be familiar with:</p> <ul> <li>SSH key management</li> <li>File permissions (chmod 600)</li> <li>Connecting to EC2 instances via SSH</li> </ul>"},{"location":"prerequisites/#next-steps","title":"Next Steps","text":"<p>Once you have these prerequisites installed:</p> <ol> <li>Installation: Set up LabLink locally</li> <li>AWS Setup: Configure AWS resources from scratch</li> <li>Configuration: Customize LabLink for your needs</li> </ol>"},{"location":"quickstart/","title":"Quickstart","text":"<p>Get LabLink running in 15 minutes.</p>"},{"location":"quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>AWS Account with admin access (setup guide)</li> <li>AWS CLI configured locally (setup guide)</li> <li>Terraform installed (setup guide)</li> </ul>"},{"location":"quickstart/#step-1-clone-template-repository","title":"Step 1: Clone Template Repository","text":"<pre><code>git clone https://github.com/talmolab/lablink-template.git\ncd lablink-template/lablink-infrastructure\n</code></pre>"},{"location":"quickstart/#step-2-configure-settings","title":"Step 2: Configure Settings","text":"<p>Edit <code>config/config.yaml</code>:</p> <pre><code># Minimal configuration for quick start\ndns:\n  enabled: false  # Start without DNS, use IP address\n\nssl:\n  provider: \"none\"  # Start without HTTPS\n\nmachine:\n  ami_id: \"ami-067cc81f948e50e06\"  # Ubuntu 22.04 LTS (us-west-2)\n  machine_type: \"t3.medium\"\n  gpu_support: false\n</code></pre> <p>Note on SSL: This configuration uses <code>provider: \"none\"</code> for simplicity. For testing with DNS, you can use: <pre><code>ssl:\n  provider: \"letsencrypt\"\n  email: \"your-email@example.com\"\n  staging: true  # HTTP only, unlimited deployments\n</code></pre></p> <p>Staging mode serves HTTP only. Your browser will show \"Not Secure\" - this is expected for testing. For production with HTTPS, set <code>staging: false</code>. See Configuration - SSL Options.</p>"},{"location":"quickstart/#step-3-initialize-and-deploy","title":"Step 3: Initialize and Deploy","text":"<pre><code># Initialize Terraform\nterraform init\n\n# Deploy (will prompt for confirmation)\nterraform apply\n</code></pre> <p>Deployment time: ~5 minutes</p> <p>Creates: - EC2 instance running allocator (Flask app + PostgreSQL) - Security groups (HTTP port 80, SSH port 22) - SSH key pair</p>"},{"location":"quickstart/#step-4-access-your-allocator","title":"Step 4: Access Your Allocator","text":"<pre><code># Get public IP\nterraform output ec2_public_ip\n# Output: 52.10.123.456\n\n# Save SSH key\nterraform output -raw private_key_pem &gt; ~/lablink-key.pem\nchmod 600 ~/lablink-key.pem\n</code></pre> <p>Web interface: <code>http://&lt;ec2_public_ip&gt;</code></p> <p>Admin login: - Username: <code>admin</code> - Password: Set via <code>ADMIN_PASSWORD</code> in config (placeholder must be replaced)</p> <p>Configure Password</p> <p>For GitHub Actions deployments, set the <code>ADMIN_PASSWORD</code> secret. For local deployments, manually replace <code>PLACEHOLDER_ADMIN_PASSWORD</code> in <code>config/config.yaml</code>. See Configuration.</p>"},{"location":"quickstart/#step-5-create-client-vms","title":"Step 5: Create Client VMs","text":"<ol> <li>Navigate to <code>http://&lt;ec2_public_ip&gt;/admin</code></li> <li>Log in with admin credentials</li> <li>Click \"Create VMs\"</li> <li>Enter number of VMs (try 2)</li> <li>Click \"Launch VMs\"</li> </ol> <p>VM creation time: ~5 minutes</p>"},{"location":"quickstart/#step-6-verify","title":"Step 6: Verify","text":"<pre><code># SSH into allocator\nssh -i ~/lablink-key.pem ubuntu@&lt;ec2_public_ip&gt;\n\n# Check allocator is running\nsudo docker ps\n\n# Check VMs in database\nsudo docker exec $(sudo docker ps -q) psql -U lablink -d lablink_db -c \"SELECT hostname FROM vms;\"\n</code></pre>"},{"location":"quickstart/#step-7-cleanup","title":"Step 7: Cleanup","text":"<pre><code># Destroy all resources\nterraform destroy\n</code></pre> <p>AWS Costs</p> <p>EC2 instances cost ~$0.04/hour (t3.medium). Always destroy test resources.</p>"},{"location":"quickstart/#troubleshooting","title":"Troubleshooting","text":"<p>Can't access web interface? <pre><code>curl http://$(terraform output -raw ec2_public_ip)\n# If this fails, check security group allows port 80\n</code></pre></p> <p>VMs not appearing? See VM Registration Issue</p> <p>SSH permission denied? <pre><code>chmod 600 ~/lablink-key.pem\n</code></pre></p>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Add DNS: DNS Configuration Guide for custom domains and HTTPS</li> <li>Production: Deployment Guide for GitHub Actions and best practices</li> <li>Customize: Configuration Reference for all options</li> <li>Secure: Security Guide before going to production</li> </ul>"},{"location":"security/","title":"Security","text":"<p>This guide covers security considerations, best practices, and how to secure your LabLink deployment.</p>"},{"location":"security/#security-overview","title":"Security Overview","text":"<p>LabLink implements multiple security layers:</p> <ul> <li>Authentication: Admin interface password protection</li> <li>Authorization: OIDC for GitHub Actions, IAM roles for AWS</li> <li>Encryption: HTTPS (optional), encrypted Terraform state</li> <li>Network: Security groups restrict access</li> <li>Secrets: Environment variables, AWS Secrets Manager</li> </ul>"},{"location":"security/#threat-model","title":"Threat Model","text":""},{"location":"security/#assets-to-protect","title":"Assets to Protect","text":"<ol> <li>Allocator Server: Controls infrastructure</li> <li>Client VMs: Run research workloads</li> <li>Database: Contains VM assignments and user data</li> <li>AWS Credentials: Access to cloud resources</li> <li>SSH Keys: Access to EC2 instances</li> <li>Admin Credentials: Access to allocator interface</li> </ol>"},{"location":"security/#potential-threats","title":"Potential Threats","text":"Threat Impact Mitigation Unauthorized admin access Full system control Strong passwords, HTTPS, IP restrictions AWS credential exposure Unauthorized infrastructure changes OIDC (no stored credentials), IAM policies SSH key leakage Direct server access Ephemeral keys, proper permissions (600) Database access Data exposure, manipulation Firewall rules, strong passwords Man-in-the-middle Credential theft, data interception HTTPS, VPC isolation Resource exhaustion Denial of service, high costs Billing alerts, resource limits"},{"location":"security/#authentication-authorization","title":"Authentication &amp; Authorization","text":""},{"location":"security/#change-default-passwords","title":"Change Default Passwords","text":"<p>Critical: Change default passwords before deployment!</p>"},{"location":"security/#allocator-admin-password","title":"Allocator Admin Password","text":"<p>Default: Configuration files use <code>PLACEHOLDER_ADMIN_PASSWORD</code> which must be replaced with a secure password.</p> <p>Method 1: GitHub Secrets (Recommended for CI/CD)</p> <p>For GitHub Actions deployments, add the <code>ADMIN_PASSWORD</code> secret to your repository:</p> <ol> <li>Go to repository Settings \u2192 Secrets and variables \u2192 Actions</li> <li>Click New repository secret</li> <li>Name: <code>ADMIN_PASSWORD</code></li> <li>Value: Your secure password</li> <li>Click Add secret</li> </ol> <p>The deployment workflow automatically injects this secret into configuration files before Terraform apply, preventing passwords from appearing in logs.</p> <p>Method 2: Manual configuration</p> <p>Edit <code>lablink-infrastructure/config/config.yaml</code>: <pre><code>app:\n  admin_user: \"admin\"\n  admin_password: \"YOUR_SECURE_PASSWORD_HERE\"\n</code></pre></p> <p>Method 3: Environment variable</p> <pre><code>export ADMIN_PASSWORD=\"your_secure_password\"\n\n# Docker\ndocker run -d \\\n  -e ADMIN_PASSWORD=\"your_secure_password\" \\\n  -p 5000:5000 \\\n  ghcr.io/talmolab/lablink-allocator-image:latest\n</code></pre> <p>Password requirements: - Minimum 12 characters - Mix of uppercase, lowercase, numbers, symbols - Not a dictionary word - Use a password manager</p>"},{"location":"security/#database-password","title":"Database Password","text":"<p>Default: Configuration files use <code>PLACEHOLDER_DB_PASSWORD</code> which must be replaced with a secure password.</p> <p>Method 1: GitHub Secrets (Recommended for CI/CD)</p> <p>For GitHub Actions deployments, add the <code>DB_PASSWORD</code> secret to your repository:</p> <ol> <li>Go to repository Settings \u2192 Secrets and variables \u2192 Actions</li> <li>Click New repository secret</li> <li>Name: <code>DB_PASSWORD</code></li> <li>Value: Your secure database password</li> <li>Click Add secret</li> </ol> <p>The deployment workflow automatically injects this secret into configuration files before Terraform apply, preventing passwords from appearing in logs.</p> <p>Method 2: Manual configuration</p> <p>Edit <code>lablink-infrastructure/config/config.yaml</code>: <pre><code>db:\n  user: \"lablink\"\n  password: \"YOUR_SECURE_DB_PASSWORD_HERE\"\n</code></pre></p> <p>Method 3: Environment variable</p> <pre><code>export DB_PASSWORD=\"your_secure_db_password\"\n</code></pre> <p>Method 4: AWS Secrets Manager (Advanced)</p> <pre><code># Store in Secrets Manager\naws secretsmanager create-secret \\\n  --name lablink/db-password \\\n  --secret-string \"your-secure-db-password\"\n\n# Retrieve in application\nimport boto3\nclient = boto3.client('secretsmanager', region_name='us-west-2')\nresponse = client.get_secret_value(SecretId='lablink/db-password')\ndb_password = response['SecretString']\n</code></pre>"},{"location":"security/#oidc-for-github-actions","title":"OIDC for GitHub Actions","text":"<p>OpenID Connect (OIDC) allows GitHub Actions to authenticate to AWS without storing credentials.</p>"},{"location":"security/#how-it-works","title":"How It Works","text":"<pre><code>1. GitHub Action requests token from GitHub OIDC provider\n2. GitHub issues short-lived token with repository info\n3. Action presents token to AWS STS\n4. AWS validates token against IAM role trust policy\n5. AWS issues temporary AWS credentials\n6. Action uses credentials for Terraform operations\n7. Credentials expire automatically\n</code></pre>"},{"location":"security/#benefits","title":"Benefits","text":"<ul> <li>No stored credentials: Nothing to leak or rotate</li> <li>Short-lived: Credentials expire quickly</li> <li>Scoped: Permissions limited to specific role</li> <li>Auditable: CloudTrail logs all API calls</li> </ul>"},{"location":"security/#trust-policy","title":"Trust Policy","text":"<p>The IAM role trust policy restricts which repositories can assume the role:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Federated\": \"arn:aws:iam::ACCOUNT_ID:oidc-provider/token.actions.githubusercontent.com\"\n      },\n      \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"token.actions.githubusercontent.com:aud\": \"sts.amazonaws.com\"\n        },\n        \"StringLike\": {\n          \"token.actions.githubusercontent.com:sub\": \"repo:talmolab/lablink:*\"\n        }\n      }\n    }\n  ]\n}\n</code></pre> <p>Key: <code>token.actions.githubusercontent.com:sub</code> restricts to specific repository.</p>"},{"location":"security/#setup","title":"Setup","text":"<p>See AWS Setup \u2192 OIDC Configuration.</p>"},{"location":"security/#iam-role-permissions","title":"IAM Role Permissions","text":"<p>Follow principle of least privilege.</p> <p>Minimal permissions for LabLink deployment:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:RunInstances\",\n        \"ec2:TerminateInstances\",\n        \"ec2:DescribeInstances\",\n        \"ec2:CreateSecurityGroup\",\n        \"ec2:DeleteSecurityGroup\",\n        \"ec2:AuthorizeSecurityGroupIngress\",\n        \"ec2:RevokeSecurityGroupIngress\",\n        \"ec2:CreateKeyPair\",\n        \"ec2:DeleteKeyPair\",\n        \"ec2:DescribeKeyPairs\",\n        \"ec2:AllocateAddress\",\n        \"ec2:AssociateAddress\",\n        \"ec2:DescribeAddresses\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n        \"s3:DeleteObject\",\n        \"s3:ListBucket\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::tf-state-lablink-*\",\n        \"arn:aws:s3:::tf-state-lablink-*/*\"\n      ]\n    }\n  ]\n}\n</code></pre> <p>Restrict by tags (advanced):</p> <pre><code>{\n  \"Effect\": \"Allow\",\n  \"Action\": \"ec2:*\",\n  \"Resource\": \"*\",\n  \"Condition\": {\n    \"StringEquals\": {\n      \"ec2:ResourceTag/Project\": \"lablink\"\n    }\n  }\n}\n</code></pre>"},{"location":"security/#network-security","title":"Network Security","text":""},{"location":"security/#security-groups","title":"Security Groups","text":"<p>LabLink creates security groups for allocator and client VMs.</p>"},{"location":"security/#allocator-security-group","title":"Allocator Security Group","text":"<p>Inbound Rules:</p> Port Protocol Source Purpose 80 TCP 0.0.0.0/0 HTTP web interface 22 TCP 0.0.0.0/0 SSH access 5432 TCP VPC CIDR PostgreSQL (internal) <p>Recommendations:</p> <ol> <li> <p>Restrict SSH: Change source from <code>0.0.0.0/0</code> to your IP:    <pre><code>YOUR_IP=$(curl -s ifconfig.me)\naws ec2 authorize-security-group-ingress \\\n  --group-id sg-xxxxx \\\n  --protocol tcp \\\n  --port 22 \\\n  --cidr $YOUR_IP/32\n</code></pre></p> </li> <li> <p>Enable HTTPS: Use port 443 instead of 80 with SSL certificate:    <pre><code># Install certbot on allocator\nsudo certbot --nginx -d lablink.yourdomain.com\n</code></pre></p> </li> <li> <p>Restrict HTTP: Limit to known client IPs if possible</p> </li> </ol>"},{"location":"security/#client-vm-security-group","title":"Client VM Security Group","text":"<p>Inbound Rules:</p> Port Protocol Source Purpose 22 TCP Your IP SSH access <p>Outbound Rules:</p> Port Protocol Destination Purpose All All 0.0.0.0/0 Internet access (packages, GitHub) <p>Recommendations:</p> <ol> <li>Restrict outbound: If possible, limit to specific destinations:</li> <li>Package repos (apt, pip)</li> <li>GitHub</li> <li> <p>Allocator IP</p> </li> <li> <p>VPC Endpoints: Use VPC endpoints for AWS services (S3, EC2) to avoid internet routing</p> </li> </ol>"},{"location":"security/#vpc-configuration","title":"VPC Configuration","text":"<p>For production, use a dedicated VPC:</p> <pre><code>resource \"aws_vpc\" \"lablink\" {\n  cidr_block           = \"10.0.0.0/16\"\n  enable_dns_hostnames = true\n  enable_dns_support   = true\n\n  tags = {\n    Name = \"lablink-vpc\"\n  }\n}\n\nresource \"aws_subnet\" \"public\" {\n  vpc_id                  = aws_vpc.lablink.id\n  cidr_block              = \"10.0.1.0/24\"\n  availability_zone       = \"us-west-2a\"\n  map_public_ip_on_launch = true\n\n  tags = {\n    Name = \"lablink-public-subnet\"\n  }\n}\n\nresource \"aws_subnet\" \"private\" {\n  vpc_id            = aws_vpc.lablink.id\n  cidr_block        = \"10.0.2.0/24\"\n  availability_zone = \"us-west-2a\"\n\n  tags = {\n    Name = \"lablink-private-subnet\"\n  }\n}\n</code></pre> <p>Benefits: - Isolation from other workloads - Custom network ACLs - VPC Flow Logs for monitoring</p>"},{"location":"security/#staging-mode-security","title":"Staging Mode Security","text":"<p>Warning: Staging mode (<code>ssl.staging: true</code>) serves unencrypted HTTP traffic. All data transmitted between users and the allocator is sent in plaintext.</p>"},{"location":"security/#data-exposed-in-staging-mode","title":"Data Exposed in Staging Mode","text":"<p>When using staging mode, the following information is transmitted unencrypted:</p> <ul> <li>Admin usernames and passwords</li> <li>Database credentials</li> <li>VM allocation requests</li> <li>Research data filenames and metadata</li> <li>SSH keys and access tokens</li> <li>All HTTP request/response data</li> </ul>"},{"location":"security/#when-staging-mode-is-acceptable","title":"When Staging Mode is Acceptable","text":"<p>Use staging mode only when:</p> <ul> <li>Testing in isolated VPCs with no internet access</li> <li>Accessing via VPN on private networks</li> <li>Local testing on development machines</li> <li>Short-term infrastructure testing (less than 1 hour)</li> <li>Automated CI/CD testing pipelines</li> <li>No sensitive data is involved</li> </ul>"},{"location":"security/#when-production-mode-is-required","title":"When Production Mode is Required","text":"<p>Use production mode (<code>ssl.staging: false</code>) for:</p> <ul> <li>Any internet-accessible deployment</li> <li>Handling sensitive research data</li> <li>Multi-user environments</li> <li>Long-running deployments</li> <li>Production or staging environments</li> <li>Compliance requirements (HIPAA, GDPR, etc.)</li> </ul>"},{"location":"security/#mitigations-for-staging-mode","title":"Mitigations for Staging Mode","text":"<p>If you must use staging mode with potentially sensitive data:</p> <ol> <li> <p>Restrict access to your IP only:    <pre><code># In Terraform security group\ningress {\n  from_port   = 80\n  to_port     = 80\n  protocol    = \"tcp\"\n  cidr_blocks = [\"YOUR_IP/32\"]\n}\n</code></pre></p> </li> <li> <p>Use a VPN - All access through VPN tunnel</p> </li> <li> <p>Deploy in private VPC - No internet gateway</p> </li> <li> <p>Time-limited - Switch to production mode as soon as testing is complete</p> </li> <li> <p>Monitor access - Check CloudWatch logs for unexpected connections</p> </li> </ol>"},{"location":"security/#switching-to-production-mode","title":"Switching to Production Mode","text":"<p>To switch a deployment from staging to production:</p> <ol> <li> <p>Update configuration:    <pre><code>ssl:\n  staging: false\n</code></pre></p> </li> <li> <p>Redeploy:    <pre><code>terraform apply\n</code></pre></p> </li> <li> <p>Wait for Let's Encrypt certificate (30-60 seconds)</p> </li> <li> <p>Access via HTTPS:    <pre><code>https://your-domain.com\n</code></pre></p> </li> <li> <p>Clear browser HSTS cache if you previously accessed via HTTP (see Troubleshooting)</p> </li> </ol>"},{"location":"security/#secrets-management","title":"Secrets Management","text":""},{"location":"security/#environment-variables","title":"Environment Variables","text":"<p>For development:</p> <pre><code>export DB_PASSWORD=\"secure_password\"\nexport ADMIN_PASSWORD=\"secure_admin_password\"\nexport AWS_ACCESS_KEY_ID=\"AKIA...\"\nexport AWS_SECRET_ACCESS_KEY=\"...\"\n</code></pre> <p>Pros: - Simple - No external dependencies</p> <p>Cons: - Visible in process list - Can leak in logs - Not encrypted at rest</p>"},{"location":"security/#aws-secrets-manager","title":"AWS Secrets Manager","text":"<p>For production:</p> <p>Store secrets: <pre><code>aws secretsmanager create-secret \\\n  --name lablink/config \\\n  --secret-string '{\n    \"db_password\": \"secure_db_password\",\n    \"admin_password\": \"secure_admin_password\"\n  }'\n</code></pre></p> <p>Retrieve in application: <pre><code>import boto3\nimport json\n\ndef get_secrets():\n    client = boto3.client('secretsmanager', region_name='us-west-2')\n    response = client.get_secret_value(SecretId='lablink/config')\n    secrets = json.loads(response['SecretString'])\n    return secrets\n\nsecrets = get_secrets()\ndb_password = secrets['db_password']\nadmin_password = secrets['admin_password']\n</code></pre></p> <p>Pros: - Encrypted at rest and in transit - Automatic rotation - Audit logs (CloudTrail) - Versioning</p> <p>Cons: - Additional cost ($0.40/secret/month) - Requires IAM permissions</p>"},{"location":"security/#github-secrets","title":"GitHub Secrets","text":"<p>For CI/CD workflows, GitHub Secrets provide secure password storage.</p> <p>Add secrets: 1. Go to repository Settings \u2192 Secrets and variables \u2192 Actions 2. Click New repository secret 3. Add both required secrets:    - Name: <code>ADMIN_PASSWORD</code>, Value: your secure admin password    - Name: <code>DB_PASSWORD</code>, Value: your secure database password 4. Click Add secret for each</p> <p>How it works:</p> <p>The deployment workflow automatically injects secrets into configuration files before Terraform runs:</p> <pre><code>- name: Inject Password Secrets\n  env:\n    ADMIN_PASSWORD: ${{ secrets.ADMIN_PASSWORD || 'CHANGEME_admin_password' }}\n    DB_PASSWORD: ${{ secrets.DB_PASSWORD || 'CHANGEME_db_password' }}\n  run: |\n    sed -i \"s/PLACEHOLDER_ADMIN_PASSWORD/${ADMIN_PASSWORD}/g\" \"$CONFIG_FILE\"\n    sed -i \"s/PLACEHOLDER_DB_PASSWORD/${DB_PASSWORD}/g\" \"$CONFIG_FILE\"\n</code></pre> <p>This replaces <code>PLACEHOLDER_ADMIN_PASSWORD</code> and <code>PLACEHOLDER_DB_PASSWORD</code> in config files with actual values from secrets, preventing passwords from appearing in Terraform logs.</p> <p>Pros: - Integrated with GitHub Actions - Encrypted at rest and in transit - Not visible in workflow logs - Prevents password exposure in Terraform apply output</p> <p>Cons: - Only available in workflows - Can't be read after creation</p>"},{"location":"security/#ssh-key-security","title":"SSH Key Security","text":""},{"location":"security/#key-generation","title":"Key Generation","text":"<p>Terraform generates SSH keys automatically:</p> <pre><code>resource \"tls_private_key\" \"lablink_key\" {\n  algorithm = \"RSA\"\n  rsa_bits  = 4096\n}\n\nresource \"aws_key_pair\" \"lablink_key_pair\" {\n  key_name   = \"lablink-${var.resource_suffix}-key\"\n  public_key = tls_private_key.lablink_key.public_key_openssh\n}\n</code></pre> <p>Good: - Unique key per environment - 4096-bit RSA (strong)</p> <p>Bad: - Stored in Terraform state (plaintext) - Artifacts expire (GitHub Actions)</p>"},{"location":"security/#key-permissions","title":"Key Permissions","text":"<p>Always set proper permissions:</p> <pre><code>chmod 600 ~/lablink-key.pem\n</code></pre> <p>Why: Prevents SSH from rejecting key: <pre><code>Permissions 0644 for 'lablink-key.pem' are too open.\nIt is required that your private key files are NOT accessible by others.\n</code></pre></p>"},{"location":"security/#key-rotation","title":"Key Rotation","text":"<p>Rotate keys regularly:</p> <pre><code># Destroy and recreate infrastructure\nterraform destroy -var=\"resource_suffix=dev\"\nterraform apply -var=\"resource_suffix=dev\"\n\n# New keys generated automatically\n</code></pre> <p>Frequency: Every 90 days for production</p>"},{"location":"security/#key-storage","title":"Key Storage","text":"<p>Never: - Commit keys to version control - Share keys via email/Slack - Store keys in cloud storage without encryption</p> <p>Instead: - Use SSH agent: <code>ssh-add ~/lablink-key.pem</code> - Store in password manager - Use AWS Systems Manager Session Manager (no SSH needed)</p>"},{"location":"security/#session-manager-alternative-to-ssh","title":"Session Manager (Alternative to SSH)","text":"<p>Use AWS Systems Manager for SSH-less access:</p> <pre><code># Install Session Manager plugin\n# macOS\nbrew install --cask session-manager-plugin\n\n# Start session\naws ssm start-session --target i-xxxxx\n</code></pre> <p>Benefits: - No SSH keys needed - Audit logs in CloudTrail - Fine-grained IAM control</p>"},{"location":"security/#data-encryption","title":"Data Encryption","text":""},{"location":"security/#encryption-at-rest","title":"Encryption at Rest","text":""},{"location":"security/#terraform-state","title":"Terraform State","text":"<p>S3 bucket encryption (AES-256): <pre><code>aws s3api put-bucket-encryption \\\n  --bucket tf-state-lablink \\\n  --server-side-encryption-configuration '{\n    \"Rules\": [{\n      \"ApplyServerSideEncryptionByDefault\": {\n        \"SSEAlgorithm\": \"AES256\"\n      }\n    }]\n  }'\n</code></pre></p>"},{"location":"security/#ebs-volumes","title":"EBS Volumes","text":"<p>Encrypt EC2 instance volumes:</p> <pre><code>resource \"aws_instance\" \"lablink_allocator\" {\n  ami           = var.ami_id\n  instance_type = \"t2.micro\"\n\n  root_block_device {\n    volume_size           = 30\n    volume_type           = \"gp3\"\n    encrypted             = true\n    delete_on_termination = true\n  }\n}\n</code></pre>"},{"location":"security/#database","title":"Database","text":"<p>For RDS (if using external database): <pre><code>resource \"aws_db_instance\" \"lablink\" {\n  storage_encrypted = true\n  kms_key_id        = aws_kms_key.lablink.arn\n}\n</code></pre></p>"},{"location":"security/#encryption-in-transit","title":"Encryption in Transit","text":""},{"location":"security/#https-for-allocator","title":"HTTPS for Allocator","text":"<p>Use Let's Encrypt certificate:</p> <pre><code># SSH into allocator\nssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\n\n# Install certbot\nsudo apt-get update\nsudo apt-get install -y certbot python3-certbot-nginx\n\n# Get certificate\nsudo certbot --nginx -d lablink.yourdomain.com --non-interactive --agree-tos -m your@email.com\n\n# Auto-renewal\nsudo systemctl enable certbot.timer\n</code></pre> <p>Update security group to allow port 443.</p>"},{"location":"security/#postgresql-ssl","title":"PostgreSQL SSL","text":"<p>Enable SSL for database connections:</p> <p><code>postgresql.conf</code>: <pre><code>ssl = on\nssl_cert_file = '/etc/ssl/certs/server.crt'\nssl_key_file = '/etc/ssl/private/server.key'\n</code></pre></p> <p>Client connection: <pre><code>import psycopg2\n\nconn = psycopg2.connect(\n    host=\"allocator-ip\",\n    database=\"lablink_db\",\n    user=\"lablink\",\n    password=\"password\",\n    sslmode=\"require\"  # Force SSL\n)\n</code></pre></p>"},{"location":"security/#monitoring-auditing","title":"Monitoring &amp; Auditing","text":""},{"location":"security/#cloudtrail","title":"CloudTrail","text":"<p>Enable CloudTrail for AWS API auditing:</p> <pre><code>aws cloudtrail create-trail \\\n  --name lablink-trail \\\n  --s3-bucket-name lablink-cloudtrail-logs\n\naws cloudtrail start-logging --name lablink-trail\n</code></pre> <p>Logs include: - EC2 instance launches/terminations - Security group changes - IAM role assumptions - S3 access</p>"},{"location":"security/#vpc-flow-logs","title":"VPC Flow Logs","text":"<p>Monitor network traffic:</p> <pre><code>aws ec2 create-flow-logs \\\n  --resource-type VPC \\\n  --resource-ids vpc-xxxxx \\\n  --traffic-type ALL \\\n  --log-destination-type cloud-watch-logs \\\n  --log-group-name lablink-vpc-flow-logs\n</code></pre>"},{"location":"security/#application-logging","title":"Application Logging","text":"<p>Log security events in application:</p> <pre><code>import logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Log authentication attempts\n@app.route('/admin')\n@requires_auth\ndef admin():\n    logger.info(f\"Admin access by {request.remote_addr}\")\n    return render_template('admin.html')\n\n# Log VM requests\n@app.route('/request_vm', methods=['POST'])\ndef request_vm():\n    logger.info(f\"VM requested by {request.form.get('email')} from {request.remote_addr}\")\n    # ... handle request\n</code></pre>"},{"location":"security/#compliance-best-practices","title":"Compliance &amp; Best Practices","text":""},{"location":"security/#security-checklist","title":"Security Checklist","text":"<ul> <li>[ ] Changed default admin password</li> <li>[ ] Changed default database password</li> <li>[ ] Enabled HTTPS for allocator</li> <li>[ ] Restricted SSH access to known IPs</li> <li>[ ] Enabled S3 bucket encryption</li> <li>[ ] Enabled EBS volume encryption</li> <li>[ ] Set up CloudTrail logging</li> <li>[ ] Set up billing alerts</li> <li>[ ] Rotated SSH keys (if older than 90 days)</li> <li>[ ] Reviewed IAM role permissions</li> <li>[ ] Enabled MFA for AWS account</li> <li>[ ] Set up VPC Flow Logs</li> <li>[ ] Documented security procedures</li> </ul>"},{"location":"security/#regular-security-tasks","title":"Regular Security Tasks","text":"Task Frequency Review CloudTrail logs Weekly Rotate SSH keys Every 90 days Update dependencies Monthly Review security group rules Quarterly Audit IAM permissions Quarterly Penetration testing Annually"},{"location":"security/#incident-response","title":"Incident Response","text":"<p>If security incident occurs:</p> <ol> <li>Isolate: Modify security groups to block traffic</li> <li>Investigate: Review CloudTrail, VPC Flow Logs, application logs</li> <li>Contain: Terminate compromised instances</li> <li>Recover: Deploy from known-good state</li> <li>Learn: Document incident, improve security</li> </ol>"},{"location":"security/#security-resources","title":"Security Resources","text":"<ul> <li>AWS Security Best Practices</li> <li>OWASP Top 10</li> <li>CIS AWS Foundations Benchmark</li> </ul>"},{"location":"security/#next-steps","title":"Next Steps","text":"<ul> <li>AWS Setup: Secure AWS resource configuration</li> <li>Deployment: Secure deployment practices</li> <li>Troubleshooting: Security-related issues</li> </ul>"},{"location":"ssh-access/","title":"SSH Access","text":"<p>This guide covers SSH access to LabLink allocator and client EC2 instances.</p>"},{"location":"ssh-access/#overview","title":"Overview","text":"<p>SSH (Secure Shell) provides secure remote access to EC2 instances for:</p> <ul> <li>Debugging and troubleshooting</li> <li>Log inspection</li> <li>Configuration changes</li> <li>Database access</li> <li>Manual operations</li> </ul>"},{"location":"ssh-access/#ssh-key-management","title":"SSH Key Management","text":""},{"location":"ssh-access/#key-generation","title":"Key Generation","text":"<p>Terraform automatically generates SSH key pairs during deployment:</p> <pre><code>resource \"tls_private_key\" \"lablink_key\" {\n  algorithm = \"RSA\"\n  rsa_bits  = 4096\n}\n\nresource \"aws_key_pair\" \"lablink_key_pair\" {\n  key_name   = \"lablink-${var.resource_suffix}-key\"\n  public_key = tls_private_key.lablink_key.public_key_openssh\n}\n</code></pre> <p>Key characteristics: - Algorithm: RSA - Key size: 4096 bits (strong security) - Format: OpenSSH - Naming: <code>lablink-&lt;environment&gt;-key</code> (e.g., <code>lablink-dev-key</code>, <code>lablink-prod-key</code>)</p>"},{"location":"ssh-access/#retrieving-ssh-keys","title":"Retrieving SSH Keys","text":""},{"location":"ssh-access/#from-terraform-output","title":"From Terraform Output","text":"<p>After deployment:</p> <pre><code>cd lablink-infrastructure\n\n# Display private key\nterraform output -raw private_key_pem\n\n# Save to file\nterraform output -raw private_key_pem &gt; ~/lablink-dev-key.pem\n\n# Set proper permissions\nchmod 600 ~/lablink-dev-key.pem\n</code></pre>"},{"location":"ssh-access/#from-github-actions-artifacts","title":"From GitHub Actions Artifacts","text":"<p>For deployments via GitHub Actions:</p> <ol> <li>Navigate to Actions tab in GitHub</li> <li>Click on the deployment workflow run</li> <li>Scroll to Artifacts section</li> <li>Download <code>lablink-key-&lt;env&gt;</code> artifact</li> <li>Extract <code>lablink-key.pem</code></li> <li>Set permissions: <code>chmod 600 ~/lablink-key.pem</code></li> </ol> <p>Artifact Expiration</p> <p>GitHub Actions artifacts expire after 1 day. Retrieve keys promptly or re-run deployment.</p>"},{"location":"ssh-access/#key-permissions","title":"Key Permissions","text":"<p>Required permissions: <code>600</code> (read/write for owner only)</p> <pre><code># Set correct permissions\nchmod 600 ~/lablink-key.pem\n\n# Verify\nls -l ~/lablink-key.pem\n# Should show: -rw------- (600)\n</code></pre> <p>Why: SSH refuses to use keys with overly permissive permissions: <pre><code>@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@         WARNING: UNPROTECTED PRIVATE KEY FILE!          @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nPermissions 0644 for 'lablink-key.pem' are too open.\n</code></pre></p>"},{"location":"ssh-access/#connecting-to-allocator","title":"Connecting to Allocator","text":""},{"location":"ssh-access/#get-allocator-ip-address","title":"Get Allocator IP Address","text":""},{"location":"ssh-access/#from-terraform-output_1","title":"From Terraform Output","text":"<pre><code>cd lablink-infrastructure\nterraform output ec2_public_ip\n</code></pre>"},{"location":"ssh-access/#from-aws-console","title":"From AWS Console","text":"<ol> <li>Navigate to EC2 \u2192 Instances</li> <li>Find instance tagged <code>lablink-allocator-&lt;env&gt;</code></li> <li>Copy Public IPv4 address</li> </ol>"},{"location":"ssh-access/#from-aws-cli","title":"From AWS CLI","text":"<pre><code>aws ec2 describe-instances \\\n  --filters \"Name=tag:Name,Values=lablink-allocator-dev\" \\\n  --query 'Reservations[0].Instances[0].PublicIpAddress' \\\n  --output text\n</code></pre>"},{"location":"ssh-access/#ssh-command","title":"SSH Command","text":"<pre><code>ssh -i ~/lablink-dev-key.pem ubuntu@&lt;allocator-public-ip&gt;\n</code></pre> <p>Example: <pre><code>ssh -i ~/lablink-dev-key.pem ubuntu@54.123.45.67\n</code></pre></p> <p>Default user: <code>ubuntu</code> (for Ubuntu AMI)</p>"},{"location":"ssh-access/#first-connection","title":"First Connection","text":"<p>On first SSH connection, you'll see:</p> <pre><code>The authenticity of host '54.123.45.67 (54.123.45.67)' can't be established.\nED25519 key fingerprint is SHA256:xxxxx.\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\n</code></pre> <p>Type <code>yes</code> and press Enter. This adds the host to <code>~/.ssh/known_hosts</code>.</p>"},{"location":"ssh-access/#successful-connection","title":"Successful Connection","text":"<p>You should see:</p> <pre><code>Welcome to Ubuntu 20.04.6 LTS (GNU/Linux 5.15.0-1023-aws x86_64)\n...\nubuntu@ip-xxx-xx-xx-xx:~$\n</code></pre>"},{"location":"ssh-access/#common-ssh-tasks","title":"Common SSH Tasks","text":""},{"location":"ssh-access/#inspect-docker-containers","title":"Inspect Docker Containers","text":"<pre><code># List running containers\nsudo docker ps\n\n# View allocator container logs\nsudo docker logs &lt;container-id&gt;\n\n# Follow logs in real-time\nsudo docker logs -f &lt;container-id&gt;\n</code></pre>"},{"location":"ssh-access/#access-allocator-container","title":"Access Allocator Container","text":"<pre><code># Get container ID\nCONTAINER_ID=$(sudo docker ps --filter \"ancestor=ghcr.io/talmolab/lablink-allocator-image\" --format \"{{.ID}}\")\n\n# Execute bash in container\nsudo docker exec -it $CONTAINER_ID bash\n</code></pre> <p>Inside container:</p> <pre><code># View configuration\ncat /app/config/config.yaml\n\n# Check Flask app\nps aux | grep flask\n\n# Access PostgreSQL\npsql -U lablink -d lablink_db\n</code></pre>"},{"location":"ssh-access/#restart-postgresql","title":"Restart PostgreSQL","text":"<p>Known issue: PostgreSQL may need restart after first boot:</p> <pre><code># SSH into allocator\nssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\n\n# Get container ID\nsudo docker ps\n\n# Access container\nsudo docker exec -it &lt;container-id&gt; bash\n\n# Inside container: restart PostgreSQL\n/etc/init.d/postgresql restart\n\n# Verify it's running\npg_isready -U lablink\n</code></pre>"},{"location":"ssh-access/#check-system-resources","title":"Check System Resources","text":"<pre><code># Disk usage\ndf -h\n\n# Memory usage\nfree -h\n\n# CPU usage\ntop\n\n# Running processes\nps aux\n\n# Network connections\nsudo netstat -tulpn\n</code></pre>"},{"location":"ssh-access/#view-logs","title":"View Logs","text":"<pre><code># System logs\nsudo journalctl -u docker\n\n# Docker container logs\nsudo docker logs &lt;container-id&gt;\n\n# PostgreSQL logs (inside container)\nsudo docker exec -it &lt;container-id&gt; tail -f /var/log/postgresql/postgresql-*.log\n\n# Application logs (if logging to file)\nsudo docker exec -it &lt;container-id&gt; tail -f /var/log/lablink/app.log\n</code></pre>"},{"location":"ssh-access/#transfer-files","title":"Transfer Files","text":""},{"location":"ssh-access/#from-local-to-ec2","title":"From Local to EC2","text":"<pre><code># Copy single file\nscp -i ~/lablink-key.pem local-file.txt ubuntu@&lt;allocator-ip&gt;:~/\n\n# Copy directory\nscp -i ~/lablink-key.pem -r local-dir/ ubuntu@&lt;allocator-ip&gt;:~/\n</code></pre>"},{"location":"ssh-access/#from-ec2-to-local","title":"From EC2 to Local","text":"<pre><code># Copy single file\nscp -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;:~/remote-file.txt ./\n\n# Copy directory\nscp -i ~/lablink-key.pem -r ubuntu@&lt;allocator-ip&gt;:~/remote-dir ./\n</code></pre>"},{"location":"ssh-access/#connecting-to-client-vms","title":"Connecting to Client VMs","text":""},{"location":"ssh-access/#get-client-vm-ip","title":"Get Client VM IP","text":"<pre><code># Via allocator database\nssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\nsudo docker exec -it &lt;container-id&gt; psql -U lablink -d lablink_db -c \"SELECT hostname, status FROM vms;\"\n\n# Via AWS CLI\naws ec2 describe-instances \\\n  --filters \"Name=tag:CreatedBy,Values=LabLink\" \\\n  --query 'Reservations[*].Instances[*].[InstanceId,PublicIpAddress,State.Name]' \\\n  --output table\n</code></pre>"},{"location":"ssh-access/#ssh-to-client-vm","title":"SSH to Client VM","text":"<p>Use the same key as allocator:</p> <pre><code>ssh -i ~/lablink-key.pem ubuntu@&lt;client-vm-ip&gt;\n</code></pre> <p>Note: If Terraform created clients, key might be different. Check Terraform outputs or client VM security settings.</p>"},{"location":"ssh-access/#common-client-vm-tasks","title":"Common Client VM Tasks","text":"<pre><code># Check Docker containers\nsudo docker ps\n\n# View client service logs\nsudo docker logs &lt;client-container-id&gt;\n\n# Check GPU availability\nnvidia-smi\n\n# Verify research repository cloned\nls -la /home/ubuntu/research-repo/\n\n# Check client service status\nsudo systemctl status lablink-client  # If using systemd\n</code></pre>"},{"location":"ssh-access/#ssh-configuration-file","title":"SSH Configuration File","text":"<p>For easier SSH access, create <code>~/.ssh/config</code>:</p> <pre><code># Allocator Dev\nHost lablink-dev\n    HostName 54.123.45.67\n    User ubuntu\n    IdentityFile ~/.ssh/lablink-dev-key.pem\n    StrictHostKeyChecking no\n    UserKnownHostsFile /dev/null\n\n# Allocator Test\nHost lablink-test\n    HostName 54.234.56.78\n    User ubuntu\n    IdentityFile ~/.ssh/lablink-test-key.pem\n\n# Allocator Prod\nHost lablink-prod\n    HostName 54.98.76.54\n    User ubuntu\n    IdentityFile ~/.ssh/lablink-prod-key.pem\n</code></pre> <p>Usage: <pre><code># Instead of:\nssh -i ~/lablink-dev-key.pem ubuntu@54.123.45.67\n\n# Simply:\nssh lablink-dev\n</code></pre></p>"},{"location":"ssh-access/#troubleshooting-ssh-issues","title":"Troubleshooting SSH Issues","text":""},{"location":"ssh-access/#permission-denied-publickey","title":"Permission Denied (publickey)","text":"<p>Error: <pre><code>Permission denied (publickey).\n</code></pre></p> <p>Causes &amp; Solutions:</p> <ol> <li> <p>Wrong key:    <pre><code># Verify key matches instance\nssh-keygen -lf ~/lablink-key.pem\n</code></pre></p> </li> <li> <p>Wrong permissions:    <pre><code>chmod 600 ~/lablink-key.pem\n</code></pre></p> </li> <li> <p>Wrong user:    <pre><code># Try 'ubuntu' or 'ec2-user'\nssh -i ~/lablink-key.pem ubuntu@&lt;ip&gt;\nssh -i ~/lablink-key.pem ec2-user@&lt;ip&gt;\n</code></pre></p> </li> <li> <p>Key not in authorized_keys:</p> </li> <li>Redeploy instance with correct key</li> </ol>"},{"location":"ssh-access/#connection-timeout","title":"Connection Timeout","text":"<p>Error: <pre><code>ssh: connect to host 54.123.45.67 port 22: Connection timed out\n</code></pre></p> <p>Causes &amp; Solutions:</p> <ol> <li>Security group doesn't allow SSH:    <pre><code>aws ec2 describe-security-groups \\\n  --group-ids sg-xxxxx \\\n  --query 'SecurityGroups[0].IpPermissions'\n</code></pre></li> </ol> <p>Add SSH rule:    <pre><code>aws ec2 authorize-security-group-ingress \\\n  --group-id sg-xxxxx \\\n  --protocol tcp \\\n  --port 22 \\\n  --cidr $(curl -s ifconfig.me)/32\n</code></pre></p> <ol> <li> <p>Instance not running:    <pre><code>aws ec2 describe-instances --instance-ids i-xxxxx \\\n  --query 'Reservations[0].Instances[0].State.Name'\n</code></pre></p> </li> <li> <p>Wrong IP address:</p> </li> <li>Verify IP from AWS console or Terraform output</li> </ol>"},{"location":"ssh-access/#host-key-verification-failed","title":"Host Key Verification Failed","text":"<p>Error: <pre><code>@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n</code></pre></p> <p>Cause: Instance was recreated with same IP but different host key.</p> <p>Solution: <pre><code># Remove old host key\nssh-keygen -R &lt;ip-address&gt;\n\n# Or remove entire known_hosts file (if safe)\nrm ~/.ssh/known_hosts\n</code></pre></p>"},{"location":"ssh-access/#too-many-authentication-failures","title":"Too Many Authentication Failures","text":"<p>Error: <pre><code>Received disconnect from 54.123.45.67: Too many authentication failures\n</code></pre></p> <p>Cause: SSH tried multiple keys before correct one.</p> <p>Solution: <pre><code># Specify only this key\nssh -o IdentitiesOnly=yes -i ~/lablink-key.pem ubuntu@&lt;ip&gt;\n</code></pre></p>"},{"location":"ssh-access/#alternative-access-methods","title":"Alternative Access Methods","text":""},{"location":"ssh-access/#aws-systems-manager-session-manager","title":"AWS Systems Manager Session Manager","text":"<p>No SSH keys needed:</p> <pre><code># Install Session Manager plugin\n# macOS\nbrew install --cask session-manager-plugin\n\n# Start session\naws ssm start-session --target i-xxxxx\n</code></pre> <p>Benefits: - No SSH keys to manage - Works even if security group blocks port 22 - Audit logs in CloudTrail - IAM-based access control</p> <p>Requirements: - SSM agent installed on instance (default for recent AMIs) - IAM role attached to instance with SSM permissions</p>"},{"location":"ssh-access/#ec2-instance-connect","title":"EC2 Instance Connect","text":"<p>Browser-based SSH (AWS Console):</p> <ol> <li>Navigate to EC2 \u2192 Instances</li> <li>Select instance</li> <li>Click Connect</li> <li>Choose EC2 Instance Connect</li> <li>Click Connect</li> </ol> <p>Limitations: - Only works for 60 seconds - Requires security group to allow port 22 from AWS IP ranges</p>"},{"location":"ssh-access/#serial-console","title":"Serial Console","text":"<p>For debugging boot issues:</p> <ol> <li>Navigate to EC2 \u2192 Instances</li> <li>Select instance</li> <li>Actions \u2192 Monitor and troubleshoot \u2192 EC2 Serial Console</li> </ol> <p>Note: Must be enabled in account settings.</p>"},{"location":"ssh-access/#security-best-practices","title":"Security Best Practices","text":"<ol> <li> <p>Restrict SSH access: Limit security group to your IP    <pre><code>YOUR_IP=$(curl -s ifconfig.me)\naws ec2 authorize-security-group-ingress \\\n  --group-id sg-xxxxx \\\n  --protocol tcp \\\n  --port 22 \\\n  --cidr $YOUR_IP/32\n</code></pre></p> </li> <li> <p>Use SSH agent: Avoid typing key path    <pre><code>ssh-add ~/lablink-key.pem\nssh ubuntu@&lt;ip&gt;  # No -i flag needed\n</code></pre></p> </li> <li> <p>Disable password authentication: Enforce key-based auth    <pre><code># In /etc/ssh/sshd_config\nPasswordAuthentication no\n</code></pre></p> </li> <li> <p>Use bastion host: For production, access via jump box    <pre><code>ssh -J bastion-user@bastion-ip ubuntu@private-ip\n</code></pre></p> </li> <li> <p>Rotate keys regularly: Every 90 days    <pre><code>terraform destroy &amp;&amp; terraform apply  # Generates new keys\n</code></pre></p> </li> <li> <p>Use Session Manager: Avoid SSH when possible</p> </li> </ol>"},{"location":"ssh-access/#next-steps","title":"Next Steps","text":"<ul> <li>Troubleshooting: Fix SSH and connectivity issues</li> <li>Security: Secure your SSH access</li> <li>Database Management: Access database via SSH</li> </ul>"},{"location":"ssh-access/#quick-reference","title":"Quick Reference","text":"<pre><code># Connect to allocator\nssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\n\n# Set key permissions\nchmod 600 ~/lablink-key.pem\n\n# Copy file to instance\nscp -i ~/lablink-key.pem file.txt ubuntu@&lt;ip&gt;:~/\n\n# Access container\nsudo docker exec -it &lt;container-id&gt; bash\n\n# View logs\nsudo docker logs -f &lt;container-id&gt;\n\n# Restart PostgreSQL\nsudo docker exec -it &lt;container-id&gt; /etc/init.d/postgresql restart\n</code></pre>"},{"location":"testing/","title":"Testing","text":"<p>This guide covers testing LabLink components, including unit tests, integration tests, and end-to-end testing.</p>"},{"location":"testing/#testing-overview","title":"Testing Overview","text":"<p>LabLink uses pytest for testing Python code. The testing strategy includes:</p> <ul> <li>Unit tests: Test individual functions and classes (mocked dependencies)</li> <li>Integration tests: Test component interactions</li> <li>End-to-end tests: Test full workflows from API to VM creation</li> <li>Infrastructure tests: Validate Terraform configurations</li> </ul>"},{"location":"testing/#continuous-integration-ci","title":"Continuous Integration (CI)","text":""},{"location":"testing/#ci-pipeline","title":"CI Pipeline","text":"<p>Workflow: <code>.github/workflows/ci.yml</code></p> <p>Triggers: - Pull requests to <code>main</code> - Pushes to <code>main</code> or development branches</p> <p>Steps: 1. Setup Python environment 2. Install dependencies 3. Run linting (ruff) 4. Run unit tests with pytest 5. Generate coverage reports</p> <p>CI runs: - Allocator service tests - Client service tests - Mock tests only (no AWS resources)</p>"},{"location":"testing/#view-ci-results","title":"View CI Results","text":"<ol> <li>Navigate to Actions tab in GitHub</li> <li>Click on CI workflow run</li> <li>View test results and coverage</li> </ol>"},{"location":"testing/#local-testing","title":"Local Testing","text":""},{"location":"testing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9+</li> <li><code>uv</code> package manager (recommended) or <code>pip</code></li> <li>Allocator and client service code</li> </ul>"},{"location":"testing/#setup-test-environment","title":"Setup Test Environment","text":""},{"location":"testing/#using-uv-recommended","title":"Using uv (Recommended)","text":"<pre><code># Install uv\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Navigate to service directory\ncd packages/allocator\n\n# Install dependencies including test deps\nuv sync --extra dev\n\n# Or for client\ncd ../client\nuv sync --extra dev\n</code></pre>"},{"location":"testing/#using-pip","title":"Using pip","text":"<pre><code>cd packages/allocator\n\n# Create virtual environment\npython3 -m venv venv\nsource venv/bin/activate  # Linux/macOS\n# venv\\Scripts\\activate   # Windows\n\n# Install dependencies\npip install -e \".[dev]\"\n</code></pre>"},{"location":"testing/#run-unit-tests","title":"Run Unit Tests","text":""},{"location":"testing/#allocator-service","title":"Allocator Service","text":"<pre><code>cd packages/allocator\n\n# Run all tests\nPYTHONPATH=. pytest\n\n# Run with verbose output\nPYTHONPATH=. pytest -v\n\n# Run specific test file\nPYTHONPATH=. pytest tests/test_api_calls.py\n\n# Run specific test\nPYTHONPATH=. pytest tests/test_api_calls.py::test_request_vm\n</code></pre>"},{"location":"testing/#client-service","title":"Client Service","text":"<pre><code>cd packages/client\n\n# Run all tests\nPYTHONPATH=. pytest\n\n# Run specific tests\nPYTHONPATH=. pytest tests/test_check_gpu.py\nPYTHONPATH=. pytest tests/test_subscribe.py\n</code></pre>"},{"location":"testing/#run-with-coverage","title":"Run with Coverage","text":"<pre><code># Generate coverage report\nPYTHONPATH=. pytest --cov=lablink_allocator --cov-report=html\n\n# View coverage report\nopen htmlcov/index.html  # macOS\nxdg-open htmlcov/index.html  # Linux\nstart htmlcov/index.html  # Windows\n</code></pre>"},{"location":"testing/#run-linting","title":"Run Linting","text":"<pre><code># Check code quality with ruff\nruff check .\n\n# Auto-fix issues\nruff check --fix .\n\n# Format code\nruff format .\n</code></pre>"},{"location":"testing/#test-structure","title":"Test Structure","text":""},{"location":"testing/#allocator-service-tests","title":"Allocator Service Tests","text":"<p>Located in <code>packages/allocator/tests/</code>:</p> Test File Purpose <code>test_api_calls.py</code> Test Flask API endpoints <code>test_admin_auth.py</code> Test admin authentication <code>test_pages.py</code> Test web page rendering <code>test_terraform_api.py</code> Test Terraform integration <code>utils/test_aws_utils.py</code> Test AWS utility functions <code>utils/test_terraform_utils.py</code> Test Terraform utilities <code>utils/test_scp.py</code> Test SCP file transfer"},{"location":"testing/#client-service-tests","title":"Client Service Tests","text":"<p>Located in <code>packages/client/tests/</code>:</p> Test File Purpose <code>test_check_gpu.py</code> Test GPU health check <code>test_subscribe.py</code> Test allocator subscription <code>test_update_inuse.py</code> Test status updates <code>test_connect_crd.py</code> Test CRD command execution <code>test_imports.py</code> Test module imports"},{"location":"testing/#terraform-tests","title":"Terraform Tests","text":"<p>Located in <code>packages/allocator/src/lablink_allocator/terraform/tests/</code>:</p> Test File Purpose <code>test_plan.py</code> Test Terraform plan validation"},{"location":"testing/#feature-testing","title":"Feature Testing","text":"<p>LabLink has two key features that require integration testing:</p>"},{"location":"testing/#feature-1-update-in-use-status","title":"Feature 1: Update In-Use Status","text":"<p>Purpose: Client VMs report their status to the allocator.</p> <p>Test Setup:</p> <ol> <li>Deploy LabLink allocator</li> <li>Create client VMs via allocator</li> <li>Verify VMs register with allocator</li> <li>Check status updates in allocator database</li> </ol> <p>Manual Test:</p> <pre><code># Deploy allocator\ncd lablink-infrastructure\nterraform apply -var=\"resource_suffix=test\"\n\n# Get allocator IP\nALLOCATOR_IP=$(terraform output -raw ec2_public_ip)\n\n# Create client VMs via web interface\nopen http://$ALLOCATOR_IP:80\n\n# Check VM status\nssh -i ~/lablink-key.pem ubuntu@$ALLOCATOR_IP\nsudo docker exec &lt;container-id&gt; psql -U lablink -d lablink_db -c \"SELECT hostname, status, updated_at FROM vms;\"\n\n# Verify status changes over time\nwatch -n 5 'sudo docker exec &lt;container-id&gt; psql -U lablink -d lablink_db -c \"SELECT hostname, status, updated_at FROM vms;\"'\n</code></pre> <p>Automated Test:</p> <p>See <code>.github/workflows/client-vm-infrastructure-test.yml</code></p>"},{"location":"testing/#feature-2-gpu-health-check","title":"Feature 2: GPU Health Check","text":"<p>Purpose: Client VMs automatically check GPU health every 20 seconds and report to allocator.</p> <p>Test Setup:</p> <ol> <li>Create client VM with GPU instance type (e.g., <code>g4dn.xlarge</code>)</li> <li>Monitor GPU health checks in client logs</li> <li>Verify status updates in allocator</li> </ol> <p>Manual Test:</p> <pre><code># SSH into client VM\nssh -i ~/lablink-key.pem ubuntu@&lt;client-vm-ip&gt;\n\n# Check GPU availability\nnvidia-smi\n\n# View client service logs\nsudo docker logs -f &lt;client-container-id&gt;\n\n# Look for GPU health check messages:\n# \"GPU health: OK\" or \"GPU health: FAILED\"\n\n# Check allocator database\nssh -i ~/lablink-key.pem ubuntu@$ALLOCATOR_IP\nsudo docker exec &lt;container-id&gt; psql -U lablink -d lablink_db -c \"SELECT hostname, status FROM vms WHERE hostname='&lt;client-hostname&gt;';\"\n</code></pre> <p>Expected Behavior:</p> <ul> <li>Client checks GPU every 20 seconds</li> <li>If GPU fails, status changes to \"failed\"</li> <li>Allocator UI shows failed status</li> </ul>"},{"location":"testing/#end-to-end-testing","title":"End-to-End Testing","text":""},{"location":"testing/#full-workflow-test","title":"Full Workflow Test","text":"<p>Test complete VM allocation workflow:</p> <pre><code># 1. Deploy allocator\ncd lablink-infrastructure\nterraform apply -var=\"resource_suffix=e2e-test\"\nALLOCATOR_IP=$(terraform output -raw ec2_public_ip)\n\n# 2. Request VM via API\ncurl -X POST http://$ALLOCATOR_IP:80/request_vm \\\n  -d \"email=test@example.com\" \\\n  -d \"crd_command=echo 'test'\"\n\n# 3. Verify VM assigned\nssh -i ~/lablink-key.pem ubuntu@$ALLOCATOR_IP \\\n  'sudo docker exec $(sudo docker ps -q) psql -U lablink -d lablink_db -c \"SELECT * FROM vms WHERE email=\\\"test@example.com\\\";\"'\n\n# 4. Create client VMs\ncurl -X POST http://$ALLOCATOR_IP:80/admin/create \\\n  -u admin:IwanttoSLEAP \\\n  -d \"instance_count=2\"\n\n# 5. Wait for VMs to be created\nsleep 300\n\n# 6. Verify VMs exist\naws ec2 describe-instances --filters \"Name=tag:CreatedBy,Values=LabLink\" \\\n  --query 'Reservations[*].Instances[*].[InstanceId,State.Name,PublicIpAddress]' \\\n  --output table\n\n# 7. Check VMs registered with allocator\nssh -i ~/lablink-key.pem ubuntu@$ALLOCATOR_IP \\\n  'sudo docker exec $(sudo docker ps -q) psql -U lablink -d lablink_db -c \"SELECT hostname, status FROM vms;\"'\n\n# 8. Cleanup\nterraform destroy -var=\"resource_suffix=e2e-test\" -auto-approve\n</code></pre>"},{"location":"testing/#infrastructure-test-ci","title":"Infrastructure Test (CI)","text":"<p>Workflow: <code>.github/workflows/client-vm-infrastructure-test.yml</code></p> <p>This workflow performs end-to-end testing of client VM creation:</p> <ol> <li>Deploys allocator to test environment</li> <li>Triggers client VM creation</li> <li>Waits for VM to become ready</li> <li>Verifies VM registration</li> <li>Checks health status</li> <li>Cleans up resources</li> </ol> <p>Run Manually:</p> <ol> <li>Navigate to Actions tab</li> <li>Select \"Client VM Infrastructure Test\"</li> <li>Click \"Run workflow\"</li> <li>Monitor progress</li> </ol>"},{"location":"testing/#mocking-for-tests","title":"Mocking for Tests","text":""},{"location":"testing/#mock-aws-services","title":"Mock AWS Services","text":"<p>Use <code>moto</code> for mocking AWS:</p> <pre><code>import boto3\nfrom moto import mock_ec2, mock_s3\n\n@mock_ec2\ndef test_create_instance():\n    ec2 = boto3.client('ec2', region_name='us-west-2')\n\n    # This creates a mock instance\n    response = ec2.run_instances(ImageId='ami-12345', MinCount=1, MaxCount=1)\n\n    assert len(response['Instances']) == 1\n</code></pre>"},{"location":"testing/#mock-database","title":"Mock Database","text":"<p>Use <code>pytest</code> fixtures for database mocking:</p> <pre><code>import pytest\nfrom unittest.mock import MagicMock\n\n@pytest.fixture\ndef mock_db():\n    \"\"\"Mock database connection.\"\"\"\n    db = MagicMock()\n    db.execute.return_value = [{'hostname': 'i-12345', 'status': 'available'}]\n    return db\n\ndef test_get_available_vm(mock_db):\n    result = get_available_vm(mock_db)\n    assert result['hostname'] == 'i-12345'\n</code></pre>"},{"location":"testing/#mock-external-apis","title":"Mock External APIs","text":"<p>Mock HTTP requests with <code>responses</code>:</p> <pre><code>import responses\nimport requests\n\n@responses.activate\ndef test_allocator_api():\n    responses.add(\n        responses.POST,\n        'http://allocator:80/request_vm',\n        json={'hostname': 'i-12345', 'status': 'assigned'},\n        status=200\n    )\n\n    resp = requests.post('http://allocator:80/request_vm', data={'email': 'test@example.com'})\n    assert resp.json()['hostname'] == 'i-12345'\n</code></pre>"},{"location":"testing/#performance-testing","title":"Performance Testing","text":""},{"location":"testing/#load-testing","title":"Load Testing","text":"<p>Test allocator under load with <code>locust</code>:</p> <p><code>locustfile.py</code>: <pre><code>from locust import HttpUser, task, between\n\nclass LablinkUser(HttpUser):\n    wait_time = between(1, 3)\n\n    @task\n    def request_vm(self):\n        self.client.post(\"/request_vm\", data={\n            \"email\": \"load-test@example.com\",\n            \"crd_command\": \"echo test\"\n        })\n\n    @task(2)\n    def view_instances(self):\n        self.client.get(\"/admin/instances\", auth=('admin', 'IwanttoSLEAP'))\n</code></pre></p> <p>Run load test: <pre><code>pip install locust\nlocust -f locustfile.py --host http://&lt;allocator-ip&gt;:80\n</code></pre></p> <p>Open http://localhost:8089 to configure and start load test.</p>"},{"location":"testing/#test-best-practices","title":"Test Best Practices","text":"<ol> <li> <p>Run tests before committing:    <pre><code>pytest &amp;&amp; git commit\n</code></pre></p> </li> <li> <p>Write tests for new features:</p> </li> <li>Add test file in <code>tests/</code></li> <li>Test happy path and error cases</li> <li> <p>Use mocks to avoid external dependencies</p> </li> <li> <p>Keep tests fast:</p> </li> <li>Use mocks for external services</li> <li>Avoid time.sleep() when possible</li> <li> <p>Run integration tests separately</p> </li> <li> <p>Use descriptive test names:    <pre><code>def test_request_vm_returns_available_vm():\n    ...\n\ndef test_request_vm_returns_error_when_no_vms_available():\n    ...\n</code></pre></p> </li> <li> <p>Test edge cases:</p> </li> <li>Empty inputs</li> <li>Invalid credentials</li> <li>Network failures</li> <li>Resource exhaustion</li> </ol>"},{"location":"testing/#troubleshooting-tests","title":"Troubleshooting Tests","text":""},{"location":"testing/#tests-fail-locally-but-pass-in-ci","title":"Tests Fail Locally But Pass in CI","text":"<p>Possible causes: - Different Python versions - Missing dependencies - Environment variables not set</p> <p>Solution: <pre><code># Match CI Python version\nuv python install 3.11\n\n# Use same dependencies as CI\nuv sync\n\n# Set required environment variables\nexport DB_PASSWORD=test\nexport ADMIN_PASSWORD=test\n</code></pre></p>"},{"location":"testing/#import-errors","title":"Import Errors","text":"<p>Error: <code>ModuleNotFoundError: No module named 'lablink_allocator'</code></p> <p>Solution: <pre><code># Set PYTHONPATH\nexport PYTHONPATH=.\npytest\n\n# Or install package in editable mode\npip install -e .\npytest\n</code></pre></p>"},{"location":"testing/#database-connection-errors","title":"Database Connection Errors","text":"<p>Error: <code>psycopg2.OperationalError: could not connect to server</code></p> <p>Solution: - Tests should use mocked database - Check for hardcoded connection strings - Use fixtures for database access</p>"},{"location":"testing/#next-steps","title":"Next Steps","text":"<ul> <li>CI/CD Workflows: Understand automated testing</li> <li>Troubleshooting: Debug test failures</li> <li>Contributing: Add new tests</li> </ul>"},{"location":"testing/#quick-reference","title":"Quick Reference","text":"<pre><code># Run all tests\nPYTHONPATH=. pytest\n\n# Run with coverage\nPYTHONPATH=. pytest --cov\n\n# Run specific test\nPYTHONPATH=. pytest tests/test_api_calls.py::test_request_vm\n\n# Run linting\nruff check .\n\n# Format code\nruff format .\n\n# Watch for changes and re-run tests\nPYTHONPATH=. pytest-watch\n</code></pre>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>This guide covers common issues and their solutions when deploying and operating LabLink.</p>"},{"location":"troubleshooting/#general-troubleshooting-workflow","title":"General Troubleshooting Workflow","text":"<p>When encountering issues, follow this systematic approach:</p> <ol> <li>Initialize Terraform state locally (if not already done)</li> <li>Plan terraform resources using AWS CLI</li> <li>Apply changes</li> <li>Retrieve PEM key for SSH access</li> <li>Check admin website or SSH into EC2 instance</li> <li>Review logs for errors</li> <li>Destroy resources when done troubleshooting</li> </ol>"},{"location":"troubleshooting/#common-issues","title":"Common Issues","text":""},{"location":"troubleshooting/#docker-and-installation","title":"Docker and Installation","text":""},{"location":"troubleshooting/#docker-permission-error","title":"Docker Permission Error","text":"<p>Error: <pre><code>Got permission denied while trying to connect to the Docker daemon socket\n</code></pre></p> <p>Solution: <pre><code># Add user to docker group\nsudo usermod -aG docker $USER\n\n# Apply group changes\nnewgrp docker\n\n# Verify\ndocker ps\n</code></pre></p>"},{"location":"troubleshooting/#docker-daemon-not-running","title":"Docker Daemon Not Running","text":"<p>Error: <pre><code>Cannot connect to the Docker daemon at unix:///var/run/docker.sock\n</code></pre></p> <p>Solution: <pre><code># Start Docker (Linux)\nsudo systemctl start docker\nsudo systemctl enable docker\n\n# macOS/Windows\n# Start Docker Desktop application\n</code></pre></p>"},{"location":"troubleshooting/#ssh-access-issues","title":"SSH Access Issues","text":""},{"location":"troubleshooting/#permission-denied-publickey","title":"Permission Denied (publickey)","text":"<p>Error: <pre><code>Permission denied (publickey).\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Check key permissions:    <pre><code>chmod 600 ~/lablink-key.pem\nls -l ~/lablink-key.pem\n# Should show: -rw-------\n</code></pre></p> </li> <li> <p>Verify correct key:    <pre><code># Extract key from Terraform\ncd lablink-infrastructure\nterraform output -raw private_key_pem &gt; ~/lablink-key.pem\nchmod 600 ~/lablink-key.pem\n</code></pre></p> </li> <li> <p>Check correct user:    <pre><code># Try ubuntu user\nssh -i ~/lablink-key.pem ubuntu@&lt;ip&gt;\n\n# Or ec2-user\nssh -i ~/lablink-key.pem ec2-user@&lt;ip&gt;\n</code></pre></p> </li> <li> <p>Verify security group allows SSH:    <pre><code>aws ec2 describe-security-groups --group-ids &lt;sg-id&gt; \\\n  --query 'SecurityGroups[0].IpPermissions[?FromPort==`22`]'\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#ssh-connection-timeout","title":"SSH Connection Timeout","text":"<p>Error: <pre><code>ssh: connect to host X.X.X.X port 22: Connection timed out\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Verify security group allows port 22:    <pre><code>aws ec2 authorize-security-group-ingress \\\n  --group-id &lt;sg-id&gt; \\\n  --protocol tcp \\\n  --port 22 \\\n  --cidr 0.0.0.0/0\n</code></pre></p> </li> <li> <p>Check instance is running:    <pre><code>aws ec2 describe-instances --instance-ids &lt;instance-id&gt; \\\n  --query 'Reservations[0].Instances[0].State.Name'\n</code></pre></p> </li> <li> <p>Verify correct public IP:    <pre><code>terraform output ec2_public_ip\n</code></pre></p> </li> <li> <p>Check network ACLs:</p> </li> <li>Verify VPC network ACLs allow inbound/outbound on port 22</li> </ol>"},{"location":"troubleshooting/#flask-server-problems","title":"Flask Server Problems","text":""},{"location":"troubleshooting/#cannot-access-web-interface","title":"Cannot Access Web Interface","text":"<p>Error: Browser shows \"Connection refused\" or \"Cannot connect\"</p> <p>Solutions:</p> <ol> <li> <p>Check if container is running:    <pre><code>ssh -i ~/lablink-key.pem ubuntu@&lt;ip&gt;\nsudo docker ps\n</code></pre></p> </li> <li> <p>View container logs:    <pre><code>sudo docker logs &lt;container-id&gt;\n\n# Follow logs in real-time\nsudo docker logs -f &lt;container-id&gt;\n</code></pre></p> </li> <li> <p>Test locally from instance:    <pre><code># From within the EC2 instance\ncurl localhost:80\n</code></pre></p> </li> <li> <p>Test connectivity from outside:    <pre><code># From your local machine\nnc -vz &lt;ec2-public-ip&gt; 80\n\n# Or\ncurl http://&lt;ec2-public-ip&gt;:80\n</code></pre></p> </li> <li> <p>Check security group allows port 80:    <pre><code>aws ec2 authorize-security-group-ingress \\\n  --group-id &lt;sg-id&gt; \\\n  --protocol tcp \\\n  --port 80 \\\n  --cidr 0.0.0.0/0\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#browser-cannot-access-http-staging-mode","title":"Browser Cannot Access HTTP (Staging Mode)","text":"<p>Symptoms: - Browser cannot connect to <code>http://your-domain.com</code> when using staging mode - \"This site can't be reached\" - \"Connection refused\" - \"ERR_CONNECTION_REFUSED\"</p> <p>Cause: Your browser previously accessed the site via HTTPS and cached the HSTS (HTTP Strict Transport Security) policy. This forces all future requests to automatically upgrade to HTTPS. Since staging mode only serves HTTP (port 443 is closed), the browser cannot connect.</p> <p>Solution - Clear HSTS Cache:</p> <p>Chrome / Edge:</p> <ol> <li> <p>Open a new tab and navigate to:    <pre><code>chrome://net-internals/#hsts\n</code></pre>    (For Edge use: <code>edge://net-internals/#hsts</code>)</p> </li> <li> <p>Scroll down to \"Delete domain security policies\"</p> </li> <li> <p>Enter your full domain name:    <pre><code>test.lablink.sleap.ai\n</code></pre></p> </li> <li> <p>Click \"Delete\"</p> </li> <li> <p>Access the site again, explicitly typing <code>http://</code>:    <pre><code>http://test.lablink.sleap.ai\n</code></pre></p> </li> </ol> <p>Firefox:</p> <ol> <li> <p>Close all Firefox windows</p> </li> <li> <p>Navigate to your Firefox profile directory:</p> </li> <li>Windows: <code>%APPDATA%\\Mozilla\\Firefox\\Profiles\\</code></li> <li>macOS: <code>~/Library/Application Support/Firefox/Profiles/</code></li> <li> <p>Linux: <code>~/.mozilla/firefox/</code></p> </li> <li> <p>Find your profile folder (e.g., <code>abc123.default-release</code>)</p> </li> <li> <p>Delete the file: <code>SiteSecurityServiceState.txt</code></p> </li> <li> <p>Restart Firefox and access with <code>http://</code>:    <pre><code>http://test.lablink.sleap.ai\n</code></pre></p> </li> </ol> <p>Safari:</p> <ol> <li> <p>Close Safari completely</p> </li> <li> <p>Open Terminal and run:    <pre><code>rm ~/Library/Cookies/HSTS.plist\n</code></pre></p> </li> <li> <p>Restart Safari and access with <code>http://</code>:    <pre><code>http://test.lablink.sleap.ai\n</code></pre></p> </li> </ol> <p>Quick Workarounds:</p> <p>If you don't want to clear HSTS cache:</p> <ol> <li>Use Incognito/Private Browsing</li> <li>HSTS cache doesn't apply in incognito mode</li> <li> <p>Access <code>http://test.lablink.sleap.ai</code></p> </li> <li> <p>Access via IP Address <pre><code>http://54.214.215.124\n</code></pre>    (Find IP in Terraform outputs: <code>terraform output allocator_public_ip</code>)</p> </li> <li> <p>Use curl for testing <pre><code>curl http://test.lablink.sleap.ai\n</code></pre></p> </li> </ol> <p>Verify Staging Mode is Working:</p> <pre><code># HTTP should return 200 OK\ncurl -I http://test.lablink.sleap.ai\n\n# HTTPS should fail (connection refused)\ncurl -I https://test.lablink.sleap.ai\n</code></pre> <p>Expected behavior: When using staging mode (<code>ssl.staging: true</code>), your browser will show \"Not Secure\" in the address bar. This is normal and expected - staging mode uses unencrypted HTTP for testing.</p> <p>To get a secure HTTPS connection, set <code>ssl.staging: false</code> in your configuration. See Configuration - SSL Options.</p>"},{"location":"troubleshooting/#flask-app-not-starting","title":"Flask App Not Starting","text":"<p>Symptoms: Container runs but Flask doesn't start</p> <p>Check:</p> <pre><code># View full logs\nsudo docker logs &lt;container-id&gt;\n\n# Look for errors like:\n# - Port already in use\n# - Module import errors\n# - Configuration errors\n</code></pre> <p>Solutions:</p> <ol> <li> <p>Restart container:    <pre><code>sudo docker restart &lt;container-id&gt;\n</code></pre></p> </li> <li> <p>Check for port conflicts:    <pre><code>sudo netstat -tulpn | grep 5000\n</code></pre></p> </li> <li> <p>Verify configuration:    <pre><code>sudo docker exec &lt;container-id&gt; cat /app/config/config.yaml\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#postgresql-issues","title":"PostgreSQL Issues","text":""},{"location":"troubleshooting/#postgresql-not-accessible","title":"PostgreSQL Not Accessible","text":"<p>Known Issue: PostgreSQL server may need manual restart after first deployment.</p> <p>Solution: <pre><code># SSH into allocator\nssh -i ~/lablink-key.pem ubuntu@&lt;ec2-public-ip&gt;\n\n# Get container name\nsudo docker ps\n\n# Enter container\nsudo docker exec -it &lt;container-name&gt; bash\n\n# Inside container, restart PostgreSQL\n/etc/init.d/postgresql restart\n\n# Verify it's running\npg_isready -U lablink\n</code></pre></p> <p>Verify manually: <pre><code># Test connection\nsudo docker exec &lt;container-id&gt; psql -U lablink -d lablink_db -c \"SELECT 1;\"\n</code></pre></p>"},{"location":"troubleshooting/#database-connection-refused","title":"Database Connection Refused","text":"<p>Error: <code>psycopg2.OperationalError: could not connect to server</code></p> <p>Solutions:</p> <ol> <li> <p>Check PostgreSQL is running:    <pre><code>sudo docker exec &lt;container-id&gt; pg_isready -U lablink\n</code></pre></p> </li> <li> <p>Check PostgreSQL logs:    <pre><code>sudo docker exec &lt;container-id&gt; tail -f /var/log/postgresql/postgresql-*-main.log\n</code></pre></p> </li> <li> <p>Verify configuration:    <pre><code># Check pg_hba.conf\nsudo docker exec &lt;container-id&gt; cat /etc/postgresql/*/main/pg_hba.conf\n\n# Should contain:\n# host    all             all             0.0.0.0/0            md5\n</code></pre></p> </li> <li> <p>Restart PostgreSQL (see above)</p> </li> </ol>"},{"location":"troubleshooting/#vm-spawning-issues","title":"VM Spawning Issues","text":""},{"location":"troubleshooting/#vms-not-being-created","title":"VMs Not Being Created","text":"<p>Symptoms: Click \"Create VMs\" but nothing happens</p> <p>Check:</p> <ol> <li> <p>Allocator container logs:    <pre><code>sudo docker logs -f &lt;allocator-container-id&gt;\n</code></pre></p> </li> <li> <p>AWS credentials configured:    <pre><code># Inside container\nsudo docker exec &lt;container-id&gt; aws sts get-caller-identity\n</code></pre></p> </li> <li> <p>Terraform installed in container:    <pre><code>sudo docker exec &lt;container-id&gt; terraform version\n</code></pre></p> </li> </ol> <p>Solutions:</p> <ol> <li>Set AWS credentials (if not using IAM role):</li> <li>Navigate to <code>/admin/set-aws-credentials</code> in web interface</li> <li>Enter AWS access key and secret key</li> <li> <p>Submit</p> </li> <li> <p>Check Terraform errors:    <pre><code># Inside container\nsudo docker exec &lt;container-id&gt; cat /tmp/terraform-*.log\n</code></pre></p> </li> <li> <p>Verify IAM permissions:</p> </li> <li>EC2 permissions (RunInstances, DescribeInstances)</li> <li>VPC permissions (CreateSecurityGroup, etc.)</li> </ol>"},{"location":"troubleshooting/#terraform-apply-failed-inside-container","title":"Terraform Apply Failed Inside Container","text":"<p>Error: Terraform operations fail when triggered from allocator</p> <p>Solutions:</p> <ol> <li> <p>Check AWS credentials:    <pre><code>sudo docker exec &lt;container-id&gt; env | grep AWS\n</code></pre></p> </li> <li> <p>Test Terraform manually:    <pre><code>sudo docker exec -it &lt;container-id&gt; bash\ncd /app/.venv/lib/python*/site-packages/lablink_allocator/terraform\nterraform init\nterraform plan\n</code></pre></p> </li> <li> <p>Verify Docker socket access (if using Docker-in-Docker):    <pre><code>sudo docker exec &lt;container-id&gt; docker ps\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#terraform-issues","title":"Terraform Issues","text":""},{"location":"troubleshooting/#terraform-init-fails","title":"Terraform Init Fails","text":"<p>Error: <code>Error configuring the backend \"s3\": ... bucket does not exist</code></p> <p>Solution: <pre><code># Create S3 bucket first\naws s3 mb s3://tf-state-lablink-allocator-bucket --region us-west-2\n\n# Enable versioning\naws s3api put-bucket-versioning \\\n  --bucket tf-state-lablink-allocator-bucket \\\n  --versioning-configuration Status=Enabled\n\n# Re-init\nterraform init\n</code></pre></p>"},{"location":"troubleshooting/#terraform-state-locked","title":"Terraform State Locked","text":"<p>Error: <code>Error acquiring the state lock</code></p> <p>Cause: A previous Terraform operation didn't complete cleanly, leaving the state locked in DynamoDB.</p> <p>Diagnosis:</p> <ol> <li> <p>Identify the lock:    <pre><code># Check for locks in DynamoDB\naws dynamodb scan --table-name lock-table --region us-west-2\n</code></pre></p> </li> <li> <p>Check if a process is actually running:    <pre><code># Look for terraform processes\nps aux | grep terraform\n\n# In allocator container\nsudo docker exec &lt;container-id&gt; ps aux | grep terraform\n</code></pre></p> </li> </ol> <p>Solutions:</p> <p>Option 1: Unlock via AWS CLI (Recommended - works from anywhere)</p> <p>Step 1: First, scan the lock table to find the exact LockID: <pre><code># List all locks\naws dynamodb scan --profile &lt;your-profile&gt; --table-name lock-table --region us-west-2\n</code></pre></p> <p>Step 2: Look for entries with an <code>Info</code> field (these are actual locks, not just digests). Copy the exact <code>LockID</code> value.</p> <p>Step 3: Delete the lock:</p> <p>Linux/macOS: <pre><code>aws dynamodb delete-item \\\n    --profile &lt;your-profile&gt; \\\n    --table-name lock-table \\\n    --key '{\"LockID\":{\"S\":\"&lt;exact-lock-id-from-scan&gt;\"}}' \\\n    --region us-west-2\n</code></pre></p> <p>Windows PowerShell: <pre><code># Create key.json file with:\n# {\n#   \"LockID\": {\n#     \"S\": \"&lt;exact-lock-id-from-scan&gt;\"\n#   }\n# }\n\naws dynamodb delete-item --profile &lt;your-profile&gt; --table-name lock-table --key file://key.json --region us-west-2\n</code></pre></p> <p>Common lock paths: - Infrastructure: <code>tf-state-lablink-allocator-bucket/test/terraform.tfstate</code> - Client VMs: <code>tf-state-lablink-allocator-bucket/test/client/terraform.tfstate</code></p> <p>Note: Lock IDs do NOT always have <code>-md5</code> suffix - use the exact value from the scan!</p> <p>Option 2: Unlock from allocator (Requires DynamoDB IAM permissions) <pre><code># SSH into allocator\nssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\n\n# Enter container\nsudo docker exec -it &lt;container-id&gt; bash\n\n# Navigate to terraform directory\ncd /app/lablink_allocator/terraform\n\n# Force unlock (using lock ID from error message)\nterraform force-unlock &lt;lock-id&gt;\n</code></pre></p> <p>Common Error: <code>AccessDeniedException: User is not authorized to perform: dynamodb:GetItem</code></p> <p>Cause: The allocator IAM role lacks DynamoDB permissions.</p> <p>Solution: Ensure the allocator IAM role includes these permissions: <pre><code>{\n  \"Effect\": \"Allow\",\n  \"Action\": [\n    \"dynamodb:GetItem\",\n    \"dynamodb:PutItem\",\n    \"dynamodb:DeleteItem\"\n  ],\n  \"Resource\": \"arn:aws:dynamodb:us-west-2:&lt;account-id&gt;:table/lock-table\"\n}\n</code></pre></p> <p>After updating IAM permissions, redeploy infrastructure for changes to take effect.</p> <p>Prevention: - Don't manually terminate EC2 instances while Terraform is running - Always let Terraform operations complete fully - Use destroy workflows instead of manual AWS console deletions - Monitor allocator logs during VM creation/destruction</p>"},{"location":"troubleshooting/#resource-already-exists","title":"Resource Already Exists","text":"<p>Error: <code>Error creating ... already exists</code></p> <p>Solutions:</p> <ol> <li> <p>Import existing resource:    <pre><code>terraform import aws_security_group.lablink sg-xxxxx\nterraform import aws_instance.lablink_allocator i-xxxxx\n</code></pre></p> </li> <li> <p>Delete resource manually (if safe):    <pre><code>aws ec2 terminate-instances --instance-ids i-xxxxx\naws ec2 delete-security-group --group-id sg-xxxxx\n</code></pre></p> </li> <li> <p>Use different resource names:</p> </li> <li>Change <code>resource_suffix</code> variable: <code>-dev</code>, <code>-test</code>, <code>-prod</code></li> </ol>"},{"location":"troubleshooting/#cannot-destroy-resources","title":"Cannot Destroy Resources","text":"<p>Error: Resources won't destroy cleanly</p> <p>Solution:</p> <ol> <li> <p>Destroy in order:    <pre><code># Terminate instances first\naws ec2 terminate-instances --instance-ids i-xxxxx\n\n# Wait for termination\naws ec2 wait instance-terminated --instance-ids i-xxxxx\n\n# Delete security group\naws ec2 delete-security-group --group-id sg-xxxxx\n</code></pre></p> </li> <li> <p>Check dependencies:</p> </li> <li>Network interfaces attached</li> <li>Elastic IPs associated</li> <li> <p>Security group rules referencing each other</p> </li> <li> <p>Force destroy:    <pre><code>terraform destroy -auto-approve\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#github-actions-workflow-issues","title":"GitHub Actions Workflow Issues","text":""},{"location":"troubleshooting/#workflow-wont-trigger","title":"Workflow Won't Trigger","text":"<p>Check:</p> <ol> <li> <p>Workflow file syntax:    <pre><code># Use YAML validator\nyamllint .github/workflows/*.yml\n</code></pre></p> </li> <li> <p>Branch protection rules:</p> </li> <li>Check repository settings</li> <li> <p>Ensure workflows are enabled</p> </li> <li> <p>Trigger conditions:</p> </li> <li>Verify branch name matches trigger</li> <li>Check file path filters</li> </ol>"},{"location":"troubleshooting/#aws-authentication-fails-in-workflow","title":"AWS Authentication Fails in Workflow","text":"<p>Error: <code>Error: Could not assume role with OIDC</code></p> <p>Solutions:</p> <ol> <li> <p>Verify OIDC provider exists:    <pre><code>aws iam list-open-id-connect-providers\n</code></pre></p> </li> <li> <p>Check IAM role trust policy:    <pre><code>aws iam get-role --role-name github-lablink-deploy \\\n  --query 'Role.AssumeRolePolicyDocument'\n</code></pre></p> </li> <li> <p>Verify repository in trust policy:</p> </li> <li> <p>Trust policy must include: <code>repo:talmolab/lablink:*</code></p> </li> <li> <p>Check role ARN in workflow:</p> </li> <li>Ensure ARN matches your account ID</li> </ol>"},{"location":"troubleshooting/#terraform-apply-fails-in-workflow","title":"Terraform Apply Fails in Workflow","text":"<p>Check workflow logs for specific error:</p> <ol> <li>Permission errors: Update IAM role permissions</li> <li>Resource limits: Check AWS service quotas</li> <li>State lock: Clear lock if safe</li> </ol>"},{"location":"troubleshooting/#client-vm-issues","title":"Client VM Issues","text":""},{"location":"troubleshooting/#client-vm-not-registering","title":"Client VM Not Registering","text":"<p>Symptoms: VM created but doesn't appear in allocator database</p> <p>Root Cause: VMs are not being inserted into the database after Terraform creates them.</p> <p>Step-by-Step Diagnosis:</p> <ol> <li> <p>Verify VMs were created by Terraform:    <pre><code># Check terraform outputs from allocator\nssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\nsudo docker exec &lt;container-id&gt; terraform -chdir=/app/.venv/lib/python*/site-packages/lablink_allocator/terraform output vm_instance_names\n</code></pre></p> </li> <li> <p>Check if VMs exist in database:    <pre><code># SSH into allocator\nssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\n\n# Query database\nsudo docker exec &lt;container-id&gt; psql -U lablink -d lablink_db -c \"SELECT hostname, inuse, status FROM vms;\"\n</code></pre></p> </li> <li> <p>Check client VM container logs:    <pre><code>ssh -i ~/lablink-key.pem ubuntu@&lt;client-vm-ip&gt;\nsudo docker logs &lt;client-container-id&gt;\n\n# Look for errors like:\n# \"POST request failed with status code: 404\"\n# \"VM not found\"\n</code></pre></p> </li> <li> <p>Check allocator logs for /vm_startup requests:    <pre><code>ssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\nsudo docker logs &lt;allocator-container-id&gt; | grep vm_startup\n\n# Look for:\n# POST /vm_startup - 404 errors\n</code></pre></p> </li> <li> <p>Test network connectivity:    <pre><code># From client VM\ncurl http://&lt;allocator-ip&gt;/vm_startup \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"hostname\": \"lablink-vm-test-1\"}'\n\n# Expected if VM not in DB: {\"error\":\"VM not found.\"}\n# Expected if VM in DB: Success response\n</code></pre></p> </li> </ol> <p>Solutions:</p> <p>Option A: Manual Database Insertion (Temporary Fix) <pre><code># SSH into allocator\nssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\n\n# Get container ID\nCONTAINER_ID=$(sudo docker ps -q)\n\n# Insert VMs manually\nsudo docker exec $CONTAINER_ID psql -U lablink -d lablink_db -c \\\n  \"INSERT INTO vms (hostname, inuse) VALUES ('lablink-vm-test-1', FALSE);\"\n\nsudo docker exec $CONTAINER_ID psql -U lablink -d lablink_db -c \\\n  \"INSERT INTO vms (hostname, inuse) VALUES ('lablink-vm-test-2', FALSE);\"\n\n# Verify\nsudo docker exec $CONTAINER_ID psql -U lablink -d lablink_db -c \\\n  \"SELECT hostname, inuse FROM vms;\"\n</code></pre></p> <p>Option B: Code Fix (Permanent Solution)</p> <p>The <code>/api/launch</code> endpoint needs to be updated to insert VMs after Terraform succeeds. See VM_REGISTRATION_ISSUE.md for details.</p> <p>After Fix - Verify Registration: <pre><code># Watch client VM logs\nssh -i ~/lablink-key.pem ubuntu@&lt;client-vm-ip&gt;\nsudo docker logs -f &lt;client-container-id&gt;\n\n# Should see:\n# \"POST request was successful.\"\n# \"Received success response from server.\"\n</code></pre></p> <p>Preventive Measures: - Always verify VMs appear in database after creation - Check allocator logs during VM creation - Monitor client VM registration within 5 minutes of creation</p>"},{"location":"troubleshooting/#gpu-not-available","title":"GPU Not Available","text":"<p>Error: <code>RuntimeError: CUDA not available</code></p> <p>Check:</p> <ol> <li> <p>GPU instance type:    <pre><code># Verify instance has GPU\naws ec2 describe-instances --instance-ids &lt;id&gt; \\\n  --query 'Reservations[0].Instances[0].InstanceType'\n</code></pre></p> </li> <li> <p>NVIDIA drivers installed:    <pre><code>ssh -i ~/lablink-key.pem ubuntu@&lt;client-vm-ip&gt;\nnvidia-smi\n</code></pre></p> </li> <li> <p>Docker GPU support:    <pre><code>docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi\n</code></pre></p> </li> </ol> <p>Solution:</p> <ol> <li> <p>Use GPU-enabled AMI or install drivers:    <pre><code># Ubuntu Deep Learning AMI\nami_id = \"ami-0c2b0d3fb02824d92\"  # us-west-2\n</code></pre></p> </li> <li> <p>Verify Docker GPU runtime:    <pre><code>cat /etc/docker/daemon.json\n# Should have: \"default-runtime\": \"nvidia\"\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#dns-and-domain-issues","title":"DNS and Domain Issues","text":"<p>For detailed DNS configuration, see DNS Configuration Guide.</p>"},{"location":"troubleshooting/#dns-record-not-resolving","title":"DNS Record Not Resolving","text":"<p>Symptoms: Domain doesn't resolve to allocator IP</p> <p>Step-by-Step Diagnosis:</p> <ol> <li> <p>Verify DNS is enabled in config:    <pre><code>cat lablink-infrastructure/config/config.yaml | grep -A 10 \"^dns:\"\n</code></pre></p> </li> <li> <p>Check Route53 record exists:    <pre><code>aws route53 list-resource-record-sets \\\n  --hosted-zone-id Z010760118DSWF5IYKMOM \\\n  --query \"ResourceRecordSets[?Name=='test.lablink.sleap.ai.']\"\n</code></pre></p> </li> <li> <p>Query authoritative nameservers directly:    <pre><code># Should return the allocator IP\nnslookup test.lablink.sleap.ai ns-158.awsdns-19.com\n</code></pre></p> </li> <li> <p>Check public DNS propagation:    <pre><code># Google DNS\nnslookup test.lablink.sleap.ai 8.8.8.8\n\n# Cloudflare DNS\nnslookup test.lablink.sleap.ai 1.1.1.1\n\n# Local DNS\nnslookup test.lablink.sleap.ai\n</code></pre></p> </li> <li> <p>Verify IP matches:    <pre><code># Get terraform output\ncd lablink-infrastructure\nterraform output allocator_public_ip\n\n# Compare with DNS resolution\ndig test.lablink.sleap.ai +short\n</code></pre></p> </li> </ol> <p>Solutions:</p> <p>If record doesn't exist: <pre><code># Re-run terraform to create DNS record\ncd lablink-infrastructure\nterraform apply\n</code></pre></p> <p>If DNS not propagating: - Wait 5-15 minutes for global propagation - Check NS delegation is correct (see below) - Try flushing local DNS cache:   <pre><code># macOS\nsudo dscacheutil -flushcache; sudo killall -HUP mDNSResponder\n\n# Linux\nsudo systemd-resolve --flush-caches\n\n# Windows\nipconfig /flushdns\n</code></pre></p>"},{"location":"troubleshooting/#dns-record-in-wrong-zone","title":"DNS Record in Wrong Zone","text":"<p>Symptoms: DNS record created in <code>sleap.ai</code> zone instead of <code>lablink.sleap.ai</code> zone</p> <p>Root Cause: Terraform data source matched parent zone</p> <p>Diagnosis: <pre><code># Check both zones\naws route53 list-resource-record-sets --hosted-zone-id &lt;sleap.ai-zone-id&gt; \\\n  --query \"ResourceRecordSets[?contains(Name, 'lablink')]\"\n\naws route53 list-resource-record-sets --hosted-zone-id Z010760118DSWF5IYKMOM \\\n  --query \"ResourceRecordSets[?contains(Name, 'test')]\"\n</code></pre></p> <p>Solution:</p> <ol> <li> <p>Add zone_id to config.yaml:    <pre><code>dns:\n  enabled: true\n  domain: \"lablink.sleap.ai\"\n  zone_id: \"Z010760118DSWF5IYKMOM\"  # Force correct zone\n  pattern: \"custom\"\n  custom_subdomain: \"test\"\n</code></pre></p> </li> <li> <p>Delete record from wrong zone (if exists):    <pre><code># Use AWS console or CLI to delete A record from sleap.ai zone\n</code></pre></p> </li> <li> <p>Re-run terraform:    <pre><code>cd lablink-infrastructure\nterraform apply\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#ns-delegation-not-working","title":"NS Delegation Not Working","text":"<p>Symptoms: - nslookup returns Cloudflare nameservers instead of AWS - DNS queries fail even though record exists in Route53</p> <p>Diagnosis: <pre><code># Check NS delegation\ndig NS lablink.sleap.ai\n\n# Should show AWS nameservers:\n# ns-158.awsdns-19.com\n# ns-697.awsdns-23.net\n# ns-1839.awsdns-37.co.uk\n# ns-1029.awsdns-00.org\n</code></pre></p> <p>Solution:</p> <ol> <li> <p>Get Route53 nameservers:    <pre><code>aws route53 get-hosted-zone --id Z010760118DSWF5IYKMOM \\\n  --query 'DelegationSet.NameServers'\n</code></pre></p> </li> <li> <p>Add NS records in Cloudflare:</p> </li> <li>Log into Cloudflare</li> <li>Navigate to DNS for <code>sleap.ai</code></li> <li> <p>Add 4 NS records:</p> <ul> <li>Type: NS</li> <li>Name: <code>lablink</code></li> <li>Content: Each of the 4 AWS nameservers</li> <li>TTL: 300 (or Auto)</li> </ul> </li> <li> <p>Verify delegation:    <pre><code># Wait 5-15 minutes, then check\ndig NS lablink.sleap.ai\n</code></pre></p> </li> <li> <p>Test resolution:    <pre><code># Should now resolve via AWS\nnslookup test.lablink.sleap.ai 8.8.8.8\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#httpsssl-certificate-not-working","title":"HTTPS/SSL Certificate Not Working","text":"<p>Symptoms: - HTTP works but HTTPS fails - Browser shows \"Connection refused\" or \"SSL error\"</p> <p>Step-by-Step Diagnosis:</p> <ol> <li> <p>Check DNS is resolving:    <pre><code>nslookup test.lablink.sleap.ai 8.8.8.8\n# Must resolve before Let's Encrypt can issue certificate\n</code></pre></p> </li> <li> <p>Check Caddy is running:    <pre><code>ssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\nsudo systemctl status caddy\n</code></pre></p> </li> <li> <p>Check Caddy logs:    <pre><code>sudo journalctl -u caddy -f\n\n# Look for:\n# - \"certificate obtained successfully\" (success)\n# - \"challenge failed\" (DNS not ready)\n# - \"timeout\" (network issue)\n</code></pre></p> </li> <li> <p>Test HTTPS manually:    <pre><code>curl -v https://test.lablink.sleap.ai\n\n# Look for SSL handshake or certificate errors\n</code></pre></p> </li> <li> <p>Check ports are open:    <pre><code># Port 80 (HTTP-01 challenge)\nnc -vz test.lablink.sleap.ai 80\n\n# Port 443 (HTTPS)\nnc -vz test.lablink.sleap.ai 443\n</code></pre></p> </li> </ol> <p>Solutions:</p> <p>If DNS not propagated: - Wait 5-10 more minutes - Verify DNS resolves to correct IP - Caddy will automatically retry every 2 minutes</p> <p>If ports blocked: <pre><code># Check security group\naws ec2 describe-security-groups \\\n  --filters \"Name=tag:Name,Values=lablink-allocator-*\" \\\n  --query 'SecurityGroups[0].IpPermissions'\n\n# Add rules if missing\naws ec2 authorize-security-group-ingress \\\n  --group-id &lt;sg-id&gt; \\\n  --protocol tcp \\\n  --port 80 \\\n  --cidr 0.0.0.0/0\n\naws ec2 authorize-security-group-ingress \\\n  --group-id &lt;sg-id&gt; \\\n  --protocol tcp \\\n  --port 443 \\\n  --cidr 0.0.0.0/0\n</code></pre></p> <p>If Caddy configuration error: <pre><code># Check Caddyfile\nssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\ncat /etc/caddy/Caddyfile\n\n# Should contain:\n# test.lablink.sleap.ai {\n#     reverse_proxy localhost:5000\n# }\n\n# Restart Caddy\nsudo systemctl restart caddy\n</code></pre></p> <p>Manual certificate check: <pre><code># View certificate details\necho | openssl s_client -servername test.lablink.sleap.ai \\\n  -connect test.lablink.sleap.ai:443 2&gt;/dev/null | \\\n  openssl x509 -noout -text\n\n# Check issuer and expiration\necho | openssl s_client -servername test.lablink.sleap.ai \\\n  -connect test.lablink.sleap.ai:443 2&gt;/dev/null | \\\n  openssl x509 -noout -issuer -dates\n</code></pre></p>"},{"location":"troubleshooting/#multiple-hosted-zones-causing-conflicts","title":"Multiple Hosted Zones Causing Conflicts","text":"<p>Symptoms: Unpredictable DNS behavior, records in multiple zones</p> <p>Diagnosis: <pre><code># List all zones\naws route53 list-hosted-zones --query 'HostedZones[*].[Name,Id]' --output table\n\n# Check for duplicates or parent/child conflicts\n</code></pre></p> <p>Solution:</p> <ol> <li>Identify which zone to keep:</li> <li>Keep <code>lablink.sleap.ai</code> in Route53 (managed by LabLink)</li> <li> <p>Delete <code>sleap.ai</code> from Route53 (managed in Cloudflare)</p> </li> <li> <p>Delete conflicting zone:    <pre><code># ONLY if sleap.ai is managed in Cloudflare\naws route53 delete-hosted-zone --id &lt;sleap-ai-zone-id&gt;\n</code></pre></p> </li> <li> <p>Verify NS delegation in Cloudflare (see above)</p> </li> </ol>"},{"location":"troubleshooting/#deployment-verification-failing","title":"Deployment Verification Failing","text":"<p>Symptoms: <code>verify-deployment.sh</code> reports failures</p> <p>Run verification: <pre><code>cd lablink-infrastructure\n./verify-deployment.sh test.lablink.sleap.ai 52.40.142.146\n</code></pre></p> <p>Common failures:</p> <ol> <li>DNS timeout - Wait longer and retry</li> <li>HTTP not responding - Check allocator container logs</li> <li>SSL not ready - Check Caddy logs, may need more time</li> </ol> <p>Interpret results: - \u2713 Green checkmarks = Success - \u26a0 Yellow warnings = May need more time - \u2717 Red errors = Actual problem requiring action</p>"},{"location":"troubleshooting/#diagnostic-commands","title":"Diagnostic Commands","text":""},{"location":"troubleshooting/#check-overall-system-health","title":"Check Overall System Health","text":"<pre><code># SSH into allocator\nssh -i ~/lablink-key.pem ubuntu@&lt;allocator-ip&gt;\n\n# System resources\ndf -h              # Disk usage\nfree -h            # Memory usage\ntop                # CPU usage\n\n# Docker\nsudo docker ps -a  # All containers\nsudo docker stats  # Resource usage\n\n# Network\nsudo netstat -tulpn    # Listening ports\nip addr show           # Network interfaces\n\n# Logs\nsudo journalctl -u docker -n 100  # Docker service logs\ndmesg | tail                      # Kernel messages\n</code></pre>"},{"location":"troubleshooting/#check-flask-app-status","title":"Check Flask App Status","text":"<pre><code># From allocator instance\ncurl localhost:80\n\n# Expected: HTML response with LabLink interface\n</code></pre>"},{"location":"troubleshooting/#check-database-status","title":"Check Database Status","text":"<pre><code># PostgreSQL running?\nsudo docker exec &lt;container-id&gt; pg_isready -U lablink\n\n# Can connect?\nsudo docker exec &lt;container-id&gt; psql -U lablink -d lablink_db -c \"SELECT version();\"\n\n# View VMs\nsudo docker exec &lt;container-id&gt; psql -U lablink -d lablink_db -c \"SELECT * FROM vms;\"\n</code></pre>"},{"location":"troubleshooting/#check-aws-connectivity","title":"Check AWS Connectivity","text":"<pre><code># From allocator container\nsudo docker exec &lt;container-id&gt; aws sts get-caller-identity\n\n# List EC2 instances\nsudo docker exec &lt;container-id&gt; aws ec2 describe-instances --region us-west-2\n</code></pre>"},{"location":"troubleshooting/#getting-help","title":"Getting Help","text":"<p>If issues persist:</p> <ol> <li>Check logs thoroughly: Most issues have error messages in logs</li> <li>Search existing issues: GitHub Issues</li> <li>Open new issue: Provide:</li> <li>Error messages</li> <li>Logs</li> <li>Steps to reproduce</li> <li>Environment (OS, versions)</li> <li>What you've tried</li> </ol>"},{"location":"troubleshooting/#related-documentation","title":"Related Documentation","text":"<ul> <li>Installation: Setup instructions</li> <li>Deployment: Deployment guides</li> <li>SSH Access: Connection help</li> <li>Database: Database issues</li> <li>Security: Security problems</li> </ul>"},{"location":"troubleshooting/#quick-troubleshooting-checklist","title":"Quick Troubleshooting Checklist","text":"<ul> <li>[ ] Docker is running</li> <li>[ ] Container is running (<code>docker ps</code>)</li> <li>[ ] Security groups allow required ports (22, 80, 5432)</li> <li>[ ] SSH key has correct permissions (600)</li> <li>[ ] AWS credentials are configured</li> <li>[ ] PostgreSQL has been restarted (if needed)</li> <li>[ ] Terraform state is not locked</li> <li>[ ] S3 bucket exists for Terraform state</li> <li>[ ] IAM role/user has necessary permissions</li> <li>[ ] Network connectivity between components</li> <li>[ ] Logs have been checked for errors</li> </ul>"},{"location":"workflows/","title":"Workflows","text":"<p>This guide explains LabLink's CI/CD workflows, how they work, and how to customize them.</p>"},{"location":"workflows/#overview","title":"Overview","text":"<p>LabLink uses GitHub Actions for continuous integration and deployment. The workflows automate:</p> <ul> <li>Python package publishing to PyPI</li> <li>Docker image building and publishing to GHCR</li> <li>Testing and validation (linting, unit tests, Docker builds)</li> <li>Documentation deployment to GitHub Pages</li> </ul> <p>Note: Infrastructure deployment workflows (Terraform) have been moved to the LabLink Template Repository.</p>"},{"location":"workflows/#workflow-files","title":"Workflow Files","text":"<p>All workflows are located in <code>.github/workflows/</code>:</p> Workflow File Purpose Trigger <code>ci.yml</code> Unit tests, linting, Docker build tests PRs, pushes <code>publish-packages.yml</code> Publish Python packages to PyPI Git tags, manual dispatch <code>lablink-images.yml</code> Build and push Docker images to GHCR Push to branches, PRs, package publish <code>docs.yml</code> Build and deploy documentation Pushes to main, docs changes"},{"location":"workflows/#continuous-integration-workflow","title":"Continuous Integration Workflow","text":"<p>File: <code>.github/workflows/ci.yml</code></p>"},{"location":"workflows/#purpose","title":"Purpose","text":"<p>Runs tests, linting, and Docker build verification on every pull request affecting service code.</p>"},{"location":"workflows/#triggers","title":"Triggers","text":"<ul> <li>Pull requests with changes to:</li> <li><code>packages/client/**</code></li> <li><code>packages/allocator/**</code></li> <li><code>.github/workflows/ci.yml</code></li> </ul>"},{"location":"workflows/#jobs","title":"Jobs","text":"<ol> <li>Lint - Checks code quality with <code>ruff</code></li> <li>Allocator service: <code>uv run ruff check src tests</code></li> <li> <p>Client service: <code>uv run ruff check src tests</code></p> </li> <li> <p>Test - Runs unit tests with <code>pytest</code></p> </li> <li>Allocator: <code>uv run pytest tests --cov=. --cov-report=xml</code></li> <li> <p>Client: <code>uv run pytest tests --cov=src/lablink_client_service --cov-report=xml</code></p> </li> <li> <p>Docker Build Test (Allocator Only)</p> </li> <li>Builds <code>packages/allocator/Dockerfile.dev</code> using <code>uv sync --extra dev</code></li> <li>Verifies virtual environment activation</li> <li>Verifies console script entry points are importable and callable</li> <li>Verifies console scripts exist (<code>lablink-allocator</code>, <code>generate-init-sql</code>)</li> <li>Verifies dev dependencies installed (pytest, ruff, coverage with versions)</li> <li>Verifies package imports (main, database, get_config)</li> <li>Verifies <code>uv sync</code> installation</li> <li>Note: Client Docker build test skipped due to large image size (~6GB with CUDA)</li> </ol>"},{"location":"workflows/#example-workflow-run","title":"Example Workflow Run","text":"<pre><code>PR opened \u2192 ci.yml triggered\n  \u251c\u2500 Lint allocator-service \u2713\n  \u251c\u2500 Lint client-service \u2713\n  \u251c\u2500 Test allocator-service \u2713\n  \u251c\u2500 Test client-service \u2713\n  \u2514\u2500 Docker Build Test - Allocator \u2713\n     \u251c\u2500 Venv activated: /app/lablink-allocator-service/.venv\n     \u251c\u2500 Entry points importable: main(), generate_init_sql.main() \u2713\n     \u251c\u2500 Console scripts: lablink-allocator, generate-init-sql \u2713\n     \u251c\u2500 Dev dependencies: pytest 8.4.2, ruff, coverage 7.10.7 \u2713\n     \u251c\u2500 Package imports: main.main, database.PostgresqlDatabase, get_config \u2713\n     \u2514\u2500 Installation: Package installed via uv sync \u2713\n</code></pre>"},{"location":"workflows/#package-publishing-workflow","title":"Package Publishing Workflow","text":"<p>File: <code>.github/workflows/publish-pip.yml</code></p>"},{"location":"workflows/#purpose_1","title":"Purpose","text":"<p>Publishes Python packages to PyPI with safety guardrails.</p>"},{"location":"workflows/#triggers_1","title":"Triggers","text":"<ul> <li>Git tags matching package name pattern (e.g., <code>lablink-allocator-service_v0.0.2a0</code>)</li> <li>Manual dispatch with dry-run option</li> </ul>"},{"location":"workflows/#features","title":"Features","text":"<ul> <li>Version verification (prevents republishing same version)</li> <li>Metadata validation</li> <li>Linting and tests before publishing</li> <li>Dry-run mode for testing</li> <li>Per-package control (publish allocator/client independently)</li> </ul>"},{"location":"workflows/#input-parameters-manual-dispatch","title":"Input Parameters (Manual Dispatch)","text":"Parameter Description Options Default <code>package</code> Which package to publish <code>lablink-allocator-service</code>, <code>lablink-client-service</code> Required <code>dry_run</code> Test without publishing <code>true</code>, <code>false</code> <code>true</code> <code>skip_tests</code> Skip test suite <code>true</code>, <code>false</code> <code>false</code>"},{"location":"workflows/#workflow-steps","title":"Workflow Steps","text":"<ol> <li>Determine which packages to publish (from tag or input)</li> <li>Run guardrails:</li> <li>Verify release from main branch (for releases)</li> <li>Validate version matches tag</li> <li>Validate package metadata</li> <li>Run linting with <code>ruff</code></li> <li>Run unit tests (unless skipped)</li> <li>Build package with <code>uv build</code></li> <li>Publish to PyPI (unless dry-run)</li> <li>Display manual Docker build instructions</li> </ol>"},{"location":"workflows/#package-versioning","title":"Package Versioning","text":"<ul> <li>Format: <code>{package-name}_v{version}</code></li> <li>Examples:</li> <li><code>lablink-allocator-service_v0.0.2a0</code></li> <li><code>lablink-client-service_v0.0.7a0</code></li> </ul>"},{"location":"workflows/#example-publishing-a-release","title":"Example: Publishing a Release","text":"<pre><code># 1. Create and push tags\ngit tag lablink-allocator-service_v0.0.2a0\ngit tag lablink-client-service_v0.0.7a0\ngit push origin lablink-allocator-service_v0.0.2a0 lablink-client-service_v0.0.7a0\n\n# 2. Workflow automatically:\n#    - Detects tags\n#    - Runs tests for each package\n#    - Publishes to PyPI\n#    - Displays Docker build instructions\n\n# 3. Manually trigger Docker image build (see below)\ngh workflow run lablink-images.yml \\\n  -f environment=prod \\\n  -f allocator_version=0.0.2a0 \\\n  -f client_version=0.0.7a0\n</code></pre>"},{"location":"workflows/#building-docker-images-after-publishing","title":"Building Docker Images After Publishing","text":"<p>CRITICAL: After successfully publishing to PyPI, you MUST manually trigger Docker image builds to create production images with the new package version. This is NOT automatic.</p> <p>Why manual? Production images should only be built with explicit version numbers to ensure traceability. Automatic builds use local code and <code>-test</code> suffix - they are NOT production images.</p> <p>Step-by-Step Process:</p> <ol> <li>Publish packages to PyPI (either via git tags or manual dispatch of <code>publish-pip.yml</code>)</li> <li>Wait for publish workflow to complete - Verify packages are on PyPI</li> <li>Manually trigger Docker image build using one of the methods below</li> </ol> <p>Option 1: Using GitHub CLI (recommended): <pre><code># Build both images with their respective versions\ngh workflow run lablink-images.yml \\\n  -f environment=prod \\\n  -f allocator_version=0.0.2a0 \\\n  -f client_version=0.0.7a0\n</code></pre></p> <p>Option 2: Using GitHub UI: 1. Go to Actions \u2192 Build and Push Docker Images 2. Click \"Run workflow\" 3. Select branch: <code>main</code> 4. Set environment: <code>prod</code> (required!) 5. Enter allocator version: <code>0.0.2a0</code> (required!) 6. Enter client version: <code>0.0.7a0</code> (required!) 7. Click \"Run workflow\"</p> <p>What happens: - Pulls packages from PyPI with specified versions - Builds Docker images using production <code>Dockerfile</code> - Tags images with version numbers (e.g., <code>:0.0.2a0</code>, <code>:linux-amd64-0.0.2a0</code>) - Tags images with <code>:latest</code> for convenience - Verifies images work correctly - No <code>-test</code> suffix on production images</p> <p>Common mistake: Forgetting this step means your packages exist on PyPI but there are no corresponding Docker images, causing deployment failures.</p> <p>Important Note: Pushing to <code>main</code> branch will NOT create production images. It will create development images with <code>-test</code> suffix using local code, not published packages.</p>"},{"location":"workflows/#image-building-workflow","title":"Image Building Workflow","text":"<p>File: <code>.github/workflows/lablink-images.yml</code></p>"},{"location":"workflows/#purpose_2","title":"Purpose","text":"<p>Builds and publishes Docker images to GitHub Container Registry (ghcr.io) using either local code (dev) or published packages (prod), then verifies the images work correctly.</p>"},{"location":"workflows/#triggers_2","title":"Triggers","text":"<ul> <li>Pull requests: Build dev images with <code>-test</code> tag</li> <li>Push to <code>test</code> branch: Build dev images with <code>-test</code> tag</li> <li>Push to <code>main</code>: Build dev images with <code>-test</code> tag</li> <li>Manual dispatch with <code>environment=test</code>: Build dev images with <code>-test</code> tag</li> <li>Manual dispatch with <code>environment=prod</code>: Build production images from PyPI (REQUIRES version parameters)</li> </ul>"},{"location":"workflows/#workflow-decision-logic","title":"Workflow Decision Logic","text":"<p>The workflow automatically selects between development (<code>Dockerfile.dev</code>) and production (<code>Dockerfile</code>) builds based on the trigger type and inputs.</p> <p>Key Principle: Production images from PyPI are ONLY created via manual dispatch with <code>environment=prod</code> and explicit version numbers. All automatic builds (PR, push to test/main) use local code.</p>"},{"location":"workflows/#decision-flow-diagram","title":"Decision Flow Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   How was the workflow triggered?              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502           \u2502             \u2502\n   Pull Request   Push       Manual Dispatch\n                              (workflow_dispatch)\n        \u2502           \u2502             \u2502\n        \u2502      \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510        \u2502\n        \u2502      \u2502         \u2502        \u2502\n        \u2502    main      test       \u2502\n        \u2502   branch   branch       \u2502\n        \u2502      \u2502         \u2502        \u2502\n        \u2502      \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518        \u2502\n        \u2502           \u2502        \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510\n        \u2502           \u2502        \u2502         \u2502\n        \u2502           \u2502   environment environment\n        \u2502           \u2502     = test      = prod\n        \u2502           \u2502        \u2502           \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2502\n              \u2502                          \u2502\n              \u25bc                          \u25bc\n       Use Dockerfile.dev         Use Dockerfile\n       (Local Code)               (PyPI Package)\n       + dev dependencies         + REQUIRES version\n       + -test suffix             + version tag\n       + runs tests               + no tests (already tested)\n</code></pre>"},{"location":"workflows/#complete-decision-table","title":"Complete Decision Table","text":"Trigger Type Branch/Ref Environment Input Dockerfile Used Package Source Version Required? Tag Suffix Dev Tests Run? Use Case Pull Request any N/A <code>Dockerfile.dev</code> Local code No <code>-test</code> Yes CI validation Push <code>test</code> N/A <code>Dockerfile.dev</code> Local code No <code>-test</code> Yes Staging/testing Push <code>main</code> N/A <code>Dockerfile.dev</code> Local code No <code>-test</code> Yes Latest development Manual Dispatch any <code>test</code> <code>Dockerfile.dev</code> Local code No <code>-test</code> Yes Test specific changes Manual Dispatch any <code>prod</code> <code>Dockerfile</code> PyPI (explicit version) YES none No Production releases"},{"location":"workflows/#key-points","title":"Key Points","text":"<ul> <li>Development builds (<code>Dockerfile.dev</code>):</li> <li>Copy local source code into image</li> <li>Install with <code>uv sync --extra dev</code> from lockfile</li> <li>Include dev dependencies (pytest, ruff)</li> <li>Run verification tests in CI</li> <li>Always have <code>-test</code> suffix</li> <li> <p>Fast iteration, reproducible via lockfile</p> </li> <li> <p>Production builds (<code>Dockerfile</code>):</p> </li> <li>Install packages from PyPI using <code>uv pip install</code></li> <li>ONLY created via manual dispatch with <code>environment=prod</code></li> <li>REQUIRES explicit <code>allocator_version</code> and <code>client_version</code></li> <li>No dev dependencies (smaller image)</li> <li>No tests run (package already tested before publishing)</li> <li>Tagged with version number for traceability</li> <li>No suffix - clean version tags</li> <li> <p>Directly traceable to specific package release</p> </li> <li> <p>Version Validation:</p> </li> <li>Manual dispatch with <code>environment=prod</code> requires both <code>allocator_version</code> and <code>client_version</code></li> <li>Workflow fails with clear error if versions are missing</li> <li>Prevents untrackable production images</li> </ul>"},{"location":"workflows/#production-release-workflow","title":"Production Release Workflow","text":"<p>IMPORTANT: Production Docker images must be built AFTER publishing packages to PyPI. This is a manual two-step process:</p> <p>Step 1: Publish packages to PyPI <pre><code># Create and push git tags\ngit tag lablink-allocator-service_v0.0.2a0\ngit tag lablink-client-service_v0.0.7a0\ngit push origin lablink-allocator-service_v0.0.2a0 lablink-client-service_v0.0.7a0\n\n# publish-pip.yml workflow automatically:\n#   - Runs tests\n#   - Publishes to PyPI\n#   - Displays manual Docker build command\n</code></pre></p> <p>Step 2: Manually trigger Docker image build (required) <pre><code># After packages are published, build production images\ngh workflow run lablink-images.yml \\\n  -f environment=prod \\\n  -f allocator_version=0.0.2a0 \\\n  -f client_version=0.0.7a0\n</code></pre></p> <p>Critical: Do NOT skip Step 2. Without it, your published packages won't have corresponding Docker images, and deployments will fail.</p>"},{"location":"workflows/#developmenttesting-workflows","title":"Development/Testing Workflows","text":"<p>Automatic (no action needed): <pre><code># Push to test branch \u2192 automatically builds dev images with -test suffix\ngit push origin test\n\n# Push to main \u2192 automatically builds dev images with -test suffix\ngit push origin main\n</code></pre></p> <p>Manual testing: <pre><code># Test specific changes without pushing\ngh workflow run lablink-images.yml -f environment=test\n</code></pre></p>"},{"location":"workflows/#common-mistakes","title":"Common Mistakes","text":"<p>Forgetting to build Docker images after publishing packages <pre><code># Published to PyPI but forgot Step 2\ngit push origin lablink-allocator-service_v0.0.2a0\n# Result: Package exists but no Docker image with version tag\n</code></pre></p> <p>Trying to build production images without versions <pre><code>gh workflow run lablink-images.yml -f environment=prod\n# Error: Production builds require both allocator_version and client_version\n</code></pre></p> <p>Correct production release <pre><code># 1. Publish packages\ngit push origin lablink-allocator-service_v0.0.2a0 lablink-client-service_v0.0.7a0\n\n# 2. Wait for publish-pip.yml to complete successfully\n\n# 3. Build Docker images with explicit versions\ngh workflow run lablink-images.yml \\\n  -f environment=prod \\\n  -f allocator_version=0.0.2a0 \\\n  -f client_version=0.0.7a0\n</code></pre></p>"},{"location":"workflows/#smart-dockerfile-selection","title":"Smart Dockerfile Selection","text":"<p>The workflow uses different Dockerfiles depending on whether you're building for development/testing or production:</p> Trigger Dockerfile Used Package Source Installation Method Tests Run? Suffix Version Tagged? PR <code>Dockerfile.dev</code> Local code (copied) <code>uv sync --extra dev</code> Yes <code>-test</code> No Push to <code>test</code> <code>Dockerfile.dev</code> Local code (copied) <code>uv sync --extra dev</code> Yes <code>-test</code> No Push to <code>main</code> <code>Dockerfile.dev</code> Local code (copied) <code>uv sync --extra dev</code> Yes <code>-test</code> No Manual <code>environment=test</code> <code>Dockerfile.dev</code> Local code (copied) <code>uv sync --extra dev</code> Yes <code>-test</code> No Manual <code>environment=prod</code> <code>Dockerfile</code> PyPI (explicit version) <code>uv pip install</code> No none Yes <p>Key Distinction: - All automatic builds = Development images with <code>-test</code> suffix - Manual production builds = Production images without suffix, with version tags</p>"},{"location":"workflows/#image-tagging-strategy","title":"Image Tagging Strategy","text":"<p>Docker images are tagged differently based on how they are triggered. This allows you to reference specific versions, latest development builds, or stable releases.</p>"},{"location":"workflows/#allocator-image-tags","title":"Allocator Image Tags","text":"<p>Manual trigger with package version (recommended for production): <pre><code>gh workflow run lablink-images.yml \\\n  -f environment=prod \\\n  -f allocator_version=0.0.2a0 \\\n  -f client_version=0.0.7a0\n</code></pre></p> <p>Creates images tagged with: - <code>ghcr.io/talmolab/lablink-allocator-image:0.0.2a0</code> - Version-specific tag - <code>ghcr.io/talmolab/lablink-allocator-image:linux-amd64-0.0.2a0</code> - Platform + version - <code>ghcr.io/talmolab/lablink-allocator-image:linux-amd64-latest</code> - Latest for platform - <code>ghcr.io/talmolab/lablink-allocator-image:linux-amd64</code> - Platform tag - <code>ghcr.io/talmolab/lablink-allocator-image:linux-amd64-terraform-1.4.6</code> - Metadata tag - <code>ghcr.io/talmolab/lablink-allocator-image:linux-amd64-postgres-15</code> - Metadata tag - <code>ghcr.io/talmolab/lablink-allocator-image:&lt;sha&gt;</code> - Git commit SHA - <code>ghcr.io/talmolab/lablink-allocator-image:latest</code> - Latest stable</p> <p>Push to main branch (automatic): <pre><code>git push origin main\n</code></pre></p> <p>Creates images tagged with (no version-specific tags): - <code>ghcr.io/talmolab/lablink-allocator-image:linux-amd64-latest</code> - <code>ghcr.io/talmolab/lablink-allocator-image:linux-amd64</code> - <code>ghcr.io/talmolab/lablink-allocator-image:&lt;sha&gt;</code> - <code>ghcr.io/talmolab/lablink-allocator-image:latest</code> - Plus metadata tags</p> <p>Pull requests / test branch (automatic): <pre><code>git push origin test\n</code></pre></p> <p>Creates images tagged with <code>-test</code> suffix: - <code>ghcr.io/talmolab/lablink-allocator-image:linux-amd64-test</code> - <code>ghcr.io/talmolab/lablink-allocator-image:&lt;sha&gt;-test</code> - Plus metadata tags with <code>-test</code> suffix</p>"},{"location":"workflows/#client-image-tags","title":"Client Image Tags","text":"<p>Manual trigger with package version (recommended for production): <pre><code>gh workflow run lablink-images.yml \\\n  -f environment=prod \\\n  -f allocator_version=0.0.2a0 \\\n  -f client_version=0.0.7a0\n</code></pre></p> <p>Creates images tagged with: - <code>ghcr.io/talmolab/lablink-client-base-image:0.0.7a0</code> - Version-specific tag - <code>ghcr.io/talmolab/lablink-client-base-image:linux-amd64-0.0.7a0</code> - Platform + version - <code>ghcr.io/talmolab/lablink-client-base-image:linux-amd64-latest</code> - Latest for platform - <code>ghcr.io/talmolab/lablink-client-base-image:linux-amd64-nvidia-cuda-11.6.1-cudnn8-runtime-ubuntu20.04</code> - <code>ghcr.io/talmolab/lablink-client-base-image:linux-amd64-ubuntu20.04-nvm-0.40.2-uv-0.6.8-miniforge3-24.11.3</code> - <code>ghcr.io/talmolab/lablink-client-base-image:&lt;sha&gt;</code> - Git commit SHA - <code>ghcr.io/talmolab/lablink-client-base-image:latest</code> - Latest stable</p> <p>Push to main branch (automatic):</p> <p>Creates same tags as manual trigger except without version-specific tags (<code>0.0.7a0</code>, <code>linux-amd64-0.0.7a0</code>)</p> <p>Pull requests / test branch (automatic):</p> <p>Creates same tags as main but with <code>-test</code> suffix</p>"},{"location":"workflows/#tag-usage-in-terraform","title":"Tag Usage in Terraform","text":"<p>For production deployments, always use version-specific tags in your Terraform configuration:</p> <pre><code># terraform.tfvars or -var flags\nallocator_image_tag = \"0.0.2a0\"  # Pin to specific version\nclient_image_tag    = \"0.0.7a0\"  # Pin to specific version\n</code></pre> <p>For development/testing, you can use environment-specific tags:</p> <pre><code># Development\nallocator_image_tag = \"linux-amd64-test\"\n\n# Latest main branch\nallocator_image_tag = \"latest\"\n</code></pre>"},{"location":"workflows/#summary-table","title":"Summary Table","text":"Trigger Type Environment Version Tag? Suffix Use Case Manual w/ version <code>prod</code> \u2705 Yes None Production releases Push to main <code>prod</code> \u274c No None Latest development Push to test <code>test</code> \u274c No <code>-test</code> Staging/testing Pull request <code>test</code> \u274c No <code>-test</code> CI/CD validation"},{"location":"workflows/#workflow-jobs","title":"Workflow Jobs","text":""},{"location":"workflows/#1-build-job","title":"1. Build Job","text":"<ol> <li>Select Dockerfile</li> <li>Dev: Uses <code>Dockerfile.dev</code> (copies local source, uses <code>uv sync</code>)</li> <li> <p>Prod: Uses <code>Dockerfile</code> (installs from PyPI with <code>uv pip install</code>)</p> </li> <li> <p>Build Allocator Image</p> </li> <li>Context: Repository root</li> <li>Dockerfile: <code>packages/allocator/Dockerfile[.dev]</code></li> <li> <p>Tags: <code>ghcr.io/talmolab/lablink-allocator-image:&lt;tags&gt;</code></p> </li> <li> <p>Build Client Image</p> </li> <li>Context: Repository root</li> <li>Dockerfile: <code>packages/client/Dockerfile[.dev]</code></li> <li> <p>Tags: <code>ghcr.io/talmolab/lablink-client-base-image:&lt;tags&gt;</code></p> </li> <li> <p>Push to Registry</p> </li> <li>Authenticates to ghcr.io</li> <li>Pushes images with all applicable tags</li> </ol>"},{"location":"workflows/#2-verify-allocator-job","title":"2. Verify Allocator Job","text":"<p>Runs after successful build, pulls and tests the allocator image:</p> <ul> <li>Image Selection: Pulls using SHA-based tag (e.g., <code>:linux-amd64-&lt;sha&gt;-test</code>) to ensure exact image match and prevent race conditions from concurrent builds</li> <li>Virtual Environment: Activates venv at <code>/app/.venv</code></li> <li>Entry Points: Verifies <code>main()</code> and <code>generate_init_sql.main()</code> are importable and callable</li> <li>Console Scripts: Verifies <code>lablink-allocator</code> and <code>generate-init-sql</code> exist and execute</li> <li>Package Imports: Tests importing <code>main</code>, <code>database.PostgresqlDatabase</code>, <code>get_config</code></li> <li>Dev Dependencies (dev images only): Verifies pytest, ruff with versions</li> </ul>"},{"location":"workflows/#3-verify-client-job","title":"3. Verify Client Job","text":"<p>Runs after successful build, pulls and tests the client image:</p> <ul> <li>Image Selection: Pulls using SHA-based tag (e.g., <code>:linux-amd64-&lt;sha&gt;-test</code>) to ensure exact image match and prevent race conditions from concurrent builds</li> <li>Virtual Environment: Activates venv at <code>/home/client/.venv</code></li> <li>Entry Points: Verifies <code>check_gpu.main()</code>, <code>subscribe.main()</code>, <code>update_inuse_status.main()</code> are importable and callable</li> <li>Console Scripts: Verifies <code>check_gpu</code>, <code>subscribe</code>, <code>update_inuse_status</code> exist and execute</li> <li>Package Imports: Tests importing subscribe, check_gpu, update_inuse_status modules</li> <li>UV Availability: Verifies <code>uv</code> command and version</li> <li>Dev Dependencies (dev images only): Verifies pytest, ruff with versions</li> </ul>"},{"location":"workflows/#example-workflow-run_1","title":"Example Workflow Run","text":"<pre><code>PR opened \u2192 lablink-images.yml triggered\n  \u2514\u2500 Build Job\n     \u251c\u2500 Build allocator dev image \u2713\n     \u251c\u2500 Build client dev image \u2713\n     \u2514\u2500 Push to ghcr.io \u2713\n  \u2514\u2500 Verify Allocator Job\n     \u251c\u2500 Pull ghcr.io/.../lablink-allocator-image:linux-amd64-abc1234-test\n     \u251c\u2500 Venv activated: /app/.venv \u2713\n     \u251c\u2500 Entry points callable: main(), generate_init_sql.main() \u2713\n     \u251c\u2500 Console scripts: lablink-allocator, generate-init-sql \u2713\n     \u251c\u2500 Imports: main.main, database.PostgresqlDatabase, get_config \u2713\n     \u2514\u2500 Dev deps: pytest 8.4.2, ruff \u2713\n  \u2514\u2500 Verify Client Job\n     \u251c\u2500 Pull ghcr.io/.../lablink-client-base-image:linux-amd64-abc1234-test\n     \u251c\u2500 Venv activated: /home/client/.venv \u2713\n     \u251c\u2500 Entry points callable: check_gpu.main(), subscribe.main(), update_inuse_status.main() \u2713\n     \u251c\u2500 Console scripts: check_gpu, subscribe, update_inuse_status \u2713\n     \u251c\u2500 Imports: subscribe.main, check_gpu.main, update_inuse_status.main \u2713\n     \u251c\u2500 UV: uv 0.6.8 \u2713\n     \u2514\u2500 Dev deps: pytest 8.4.2, ruff \u2713\n</code></pre>"},{"location":"workflows/#customization","title":"Customization","text":"<p>To modify image building:</p> <pre><code># .github/workflows/lablink-images.yml\n\n# Build for different platforms\n- name: Build and push\n  uses: docker/build-push-action@v5\n  with:\n    platforms: linux/amd64,linux/arm64  # Add ARM support\n    push: ${{ github.event_name != 'pull_request' }}\n    tags: ${{ steps.meta.outputs.tags }}\n</code></pre>"},{"location":"workflows/#terraform-deployment-workflow","title":"Terraform Deployment Workflow","text":"<p>File: <code>.github/workflows/lablink-allocator-terraform.yml</code></p>"},{"location":"workflows/#purpose_3","title":"Purpose","text":"<p>Deploys LabLink infrastructure to AWS using Terraform.</p>"},{"location":"workflows/#triggers_3","title":"Triggers","text":"<ul> <li>Push to <code>test</code> branch: Automatic test deployment</li> <li>Workflow dispatch: Manual deployment (dev/test/prod)</li> <li>Repository dispatch: Programmatic deployment (for prod)</li> </ul>"},{"location":"workflows/#input-parameters-manual-dispatch_1","title":"Input Parameters (Manual Dispatch)","text":"Parameter Description Required Default <code>environment</code> Environment to deploy (<code>dev</code>, <code>test</code>, <code>prod</code>) Yes <code>dev</code> <code>image_tag</code> Docker image tag (required for prod) For prod only N/A <p>All deployments use the <code>lablink-infrastructure/</code> directory structure with configuration at <code>lablink-infrastructure/config/config.yaml</code>.</p>"},{"location":"workflows/#workflow-steps_1","title":"Workflow Steps","text":""},{"location":"workflows/#1-environment-determination","title":"1. Environment Determination","text":"<pre><code>Push to 'test' branch \u2192 env=test\nManual dispatch \u2192 env=&lt;user input&gt;\nRepository dispatch \u2192 env=&lt;payload&gt;\n</code></pre>"},{"location":"workflows/#2-aws-authentication","title":"2. AWS Authentication","text":"<p>Uses OpenID Connect (OIDC) to assume IAM role: <pre><code>- name: Configure AWS credentials via OIDC\n  uses: aws-actions/configure-aws-credentials@v3\n  with:\n    role-to-assume: arn:aws:iam::711387140753:role/github_lablink_repository-AE68499B37C7\n    aws-region: us-west-2\n</code></pre></p> <p>No AWS credentials stored in GitHub!</p>"},{"location":"workflows/#3-terraform-initialization","title":"3. Terraform Initialization","text":"<pre><code># Dev (local state)\nterraform init\n\n# Test/Prod (remote state)\nterraform init -backend-config=backend-&lt;env&gt;.hcl\n</code></pre>"},{"location":"workflows/#4-validation","title":"4. Validation","text":"<pre><code>terraform fmt -check  # Check formatting\nterraform validate    # Validate syntax\n</code></pre>"},{"location":"workflows/#5-planning","title":"5. Planning","text":"<pre><code>terraform plan \\\n  -var=\"resource_suffix=&lt;env&gt;\" \\\n  -var=\"allocator_image_tag=&lt;tag&gt;\"\n</code></pre>"},{"location":"workflows/#6-application","title":"6. Application","text":"<pre><code>terraform apply -auto-approve \\\n  -var=\"resource_suffix=&lt;env&gt;\" \\\n  -var=\"allocator_image_tag=&lt;tag&gt;\"\n</code></pre>"},{"location":"workflows/#7-artifact-handling","title":"7. Artifact Handling","text":"<ul> <li>Extracts SSH private key from Terraform output</li> <li>Saves as artifact (expires in 1 day)</li> <li>Provides download link in workflow summary</li> </ul>"},{"location":"workflows/#8-failure-handling","title":"8. Failure Handling","text":"<p>If <code>terraform apply</code> fails: <pre><code>terraform destroy -auto-approve\n</code></pre></p> <p>Automatically cleans up partial deployments.</p>"},{"location":"workflows/#example-workflow-run_2","title":"Example Workflow Run","text":"<p>Scenario: Deploy to production</p> <pre><code>1. Navigate to Actions \u2192 Terraform Deploy \u2192 Run workflow\n2. Select:\n   - Environment: prod\n   - Image tag: v1.0.0\n3. Workflow starts:\n   - Authenticates to AWS via OIDC\n   - Initializes Terraform with backend-prod.hcl\n   - Plans infrastructure\n   - Applies changes\n   - Saves SSH key to artifacts\n4. Deployment complete\n5. Outputs displayed in workflow summary:\n   - Allocator FQDN: lablink-prod.example.com\n   - EC2 Public IP: 54.xxx.xxx.xxx\n   - EC2 Key Name: lablink-prod-key\n</code></pre>"},{"location":"workflows/#customization_1","title":"Customization","text":"<p>To add deployment notifications:</p> <pre><code># Add at end of workflow\n- name: Notify Slack\n  if: success()\n  uses: slackapi/slack-github-action@v1\n  with:\n    webhook-url: ${{ secrets.SLACK_WEBHOOK }}\n    payload: |\n      {\n        \"text\": \"LabLink deployed to ${{ steps.setenv.outputs.env }}!\"\n      }\n</code></pre>"},{"location":"workflows/#destroy-workflow","title":"Destroy Workflow","text":"<p>File: <code>.github/workflows/lablink-allocator-destroy.yml</code></p>"},{"location":"workflows/#purpose_4","title":"Purpose","text":"<p>Safely destroy LabLink infrastructure for an environment.</p>"},{"location":"workflows/#triggers_4","title":"Triggers","text":"<ul> <li>Manual dispatch only: Requires explicit user action</li> </ul>"},{"location":"workflows/#input-parameters","title":"Input Parameters","text":"Parameter Description Required <code>environment</code> Environment to destroy (<code>dev</code>, <code>test</code>, <code>prod</code>) Yes"},{"location":"workflows/#safety-features","title":"Safety Features","text":"<ul> <li>Manual trigger only (no automatic destruction)</li> <li>Requires environment selection</li> <li>Shows plan before destroying</li> <li>Logs all destroyed resources</li> </ul>"},{"location":"workflows/#workflow-steps_2","title":"Workflow Steps","text":"<ol> <li>Authenticate to AWS via OIDC</li> <li>Initialize Terraform with correct backend</li> <li>Plan destruction</li> <li>Execute <code>terraform destroy -auto-approve</code></li> <li>Output destroyed resources</li> </ol>"},{"location":"workflows/#example-usage","title":"Example Usage","text":"<pre><code>1. Navigate to Actions \u2192 Allocator Master Destroy\n2. Click \"Run workflow\"\n3. Select environment: dev\n4. Confirm\n5. Workflow destroys:\n   - EC2 instance\n   - Security group\n   - SSH key pair\n6. Terraform state updated\n</code></pre> <p>Destructive Operation</p> <p>This action is irreversible. Ensure you have backups of any data before destroying.</p>"},{"location":"workflows/#infrastructure-testing-workflow","title":"Infrastructure Testing Workflow","text":"<p>File: <code>.github/workflows/client-vm-infrastructure-test.yml</code></p>"},{"location":"workflows/#purpose_5","title":"Purpose","text":"<p>End-to-end test of client VM creation and management.</p>"},{"location":"workflows/#triggers_5","title":"Triggers","text":"<ul> <li>Manual dispatch: On-demand testing</li> <li>Scheduled: Nightly/weekly regression tests (optional)</li> </ul>"},{"location":"workflows/#what-it-tests","title":"What It Tests","text":"<ol> <li>Allocator deployment</li> <li>Client VM spawning</li> <li>VM registration with allocator</li> <li>Health check reporting</li> <li>VM destruction</li> </ol>"},{"location":"workflows/#test-workflow","title":"Test Workflow","text":"<pre><code>1. Deploy test allocator\n2. Request client VM via API\n3. Wait for VM to be created and register\n4. Verify VM appears in allocator database\n5. Check VM health status\n6. Destroy client VM\n7. Destroy allocator\n8. Verify all resources cleaned up\n</code></pre>"},{"location":"workflows/#documentation-workflow","title":"Documentation Workflow","text":"<p>File: <code>.github/workflows/docs.yml</code></p>"},{"location":"workflows/#purpose_6","title":"Purpose","text":"<p>Builds and deploys MkDocs documentation to GitHub Pages.</p>"},{"location":"workflows/#triggers_6","title":"Triggers","text":"<ul> <li>Pushes to <code>main</code> branch</li> <li>Pull requests affecting <code>docs/**</code> or <code>mkdocs.yml</code></li> </ul>"},{"location":"workflows/#what-it-does","title":"What It Does","text":"<ol> <li>Installs Python and dependencies (including docs extras from <code>pyproject.toml</code>)</li> <li>Builds documentation with <code>mkdocs build</code></li> <li>Deploys to GitHub Pages branch (<code>gh-pages</code>)</li> </ol>"},{"location":"workflows/#deployment","title":"Deployment","text":"<p>Documentation is available at: <code>https://talmolab.github.io/lablink/</code></p>"},{"location":"workflows/#workflow-environment-variables","title":"Workflow Environment Variables","text":"<p>Common environment variables used across workflows:</p> Variable Description Source <code>GITHUB_TOKEN</code> GitHub API token Automatic <code>AWS_REGION</code> AWS region Hardcoded (us-west-2) <code>GITHUB_REPOSITORY</code> Repo name Automatic <code>GITHUB_REF_NAME</code> Branch/tag name Automatic"},{"location":"workflows/#secrets-management","title":"Secrets Management","text":""},{"location":"workflows/#required-secrets","title":"Required Secrets","text":"Secret Purpose Where Used None! OIDC handles AWS auth All AWS workflows"},{"location":"workflows/#optional-secrets","title":"Optional Secrets","text":"Secret Purpose How to Set <code>ADMIN_PASSWORD</code> Override admin password Settings \u2192 Secrets <code>DB_PASSWORD</code> Override DB password Settings \u2192 Secrets <code>SLACK_WEBHOOK</code> Notifications Settings \u2192 Secrets"},{"location":"workflows/#adding-secrets","title":"Adding Secrets","text":"<pre><code>1. Go to repository Settings\n2. Navigate to Secrets and variables \u2192 Actions\n3. Click \"New repository secret\"\n4. Name: ADMIN_PASSWORD\n5. Value: your-secure-password\n6. Click \"Add secret\"\n</code></pre> <p>Access in workflows: <pre><code>- name: Use secret\n  env:\n    ADMIN_PASSWORD: ${{ secrets.ADMIN_PASSWORD }}\n  run: |\n    echo \"Password is set\"\n</code></pre></p>"},{"location":"workflows/#workflow-monitoring","title":"Workflow Monitoring","text":""},{"location":"workflows/#view-workflow-runs","title":"View Workflow Runs","text":"<ol> <li>Navigate to Actions tab in GitHub</li> <li>Select workflow from left sidebar</li> <li>View recent runs</li> </ol>"},{"location":"workflows/#workflow-status","title":"Workflow Status","text":"<ul> <li>\u2705 Green checkmark: Success</li> <li>\u274c Red X: Failure</li> <li>\ud83d\udfe1 Yellow dot: In progress</li> <li>\u26aa Gray circle: Queued</li> </ul>"},{"location":"workflows/#debugging-failed-workflows","title":"Debugging Failed Workflows","text":"<ol> <li>Click on failed workflow run</li> <li>Click on failed job</li> <li>Expand failed step</li> <li>Read error logs</li> <li>Fix issue and re-run</li> </ol>"},{"location":"workflows/#re-running-workflows","title":"Re-running Workflows","text":"<p>From workflow run page: - Re-run all jobs: Retry entire workflow - Re-run failed jobs: Only retry failures</p>"},{"location":"workflows/#creating-custom-workflows","title":"Creating Custom Workflows","text":""},{"location":"workflows/#example-backup-workflow","title":"Example: Backup Workflow","text":"<p>Create <code>.github/workflows/backup.yml</code>:</p> <pre><code>name: Backup Database\n\non:\n  schedule:\n    - cron: '0 2 * * *'  # Daily at 2 AM\n  workflow_dispatch:\n\njobs:\n  backup:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Configure AWS\n        uses: aws-actions/configure-aws-credentials@v3\n        with:\n          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}\n          aws-region: us-west-2\n\n      - name: Backup Database\n        run: |\n          # SSH into allocator\n          # Run pg_dump\n          # Upload to S3\n          echo \"Backup complete\"\n</code></pre>"},{"location":"workflows/#example-notification-workflow","title":"Example: Notification Workflow","text":"<pre><code>name: Deployment Notifications\n\non:\n  workflow_run:\n    workflows: [\"Terraform Deploy\"]\n    types: [completed]\n\njobs:\n  notify:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Send Email\n        uses: dawidd6/action-send-mail@v3\n        with:\n          server_address: smtp.gmail.com\n          server_port: 465\n          username: ${{ secrets.EMAIL_USERNAME }}\n          password: ${{ secrets.EMAIL_PASSWORD }}\n          subject: \"LabLink Deployment: ${{ github.event.workflow_run.conclusion }}\"\n          body: \"Deployment finished with status: ${{ github.event.workflow_run.conclusion }}\"\n          to: admin@example.com\n</code></pre>"},{"location":"workflows/#best-practices","title":"Best Practices","text":"<ol> <li>Pin action versions: Use <code>@v3</code> not <code>@latest</code></li> <li>Minimize secrets: Use OIDC when possible</li> <li>Cache dependencies: Speed up workflows</li> <li>Fail fast: Stop on first error</li> <li>Use matrix builds: Test multiple versions</li> <li>Set timeouts: Prevent runaway workflows</li> <li>Add status badges: Show workflow status in README</li> </ol>"},{"location":"workflows/#status-badge-example","title":"Status Badge Example","text":"<p>Add to <code>README.md</code>: <pre><code>![CI](https://github.com/talmolab/lablink/actions/workflows/ci.yml/badge.svg)\n![Deploy](https://github.com/talmolab/lablink/actions/workflows/lablink-allocator-terraform.yml/badge.svg)\n</code></pre></p>"},{"location":"workflows/#troubleshooting-workflows","title":"Troubleshooting Workflows","text":""},{"location":"workflows/#workflow-wont-trigger","title":"Workflow Won't Trigger","text":"<p>Check: - Workflow file syntax (use YAML validator) - Trigger conditions match your action - Workflows enabled in repository settings</p>"},{"location":"workflows/#aws-authentication-fails","title":"AWS Authentication Fails","text":"<p>Check: - IAM role ARN is correct - Trust policy includes GitHub OIDC provider - Role has necessary permissions</p>"},{"location":"workflows/#terraform-failures","title":"Terraform Failures","text":"<p>Check: - Terraform syntax (<code>terraform validate</code>) - AWS resource limits - Terraform state lock status</p>"},{"location":"workflows/#image-push-fails","title":"Image Push Fails","text":"<p>Check: - GHCR authentication (should be automatic) - Image size limits - Registry permissions</p>"},{"location":"workflows/#next-steps","title":"Next Steps","text":"<ul> <li>Deployment: Deploy using these workflows</li> <li>Security: Understand OIDC and secrets</li> <li>AWS Setup: Configure AWS for workflows</li> <li>Troubleshooting: Fix workflow issues</li> </ul>"},{"location":"reference/","title":"API Reference","text":"<p>API reference generation requires LabLink packages to be installed.</p> <p>To generate full API documentation:</p> <pre><code>uv pip install -e packages/allocator\nuv pip install -e packages/client\nmkdocs build\n</code></pre>"}]}